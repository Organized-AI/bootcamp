WEBVTT
Kind: captions
Language: en

00:00:05.680 --> 00:00:10.720
All right, thank you. Thank you everyone for&nbsp;
joining us. Uh, so we're picking up with prompting&nbsp;&nbsp;

00:00:10.720 --> 00:00:15.920
for agents. Um, hopefully you were here for&nbsp;
prompting 101 or maybe you're just joining us. U,&nbsp;&nbsp;

00:00:15.920 --> 00:00:20.800
but I'll give a little intro. My name is Hannah.&nbsp;
I'm part of the applied AI team in Anthropic. Hi,&nbsp;&nbsp;

00:00:20.800 --> 00:00:25.840
I'm Jeremy. I'm on our applied AI team as well&nbsp;
and I'm a product engineer. Uh, so we're going&nbsp;&nbsp;

00:00:25.840 --> 00:00:28.800
to talk about prompting for agents. So, we're&nbsp;
going to switch gears a little bit, move on from&nbsp;&nbsp;

00:00:28.800 --> 00:00:34.400
the basics of prompting, um, and talk about how&nbsp;
we do this for agents like playing Pokemon. Uh,&nbsp;&nbsp;

00:00:34.400 --> 00:00:39.120
so hopefully you were here, uh, for prompting&nbsp;
101 or maybe you have some familiarity with&nbsp;&nbsp;

00:00:39.120 --> 00:00:44.000
basic prompting. So, we're not going to go over&nbsp;
um the really kind of basic console prompting or&nbsp;&nbsp;

00:00:44.000 --> 00:00:48.960
interacting with Claude and the desktop today.&nbsp;
But just a refresher, uh, we think about prompt&nbsp;&nbsp;

00:00:48.960 --> 00:00:53.200
engineering as kind of programming in natural&nbsp;
language. you're thinking about what your agent&nbsp;&nbsp;

00:00:53.200 --> 00:00:57.680
or your model is going to be doing, what kind&nbsp;
of tasks it's accomplishing. You're trying to&nbsp;&nbsp;

00:00:57.680 --> 00:01:03.600
clearly communicate to the agent, give examples&nbsp;
where necessary, um, and give guidelines. Uh,&nbsp;&nbsp;

00:01:03.600 --> 00:01:08.960
we do, you know, follow kind of a very specific&nbsp;
structure for console prompting. I want you to&nbsp;&nbsp;

00:01:08.960 --> 00:01:12.640
remove this from your mind because it could look&nbsp;
very different for an agent. So, for an agent,&nbsp;&nbsp;

00:01:12.640 --> 00:01:17.440
you may not be laying out this type of very&nbsp;
structured prompt. Uh, it's actually going to&nbsp;&nbsp;

00:01:17.440 --> 00:01:21.040
look a lot different. We're going to allow&nbsp;
a lot of different things to come in. So,&nbsp;&nbsp;

00:01:21.040 --> 00:01:24.560
I'm going to turn it over I'm going to talk about&nbsp;
what agents are and then I'll turn it over to&nbsp;&nbsp;

00:01:24.560 --> 00:01:29.680
Jeremy to talk about how we do this for agents.&nbsp;
So, hopefully you have a sense in your mind of&nbsp;&nbsp;

00:01:29.680 --> 00:01:35.120
what an agent is. At Anthropic, we like to say&nbsp;
that agents are models using tools in a loop. So,&nbsp;&nbsp;

00:01:35.120 --> 00:01:41.920
we give the agent a task and we allow it to work&nbsp;
continuously and use tools as it thinks fit. Um,&nbsp;&nbsp;

00:01:41.920 --> 00:01:46.240
update its decisions based on the information&nbsp;
that it's getting back from its tool calls and&nbsp;&nbsp;

00:01:46.240 --> 00:01:51.920
continue working independently until it completes&nbsp;
the task. So that's we kind of keep it as simple&nbsp;&nbsp;

00:01:51.920 --> 00:01:56.880
as that. Um the environment which is where the&nbsp;
agent is working, the tools that the agent has&nbsp;&nbsp;

00:01:56.880 --> 00:02:01.280
and the system prompt is just where we tell the&nbsp;
agent what it should be doing or what it should be&nbsp;&nbsp;

00:02:01.280 --> 00:02:06.400
accomplishing. And we typically find the simpler&nbsp;
you can keep this the better. Allow the agent to&nbsp;&nbsp;

00:02:06.400 --> 00:02:12.720
do its work. Allow the model to be the model and&nbsp;
kind of work through this task. So when do you use&nbsp;&nbsp;

00:02:12.720 --> 00:02:18.000
agents? You do not always need to use an agent.&nbsp;
In fact, there's many scenarios in which you won't&nbsp;&nbsp;

00:02:18.000 --> 00:02:22.560
actually want to use an agent. There are other&nbsp;
approaches that would be more appropriate. Um,&nbsp;&nbsp;

00:02:22.560 --> 00:02:27.280
agents are really best for complex and&nbsp;
valuable tasks. It's not something you&nbsp;&nbsp;

00:02:27.280 --> 00:02:31.840
should deploy in every possible scenario. You&nbsp;
will not get the results that you want. Um,&nbsp;&nbsp;

00:02:31.840 --> 00:02:35.680
and you'll spend a lot more resources than&nbsp;
you maybe need to. So, we'll talk a little&nbsp;&nbsp;

00:02:35.680 --> 00:02:40.880
bit about checklist or or kind of ways of thinking&nbsp;
about when you should be using an agent and maybe&nbsp;&nbsp;

00:02:40.880 --> 00:02:46.560
you don't want to be using an agent. So, is the&nbsp;
task complex? Is this a task that you, a human,&nbsp;&nbsp;

00:02:46.560 --> 00:02:51.200
can think through a step-by-step process to&nbsp;
complete? If so, you probably don't need an&nbsp;&nbsp;

00:02:51.200 --> 00:02:55.840
agent. You want to use an agent where it's not&nbsp;
clear to you how you'll go about accomplishing the&nbsp;&nbsp;

00:02:55.840 --> 00:03:00.320
task. You might know where you want to go, but you&nbsp;
don't know exactly how you're going to get there,&nbsp;&nbsp;

00:03:00.320 --> 00:03:05.120
what tools, and what information you might need&nbsp;
to arrive at the end state. Is a task valuable?&nbsp;&nbsp;

00:03:05.120 --> 00:03:09.760
Are you going to get a lot of value out of the&nbsp;
agent accomplishing this task? Or is this a kind&nbsp;&nbsp;

00:03:09.760 --> 00:03:14.960
of a low value uh task or workflow? In that case,&nbsp;
a workflow might also be better. You don't really&nbsp;&nbsp;

00:03:14.960 --> 00:03:19.600
want to be using the resources of an agent unless&nbsp;
this is something you get that's highly leveraged.&nbsp;&nbsp;

00:03:19.600 --> 00:03:23.360
It's maybe revenue generating. It's something&nbsp;
that's really valuable to your user. Again,&nbsp;&nbsp;

00:03:23.360 --> 00:03:29.120
it's something that's complex. Uh the last next&nbsp;
piece is are the parts of the task doable? So,&nbsp;&nbsp;

00:03:29.120 --> 00:03:34.160
when you think about the task that has to occur,&nbsp;
would you be able to give the agents the tools&nbsp;&nbsp;

00:03:34.160 --> 00:03:39.440
that it needs in order to accomplish this task?&nbsp;
If you can't define the tools or if you can't&nbsp;&nbsp;

00:03:39.440 --> 00:03:43.840
give the agent access to the information or&nbsp;
the tool that it would need, you may want to&nbsp;&nbsp;

00:03:43.840 --> 00:03:49.200
scope the task down. Um, if you can define and&nbsp;
give to the agent the tools that it would want,&nbsp;&nbsp;

00:03:49.200 --> 00:03:54.240
that's a better use case for an agent. The last&nbsp;
thing you might want to think about is the cost of&nbsp;&nbsp;

00:03:54.240 --> 00:04:00.480
errors or how easy it is to discover errors. So,&nbsp;
if it's really uh difficult to correct an error or&nbsp;&nbsp;

00:04:00.480 --> 00:04:04.960
detect an error, that is maybe not a place where&nbsp;
you want the agent to be working independently.&nbsp;&nbsp;

00:04:04.960 --> 00:04:09.040
you might want to have a human in the loop in&nbsp;
that case. If it the error is something that&nbsp;&nbsp;

00:04:09.040 --> 00:04:14.320
you can recover from or if it's not too costly to&nbsp;
have an error occurring, then you might continue&nbsp;&nbsp;

00:04:14.320 --> 00:04:20.080
to allow the agent to work independently. So to&nbsp;
make this a little bit more real, uh we'll talk&nbsp;&nbsp;

00:04:20.080 --> 00:04:24.880
about a few examples. I'm not going to go through&nbsp;
each single one of these, but let's pick out a few&nbsp;&nbsp;

00:04:24.880 --> 00:04:30.400
that will be pretty clear or intuitive for most of&nbsp;
us. So coding, obviously, um all of you are very&nbsp;&nbsp;

00:04:30.400 --> 00:04:35.440
familiar with using agents and coding. Uh coding&nbsp;
is a great use case. We can think about something&nbsp;&nbsp;

00:04:35.440 --> 00:04:41.280
uh like a design document. And although you know&nbsp;
where you want to get to, which is raising a PR,&nbsp;&nbsp;

00:04:41.280 --> 00:04:44.800
you don't know exactly how you're going to get&nbsp;
there. It's not clear to you what you'll build&nbsp;&nbsp;

00:04:44.800 --> 00:04:49.200
first, how you'll iterate on that, what changes&nbsp;
you might make along the way depending on what&nbsp;&nbsp;

00:04:49.200 --> 00:04:56.720
you find. Um this is high value. You're all very&nbsp;
skilled. If an agent, okay, if an agent is able,&nbsp;&nbsp;

00:04:56.720 --> 00:05:04.800
this is like more like what the midway is like&nbsp;
at night. I feel I feel more at home now. Um,&nbsp;&nbsp;

00:05:04.800 --> 00:05:09.520
uh, Claude Claude is great at coding. Um, and this&nbsp;
is a high value use case, right? If your agent is&nbsp;&nbsp;

00:05:09.520 --> 00:05:14.720
actually able to go from a design document to&nbsp;
a PR, that's a lot of time that you, a highly&nbsp;&nbsp;

00:05:14.720 --> 00:05:19.040
skilled engineer, are saved and you're able to&nbsp;
then spend your time on something else that's&nbsp;&nbsp;

00:05:19.040 --> 00:05:25.200
higher leverage. So, great use case for agents.&nbsp;
A couple other examples I'll mention here. Um,&nbsp;&nbsp;

00:05:25.200 --> 00:05:30.640
maybe we'll talk about the the cost of error.&nbsp;
So, search, if we make an error in the search,&nbsp;&nbsp;

00:05:30.640 --> 00:05:35.600
there's ways that we can correct that, right? So&nbsp;
we can use citations, we can use other methods of&nbsp;&nbsp;

00:05:35.600 --> 00:05:39.920
double-checking the results. So if the agent makes&nbsp;
a mistake in the search process, this is something&nbsp;&nbsp;

00:05:39.920 --> 00:05:45.520
we can recover from and it's probably not too&nbsp;
costly. Computer use, um, this is also a place&nbsp;&nbsp;

00:05:45.520 --> 00:05:50.480
where we can recover from errors. We might just go&nbsp;
back, we might try clicking again. It's not, uh,&nbsp;&nbsp;

00:05:50.480 --> 00:05:56.240
too difficult to allow Claude just to click a few&nbsp;
times until it's able to use the tool properly.&nbsp;&nbsp;

00:05:56.240 --> 00:06:01.440
Um, data analysis, I think, is another interesting&nbsp;
example, kind of analogous to coding. We might&nbsp;&nbsp;

00:06:01.440 --> 00:06:05.920
know uh the end result that we want to get to.&nbsp;
We know a set of insights that we want to gather&nbsp;&nbsp;

00:06:05.920 --> 00:06:10.320
out of data or a visualization that we want to&nbsp;
produce from data. We don't know exactly what the&nbsp;&nbsp;

00:06:10.320 --> 00:06:14.560
data might look like. Uh so the data could have&nbsp;
different formats. It could have errors in it.&nbsp;&nbsp;

00:06:14.560 --> 00:06:20.000
It could have other uh it could have granularity&nbsp;
issues that we're not sure how to disagregate. We&nbsp;&nbsp;

00:06:20.000 --> 00:06:23.760
don't know the exact process that we're going to&nbsp;
take in analyzing that data, but we know where we&nbsp;&nbsp;

00:06:23.760 --> 00:06:30.000
want to get in the end. Um so this is another&nbsp;
example of a great use case for agents. Uh,&nbsp;&nbsp;

00:06:30.000 --> 00:06:34.160
so hopefully these make sense to you and I'm going&nbsp;
to turn it over to Jeremy now. He has some really&nbsp;&nbsp;

00:06:34.160 --> 00:06:39.920
rich experience building agents and he's going to&nbsp;
share some best practices for actually prompting&nbsp;&nbsp;

00:06:39.920 --> 00:06:46.080
them well and how to structure a great prompt&nbsp;
for an agent. Thanks Hannah. Hi all. Um, yeah,&nbsp;&nbsp;

00:06:46.080 --> 00:06:51.440
so prompting for agents. Um, I think some things&nbsp;
that we think about here, I I'll go over a few of&nbsp;&nbsp;

00:06:51.440 --> 00:06:56.480
them. We've learned these experiences mostly from&nbsp;
building agents ourselves. So some agents that you&nbsp;&nbsp;

00:06:56.480 --> 00:07:01.680
can try from enthropic are cla code which works in&nbsp;
your terminal and sort of agentically browses your&nbsp;&nbsp;

00:07:01.680 --> 00:07:07.520
files and uses the bash tool to really accomplish&nbsp;
tasks um in coding. Similarly we have our new&nbsp;&nbsp;

00:07:07.520 --> 00:07:12.480
advanced research feature in cloud.ai and this&nbsp;
allows you to do hours of research. For example,&nbsp;&nbsp;

00:07:12.480 --> 00:07:18.000
you can find hundreds of startups building agents&nbsp;
or you can find hundreds of potential prospects&nbsp;&nbsp;

00:07:18.000 --> 00:07:24.160
for your company. And this allows the model to&nbsp;
do research across your tools, your Google Drive,&nbsp;&nbsp;

00:07:24.160 --> 00:07:28.400
web search and stuff like that. And so in the&nbsp;
process of building these products, one things&nbsp;&nbsp;

00:07:28.400 --> 00:07:32.800
that we learned is that you need to think like&nbsp;
your agents. This is maybe the most important&nbsp;&nbsp;

00:07:32.800 --> 00:07:37.360
principle. Um the idea is that essentially you&nbsp;
need to understand and develop a mental model&nbsp;&nbsp;

00:07:37.360 --> 00:07:42.320
of what your agent is doing and what it's like to&nbsp;
be in that environment. So the environment for the&nbsp;&nbsp;

00:07:42.320 --> 00:07:47.120
agent is a set of tools and the responses it gets&nbsp;
back from those tools. In the context of cloud&nbsp;&nbsp;

00:07:47.120 --> 00:07:52.640
code, the way you might do this is by actually&nbsp;
simulating the process and just imagining if you&nbsp;&nbsp;

00:07:52.640 --> 00:07:58.160
were in cloud code's shoes given the exact tool&nbsp;
descriptions it has and the tool schemas it has,&nbsp;&nbsp;

00:07:58.160 --> 00:08:02.720
would you be confused or would you be able to&nbsp;
do do the task that it's doing? If a human can't&nbsp;&nbsp;

00:08:02.720 --> 00:08:07.520
understand what your agent should be doing, then&nbsp;
an AI will not be able to either. And so this is&nbsp;&nbsp;

00:08:07.520 --> 00:08:11.840
really important for thinking about tool design,&nbsp;
thinking about prompting is to simulate and go&nbsp;&nbsp;

00:08:11.840 --> 00:08:17.360
through their environment. Another is that you&nbsp;
need to give your agents reasonable heristics.&nbsp;&nbsp;

00:08:17.360 --> 00:08:21.600
And so, you know, Hannah mentioned that prompt&nbsp;
engineering is conceptual engineering. What does&nbsp;&nbsp;

00:08:21.600 --> 00:08:26.000
that really mean? It's one of the reasons why&nbsp;
prompt engineering is not going away and why I&nbsp;&nbsp;

00:08:26.000 --> 00:08:30.720
personally expect prompting to get more important,&nbsp;
not less important as models get smarter. This is&nbsp;&nbsp;

00:08:30.720 --> 00:08:35.040
because prompting is not just about text. It's not&nbsp;
just about the words that you give the model. It's&nbsp;&nbsp;

00:08:35.040 --> 00:08:40.400
about deciding what concepts the model should have&nbsp;
and what behaviors it should follow to perform&nbsp;&nbsp;

00:08:40.400 --> 00:08:46.240
well in a specific environment. So for example,&nbsp;
cloud code has the concept of irreversibility.&nbsp;&nbsp;

00:08:46.240 --> 00:08:51.280
It should not take irreversible actions that&nbsp;
might harm the user or harm their environment.&nbsp;&nbsp;

00:08:51.280 --> 00:08:56.160
So it will avoid these kinds of harmful actions&nbsp;
or anything that might cause irreversible damage&nbsp;&nbsp;

00:08:56.160 --> 00:09:00.720
to your environment or to your code or anything&nbsp;
like that. So that concept of irreversibility is&nbsp;&nbsp;

00:09:00.720 --> 00:09:04.800
something that you need to instill in the model&nbsp;
and be very clear about and think about the edge&nbsp;&nbsp;

00:09:04.800 --> 00:09:09.360
cases. How might the model in misinterpret&nbsp;
this concept? How might it not know what it&nbsp;&nbsp;

00:09:09.360 --> 00:09:14.400
means? For example, if you want the model to be&nbsp;
very eager and you want it to be very agentic,&nbsp;&nbsp;

00:09:14.400 --> 00:09:18.560
well, it might go over the top a little bit. It&nbsp;
might misinterpret what you're saying and do more&nbsp;&nbsp;

00:09:18.560 --> 00:09:22.240
than what you expect. And so, you have to be very&nbsp;
crisp and clear about the concepts you're giving&nbsp;&nbsp;

00:09:22.240 --> 00:09:27.600
the models. Um, some examples of these reasonable&nbsp;
heristics that we've learned. One is that while&nbsp;&nbsp;

00:09:27.600 --> 00:09:31.680
we were building research, we noticed that the&nbsp;
model would often do a ton of web searches when&nbsp;&nbsp;

00:09:31.680 --> 00:09:36.720
it was unnecessary. For example, it would find the&nbsp;
actual answer it needed. like maybe you would find&nbsp;&nbsp;

00:09:36.720 --> 00:09:42.080
a list of scaleups in the United States and then&nbsp;
it would keep going even though it already had the&nbsp;&nbsp;

00:09:42.080 --> 00:09:47.200
answer and that's because we hadn't told the model&nbsp;
explicitly when you find the answer you can stop&nbsp;&nbsp;

00:09:47.200 --> 00:09:51.920
you no longer need to keep searching uh similarly&nbsp;
we had to give the model sort of budgets to think&nbsp;&nbsp;

00:09:51.920 --> 00:09:57.200
about for example we told it that for simple&nbsp;
queries it should use under five tool calls&nbsp;&nbsp;

00:09:57.200 --> 00:10:02.640
but for more complex queries it might use up to 10&nbsp;
or 15 so these kinds of heruristics that you might&nbsp;&nbsp;

00:10:02.640 --> 00:10:07.520
assume the model already understands you really&nbsp;
have to articulate clearly. A good way to think&nbsp;&nbsp;

00:10:07.520 --> 00:10:12.240
about this is that if you're managing maybe a new&nbsp;
intern who's fresh out of college and has not had&nbsp;&nbsp;

00:10:12.240 --> 00:10:17.600
a job before, how would you articulate to them&nbsp;
how to get around all the problems they might get&nbsp;&nbsp;

00:10:17.600 --> 00:10:21.760
run into in their first job? And how would you&nbsp;
be very crisp and clear with them about how to&nbsp;&nbsp;

00:10:21.760 --> 00:10:25.440
accomplish that? That's often how you should&nbsp;
think about giving heristics to your agents,&nbsp;&nbsp;

00:10:25.440 --> 00:10:29.120
which are just general principles that it&nbsp;
should follow. They may not be strict rules,&nbsp;&nbsp;

00:10:29.120 --> 00:10:34.480
but they're, you know, sort of practices.&nbsp;
Another point is that tool selection is key.&nbsp;&nbsp;

00:10:34.480 --> 00:10:40.240
So as models get more powerful able to handle more&nbsp;
and more tools. Sonnet 4 and Opus 4 can handle&nbsp;&nbsp;

00:10:40.240 --> 00:10:44.800
you know up to a hundred tools even more than&nbsp;
that if you have great prompting. But in order&nbsp;&nbsp;

00:10:44.800 --> 00:10:48.640
to use these tools you have to be clear about&nbsp;
which tools it should use for different tasks.&nbsp;&nbsp;

00:10:48.640 --> 00:10:52.880
So for example for research we can give the model&nbsp;
access to Google Drive. We can give it access to&nbsp;&nbsp;

00:10:52.880 --> 00:10:59.600
MCP tools like Sentry or Data Dog or GitHub. It&nbsp;
can search across all these tools, but the model&nbsp;&nbsp;

00:10:59.600 --> 00:11:04.480
doesn't know already which tools are important&nbsp;
for which tasks. Especially in your specific&nbsp;&nbsp;

00:11:04.480 --> 00:11:09.680
company context. For example, if your company uses&nbsp;
Slack a lot, maybe it should default to searching&nbsp;&nbsp;

00:11:09.680 --> 00:11:15.760
Slack for company related information. All these&nbsp;
questions about how the model should use tools,&nbsp;&nbsp;

00:11:15.760 --> 00:11:21.120
you have to give it explicit principles about&nbsp;
when to use which tools and in which contexts. Um,&nbsp;&nbsp;

00:11:21.120 --> 00:11:25.120
and this is really important and it's often&nbsp;
something I see where people don't prompt the&nbsp;&nbsp;

00:11:25.120 --> 00:11:29.840
agent at all about which tool to use and they&nbsp;
just give the model some tools with some very&nbsp;&nbsp;

00:11:29.840 --> 00:11:34.240
short descriptions and then they wonder like&nbsp;
why isn't the model using the right tool? Well,&nbsp;&nbsp;

00:11:34.240 --> 00:11:38.640
it's likely because the model doesn't know what&nbsp;
it should be doing in that context. Another point&nbsp;&nbsp;

00:11:38.640 --> 00:11:43.600
here is that you can guide the thinking process.&nbsp;
So people often sort of turn extended thinking on&nbsp;&nbsp;

00:11:43.600 --> 00:11:48.160
and then let their agents run and assume it will&nbsp;
get out of the box better performance. Actually&nbsp;&nbsp;

00:11:48.160 --> 00:11:52.480
that assumption is true. Most of the time you will&nbsp;
get out of the box better performance, but you can&nbsp;&nbsp;

00:11:52.480 --> 00:11:57.280
squeeze even more performance out of it if you&nbsp;
just prompt the agent to use its thinking well.&nbsp;&nbsp;

00:11:57.280 --> 00:12:02.480
So for example, for search, what we do is tell&nbsp;
the model to plan out its search process. So in&nbsp;&nbsp;

00:12:02.480 --> 00:12:08.000
advance, it should decide how complicated is this&nbsp;
query? How many tool calls should I use here? What&nbsp;&nbsp;

00:12:08.000 --> 00:12:12.880
sources should I look for? How will I know when&nbsp;
I'm successful? We tell it to plan out all these&nbsp;&nbsp;

00:12:12.880 --> 00:12:18.400
exact things in its first thinking block. And then&nbsp;
a new capability that the cloud 4 models have is&nbsp;&nbsp;

00:12:18.400 --> 00:12:23.760
the ability to use interled thinking between tool&nbsp;
calls. So after getting results from the web, we&nbsp;&nbsp;

00:12:23.760 --> 00:12:28.560
often find that models assume that all web search&nbsp;
results are true, right? They don't have any,&nbsp;&nbsp;

00:12:28.560 --> 00:12:32.640
you know, we we haven't told them explicitly that&nbsp;
this isn't the case. And so they might take these&nbsp;&nbsp;

00:12:32.640 --> 00:12:36.880
web results and run with them immediately. So,&nbsp;
one thing we prompted our models to do is to use&nbsp;&nbsp;

00:12:36.880 --> 00:12:41.920
this interleaf thinking to really reflect on the&nbsp;
quality of the search results and decide if they&nbsp;&nbsp;

00:12:41.920 --> 00:12:45.680
need to verify them, if they need to get more&nbsp;
information, or if they should add a disclaimer&nbsp;&nbsp;

00:12:45.680 --> 00:12:50.720
about how the results might not be accurate. Um,&nbsp;
another point with when prompting agents is that&nbsp;&nbsp;

00:12:50.720 --> 00:12:56.960
agents are more unpredictable than workflows&nbsp;
or just, you know, classification type prompts.&nbsp;&nbsp;

00:12:56.960 --> 00:13:02.000
Most changes will have unintended side effects.&nbsp;
This is because agents will operate in a loop&nbsp;&nbsp;

00:13:02.000 --> 00:13:07.840
autonomously. And so for example, if you tell the&nbsp;
agent, you know, keep searching until you find the&nbsp;&nbsp;

00:13:07.840 --> 00:13:12.400
correct answer, you know, find the highest quality&nbsp;
possible source and always keep searching until&nbsp;&nbsp;

00:13:12.400 --> 00:13:17.840
you find that source. What you might run into is&nbsp;
the unintended side effect of the agent just not&nbsp;&nbsp;

00:13:17.840 --> 00:13:22.640
finding any sources. Maybe this perfect source&nbsp;
doesn't exist for the for the query. And so it&nbsp;&nbsp;

00:13:22.640 --> 00:13:26.800
will just keep searching until it hits its context&nbsp;
window. And that's actually what we ran into as&nbsp;&nbsp;

00:13:26.800 --> 00:13:30.800
well. And so you have to tell the agent if you&nbsp;
don't find the perfect source, that's okay. You&nbsp;&nbsp;

00:13:30.800 --> 00:13:36.160
can stop after a few tool calls. Um, so just be&nbsp;
aware that your prompts may have unintended side&nbsp;&nbsp;

00:13:36.160 --> 00:13:41.280
effects and you may have to roll those back.&nbsp;
Another point is to help the agent manage its&nbsp;&nbsp;

00:13:41.280 --> 00:13:46.880
context window. The Cloud 4 models have a 200k&nbsp;
token context window. Um, this is long enough for&nbsp;&nbsp;

00:13:46.880 --> 00:13:51.920
a lot of longrunning tasks, but when you're using&nbsp;
an agent to do work autonomously, you may hit this&nbsp;&nbsp;

00:13:51.920 --> 00:13:56.080
context window and there are several strategies&nbsp;
you can use to sort of extend the effective&nbsp;&nbsp;

00:13:56.080 --> 00:14:00.960
context window. One of them that we use for cloud&nbsp;
code is called compaction. And this is just a tool&nbsp;&nbsp;

00:14:00.960 --> 00:14:08.000
that the model has um that will automatically be&nbsp;
called once it hits around 190,000 tokens. So near&nbsp;&nbsp;

00:14:08.000 --> 00:14:12.160
the context window. And this will summarize&nbsp;
or compress everything in the context window&nbsp;&nbsp;

00:14:12.160 --> 00:14:17.280
to a really dense but accurate summary that is&nbsp;
then passed to a new instance of claude with the&nbsp;&nbsp;

00:14:17.280 --> 00:14:22.160
summary. And it continues the process. And we find&nbsp;
that this essentially allows you to run infinitely&nbsp;&nbsp;

00:14:22.160 --> 00:14:27.040
with cloud code. You almost never run out of&nbsp;
context. um occasionally it will miss details&nbsp;&nbsp;

00:14:27.040 --> 00:14:31.680
from the previous session but the vast majority of&nbsp;
the time this will keep all the important details&nbsp;&nbsp;

00:14:31.680 --> 00:14:36.080
and the model will sort of remember what happened&nbsp;
in the last session. Similarly you can sort of&nbsp;&nbsp;

00:14:36.080 --> 00:14:41.920
write to an external file. So the model can have&nbsp;
access to an extra file and these cloud for models&nbsp;&nbsp;

00:14:41.920 --> 00:14:47.040
are especially good at writing memory to a file&nbsp;
and they can use this file to essentially extend&nbsp;&nbsp;

00:14:47.040 --> 00:14:52.720
their context window. Another point is that you&nbsp;
can use sub aents. Um, we won't talk about this&nbsp;&nbsp;

00:14:52.720 --> 00:14:57.840
a lot here, but essentially if you have agents&nbsp;
that are always hitting their context windows, you&nbsp;&nbsp;

00:14:57.840 --> 00:15:03.760
may delegate some of what the agent is doing to&nbsp;
another agent. Um, which can sort of, for example,&nbsp;&nbsp;

00:15:03.760 --> 00:15:08.720
you can have one agent be the lead agent and then&nbsp;
sub agents do the actual searching process. Then&nbsp;&nbsp;

00:15:08.720 --> 00:15:12.880
the sub agents can compress the results to the&nbsp;
lead agent in a really dense form that doesn't&nbsp;&nbsp;

00:15:12.880 --> 00:15:18.320
use as many tokens and the lead agent can give the&nbsp;
final report to the user. So we actually use this&nbsp;&nbsp;

00:15:18.320 --> 00:15:23.680
process in our research system and this allows you&nbsp;
to sort of compress what's going on in the search&nbsp;&nbsp;

00:15:23.680 --> 00:15:28.800
and then only use the context window for the lead&nbsp;
agent for actually writing the report. So this&nbsp;&nbsp;

00:15:28.800 --> 00:15:33.680
kind of multi- aent system can be effective&nbsp;
for limiting the context window. Finally,&nbsp;&nbsp;

00:15:33.680 --> 00:15:37.600
you can let Claude be Claude. And essentially&nbsp;
what this means is that Claude is great at being&nbsp;&nbsp;

00:15:37.600 --> 00:15:42.240
an agent already. You don't have to do a ton of&nbsp;
work at the very beginning. So, I would recommend&nbsp;&nbsp;

00:15:42.240 --> 00:15:46.880
just trying out your system with sort of a bare&nbsp;
bones prompt and barebones tools and seeing where&nbsp;&nbsp;

00:15:46.880 --> 00:15:51.200
it goes wrong and then working from there. Don't&nbsp;
sort of assume that Claude can't do it ahead of&nbsp;&nbsp;

00:15:51.200 --> 00:15:58.160
time because cloud often will surprise you with&nbsp;
how good it is. Um, I talked already about tool&nbsp;&nbsp;

00:15:58.160 --> 00:16:02.400
design, but essentially the key point here is you&nbsp;
want to make sure that your tools are good. Um,&nbsp;&nbsp;

00:16:02.400 --> 00:16:06.880
what is a good tool? It will have a simple&nbsp;
accurate tool name that reflects what it does.&nbsp;&nbsp;

00:16:06.880 --> 00:16:11.360
You'll have tested it and make sure that it works&nbsp;
well. um it'll have a well-formed description&nbsp;&nbsp;

00:16:11.360 --> 00:16:16.480
so that a human reading this tool like imagine&nbsp;
you give a function to another engineer on your&nbsp;&nbsp;

00:16:16.480 --> 00:16:21.440
team would they understand this function and be&nbsp;
able to use it. You should ask the same question&nbsp;&nbsp;

00:16:21.440 --> 00:16:27.040
about the agent computer interfaces or the tools&nbsp;
that you are giving your agent. Make sure that&nbsp;&nbsp;

00:16:27.040 --> 00:16:32.720
they're usable and clear. Um we also often find&nbsp;
that people will give an agent a bunch of tools&nbsp;&nbsp;

00:16:32.720 --> 00:16:38.080
that have very similar names or descriptions.&nbsp;
So for example, you give it six search tools&nbsp;&nbsp;

00:16:38.080 --> 00:16:42.800
and each of the search tools searches a slightly&nbsp;
different database. This will confuse the model.&nbsp;&nbsp;

00:16:42.800 --> 00:16:48.960
So try to keep your tools fairly distinct&nbsp;
um and combine similar tools into just one.&nbsp;&nbsp;

00:16:49.840 --> 00:16:53.920
So, one quick example here is just that you can&nbsp;
have an agent, for example, use these different&nbsp;&nbsp;

00:16:53.920 --> 00:17:00.080
tools to first search the inventory in a database,&nbsp;
run a query. Based on the information it finds, it&nbsp;&nbsp;

00:17:00.080 --> 00:17:05.440
can reflect on the inventory, think about it for&nbsp;
a little bit, then decide to generate an invoice,&nbsp;&nbsp;

00:17:05.440 --> 00:17:10.320
generate this invoice, think about what it should&nbsp;
do next, and then decide to send an email. And so,&nbsp;&nbsp;

00:17:10.320 --> 00:17:14.320
this loop involves the agent getting information&nbsp;
from the database, which is its external&nbsp;&nbsp;

00:17:14.320 --> 00:17:19.520
environment, using its tools, and then updating&nbsp;
based on that information. until it accomplishes&nbsp;&nbsp;

00:17:19.520 --> 00:17:24.480
the task. And that's sort of how agents work&nbsp;
in general. So, let's walk through a demo real&nbsp;&nbsp;

00:17:24.480 --> 00:17:30.560
quick. I'll switch to my computer. Um, so you can&nbsp;
see here that this is our console. The console is&nbsp;&nbsp;

00:17:30.560 --> 00:17:35.120
a great tool for sort of simulating your prompts&nbsp;
and seeing what they would look like in a UI. Um,&nbsp;&nbsp;

00:17:35.120 --> 00:17:40.080
and I use this while we were iterating on research&nbsp;
to sort of understand what's really going on and&nbsp;&nbsp;

00:17:40.080 --> 00:17:44.000
what the agents doing. This is a great way to&nbsp;
think like your agents and sort of put yourself&nbsp;&nbsp;

00:17:44.000 --> 00:17:48.960
in their shoes. So, you can see we have a big&nbsp;
prompt here. Um, it's not sort of super long.&nbsp;&nbsp;

00:17:48.960 --> 00:17:53.520
It's around a thousand tokens. It involves the&nbsp;
researcher going through a research process. We&nbsp;&nbsp;

00:17:53.520 --> 00:17:57.760
tell it exactly what should what it what it should&nbsp;
plan ahead of time. We tell it how many tool&nbsp;&nbsp;

00:17:57.760 --> 00:18:02.480
calls it should typically use. We give it some&nbsp;
guidelines about what facts it should think about,&nbsp;&nbsp;

00:18:02.480 --> 00:18:06.720
what makes a high quality source, stuff like&nbsp;
that. And then we tell it to use parallel tool&nbsp;&nbsp;

00:18:06.720 --> 00:18:12.080
calls. So, you know, run multiple web searches in&nbsp;
parallel at the same time rather than running them&nbsp;&nbsp;

00:18:12.080 --> 00:18:17.920
all sequentially. Then we give it this question.&nbsp;
How many bananas can fit in a Rivian R1S? This&nbsp;&nbsp;

00:18:17.920 --> 00:18:21.760
is not a question that the model will be able&nbsp;
to answer because the Rivian R1S came out very&nbsp;&nbsp;

00:18:21.760 --> 00:18:26.800
recently. It's a car. It doesn't know in advance&nbsp;
all the specifications and everything. So, it'll&nbsp;&nbsp;

00:18:26.800 --> 00:18:31.200
have to search the web. Let's run it and see what&nbsp;
happens. You'll see that at the very beginning,&nbsp;&nbsp;

00:18:31.200 --> 00:18:35.520
it will think and break down this request. And&nbsp;
so, it realizes, okay, web search is going to&nbsp;&nbsp;

00:18:35.520 --> 00:18:43.600
be helpful here. I should get cargo capacity.&nbsp;
I should search. Um, woo. Um, and you see here&nbsp;&nbsp;

00:18:43.600 --> 00:18:47.920
it ran two web searches in parallel at the same&nbsp;
time. That allowed it to get these results back&nbsp;&nbsp;

00:18:47.920 --> 00:18:52.480
very quickly. And then it's reflecting on the&nbsp;
results. So it's realizing, okay, I found the&nbsp;&nbsp;

00:18:52.480 --> 00:18:58.560
banana dimensions. I know that a USDA identifies&nbsp;
bananas as 7 to 8 in long. I need to run another&nbsp;&nbsp;

00:18:58.560 --> 00:19:03.200
web search. Let me convert these to more standard&nbsp;
measurements. You can see it's using tool calls&nbsp;&nbsp;

00:19:03.200 --> 00:19:07.440
interled with thinking, which is something&nbsp;
new that the quad 4 models can do. Finally,&nbsp;&nbsp;

00:19:07.440 --> 00:19:11.920
it's running some calculations. It's about how&nbsp;
many bananas could be packed into the cargo space&nbsp;&nbsp;

00:19:11.920 --> 00:19:28.560
of the truck. And it's running a few more web&nbsp;
searches. You can see here that this is a fairly

00:19:28.560 --> 00:19:31.600
pending

00:19:31.600 --> 00:19:37.520
approximately 48,000 bananas. I've seen the model&nbsp;
estimate anything between 30,000 50,000. I think&nbsp;&nbsp;

00:19:37.520 --> 00:19:45.920
the right answer is around 30,000. So this is this&nbsp;
is roughly correct. Um going back to the slides,&nbsp;&nbsp;

00:19:45.920 --> 00:19:51.360
I think that you know this this sort of approach&nbsp;
of testing out your prompt, seeing what tools&nbsp;&nbsp;

00:19:51.360 --> 00:19:55.760
the model calls, reading its thinking blocks,&nbsp;
and actually seeing how the model's thinking&nbsp;&nbsp;

00:19:55.760 --> 00:20:01.120
will often make it really obvious. um what the&nbsp;
issues are and what's going wrong. So you'll&nbsp;&nbsp;

00:20:01.120 --> 00:20:05.520
test it out and you'll just see like okay&nbsp;
maybe the model's using too many tools here,&nbsp;&nbsp;

00:20:05.520 --> 00:20:10.080
maybe it's using the wrong sources or maybe&nbsp;
it's just following the wrong guidelines. Um&nbsp;&nbsp;

00:20:10.080 --> 00:20:17.360
so this is a really helpful way to sort of think&nbsp;
like your agents and make them more concrete.

00:20:17.360 --> 00:20:23.760
Um switching back to the slides.

00:20:23.760 --> 00:20:29.360
Okay, so eval evaluations are really important&nbsp;
for any system. Um, they're really important&nbsp;&nbsp;

00:20:29.360 --> 00:20:34.240
for systematically measuring whether you're&nbsp;
making progress in your prompt. Very quickly,&nbsp;&nbsp;

00:20:34.240 --> 00:20:38.720
you'll notice that it's difficult to really make&nbsp;
progress on a prompt if you don't have an eval&nbsp;&nbsp;

00:20:38.720 --> 00:20:42.240
that tells you meaningfully whether your prompt is&nbsp;
getting better and whether your system is getting&nbsp;&nbsp;

00:20:42.240 --> 00:20:48.640
better. But eval are much more difficult for&nbsp;
agents. Um, agents are longunning. They do a bunch&nbsp;&nbsp;

00:20:48.640 --> 00:20:54.240
of things. They may not they may not always have&nbsp;
a predictable process. classification is easier to&nbsp;&nbsp;

00:20:54.240 --> 00:20:59.440
eval because you can just check did it classify&nbsp;
this output correctly but agents are harder. So&nbsp;&nbsp;

00:20:59.440 --> 00:21:04.640
a few tips to make this a bit easier. One is that&nbsp;
the larger the effect size the smaller the sample&nbsp;&nbsp;

00:21:04.640 --> 00:21:09.680
size you mean you need um and so this is sort of&nbsp;
just a principle from science in general where&nbsp;&nbsp;

00:21:09.680 --> 00:21:15.200
if an effect size is very large for example if a&nbsp;
medication will cure people immediately you don't&nbsp;&nbsp;

00:21:15.200 --> 00:21:19.920
really need a large sample size of a ton of people&nbsp;
to know that the model is that that this treatment&nbsp;&nbsp;

00:21:19.920 --> 00:21:24.800
is having an effect. Similarly, when you change a&nbsp;
prompt, if it's really obvious that the system is&nbsp;&nbsp;

00:21:24.800 --> 00:21:29.280
getting better, you don't need a large eval. I&nbsp;
often see teams think that they need to set up&nbsp;&nbsp;

00:21:29.280 --> 00:21:33.680
a huge eval of like hundreds of test cases and&nbsp;
make it completely automated when they're just&nbsp;&nbsp;

00:21:33.680 --> 00:21:38.400
starting out building an agent. This is a failure&nbsp;
mode and it's an antiattern. You should start out&nbsp;&nbsp;

00:21:38.400 --> 00:21:44.240
with a very small eval and just run it and see&nbsp;
what happens. You can even start out manually. Um,&nbsp;&nbsp;

00:21:44.240 --> 00:21:49.120
but the important thing is to just get started.&nbsp;
I often see teams delaying evals because they&nbsp;&nbsp;

00:21:49.120 --> 00:21:53.840
think that they're so intimidating or that they&nbsp;
need such a sort of intense eval to really get&nbsp;&nbsp;

00:21:53.840 --> 00:21:58.160
some signal, but you can get great signal from&nbsp;
a small number of test cases. You just want to&nbsp;&nbsp;

00:21:58.160 --> 00:22:02.720
keep those test cases s consistent and then keep&nbsp;
testing them so you know whether the model and&nbsp;&nbsp;

00:22:02.720 --> 00:22:07.920
the prompt is getting better. You also want to&nbsp;
use realistic tasks. So don't just sort of come&nbsp;&nbsp;

00:22:07.920 --> 00:22:12.560
up with arbitrary prompts or descriptions&nbsp;
or tasks that don't really have any real&nbsp;&nbsp;

00:22:12.560 --> 00:22:16.880
correlation to what your system will be doing.&nbsp;
For example, if you're working on coding tasks,&nbsp;&nbsp;

00:22:16.880 --> 00:22:21.120
you don't won't want to give the model just&nbsp;
competitive programming problems because this is&nbsp;&nbsp;

00:22:21.120 --> 00:22:25.840
not what real world coding is like. You'll want to&nbsp;
give it realistic tasks that really reflect what&nbsp;&nbsp;

00:22:25.840 --> 00:22:30.560
your agent will be doing. Similarly, in finance,&nbsp;
you'll want to sort of take tasks that real people&nbsp;&nbsp;

00:22:30.560 --> 00:22:35.680
are trying to solve and just use them to evaluate&nbsp;
whether the model can do those. This allows you&nbsp;&nbsp;

00:22:35.680 --> 00:22:40.400
to really measure whether the model is getting&nbsp;
better at the tasks that you care about. Another&nbsp;&nbsp;

00:22:40.400 --> 00:22:45.440
point is that LLM is judge is really powerful,&nbsp;
especially when you give it a rubric. So agents&nbsp;&nbsp;

00:22:45.440 --> 00:22:48.960
will have lots of different kinds of outputs.&nbsp;
For example, if you're using them for search,&nbsp;&nbsp;

00:22:48.960 --> 00:22:53.920
they might have tons of different kinds of search&nbsp;
reports with different kinds of structure. But LMS&nbsp;&nbsp;

00:22:53.920 --> 00:22:58.400
are great at handling lots of different kinds of&nbsp;
structure and text with different characteristics.&nbsp;&nbsp;

00:22:58.400 --> 00:23:02.960
And so one thing that we've done, for example,&nbsp;
is given the model just a clear rubric and then&nbsp;&nbsp;

00:23:02.960 --> 00:23:07.600
ask it to evaluate the output of the agent.&nbsp;
For example, for search tasks, we might give&nbsp;&nbsp;

00:23:07.600 --> 00:23:12.320
it a rubric that says, check that the model,&nbsp;
you know, um, looked at the right sources,&nbsp;&nbsp;

00:23:12.320 --> 00:23:17.120
check that it got the correct answer. In this&nbsp;
case, we might say, um, check that the model&nbsp;&nbsp;

00:23:17.120 --> 00:23:22.960
guessed that the amount of bananas that can fit&nbsp;
in a Rivian R1s is between like 10,000 and 50,000.&nbsp;&nbsp;

00:23:22.960 --> 00:23:27.840
Anything outside that range is not realistic. So,&nbsp;
you know, you can use things like that to sort of&nbsp;&nbsp;

00:23:27.840 --> 00:23:32.800
benchmark whether the model is getting the right&nbsp;
answers, whether it's following the right process.&nbsp;&nbsp;

00:23:33.440 --> 00:23:37.600
At the end of the day though, nothing is a perfect&nbsp;
replacement for human evals. You need to test the&nbsp;&nbsp;

00:23:37.600 --> 00:23:42.000
system manually. You need to see what it's doing.&nbsp;
You need to sort of look at the transcripts, look&nbsp;&nbsp;

00:23:42.000 --> 00:23:48.000
at what the model is doing, and sort of understand&nbsp;
your system if you want to make progress on it.&nbsp;&nbsp;

00:23:48.000 --> 00:23:53.360
Here are some examples of eval. So one example&nbsp;
that I sort of showed uh talked about is answer&nbsp;&nbsp;

00:23:53.360 --> 00:23:58.320
accuracy. And this is where you just use an LLM&nbsp;
as judge to judge whether the answer is accurate.&nbsp;&nbsp;

00:23:58.320 --> 00:24:02.880
So for example in this case you might say the&nbsp;
agent needs to use a tool to query the number&nbsp;&nbsp;

00:24:02.880 --> 00:24:06.960
of employees and then report the answer and then&nbsp;
you know the number of employees at your company.&nbsp;&nbsp;

00:24:06.960 --> 00:24:11.360
So you can just check that with an LM as judge.&nbsp;
The reason you use an LMS as judge here is because&nbsp;&nbsp;

00:24:11.360 --> 00:24:16.720
it's more robust to variations. For example, if&nbsp;
you're just checking for the integer 47 in this&nbsp;&nbsp;

00:24:16.720 --> 00:24:22.400
case in the output that is not very robust and&nbsp;
if the model says 47 as text you'll grade it&nbsp;&nbsp;

00:24:22.400 --> 00:24:27.200
incorrectly. So you want to use an LMS as judge&nbsp;
there to be robust to those minor variations.&nbsp;&nbsp;

00:24:27.200 --> 00:24:31.840
Another way you can eval agents is tool use&nbsp;
accuracy. Agents involve using tools in a&nbsp;&nbsp;

00:24:31.840 --> 00:24:36.480
loop. And so if you know in advance what tools&nbsp;
the model should use or how it should use them,&nbsp;&nbsp;

00:24:36.480 --> 00:24:41.680
you can just evaluate if it used the correct tools&nbsp;
in the process. For example, in this case, I might&nbsp;&nbsp;

00:24:41.680 --> 00:24:47.680
evaluate the agent should use web search at least&nbsp;
five times to answer this question. And so I could&nbsp;&nbsp;

00:24:47.680 --> 00:24:52.960
just check in the transcript programmatically did&nbsp;
the tool call for web search appear five times or&nbsp;&nbsp;

00:24:52.960 --> 00:24:57.600
not. Similarly, you might check in this case&nbsp;
for in response to the question book a flight,&nbsp;&nbsp;

00:24:57.600 --> 00:25:02.000
the agent should use the search flights tool&nbsp;
and you can just check that programmatically&nbsp;&nbsp;

00:25:02.000 --> 00:25:06.800
and this allows you to make sure that the right&nbsp;
tools are being used at the right times. Finally,&nbsp;&nbsp;

00:25:06.800 --> 00:25:12.080
a really good eval for agents is tobench. You&nbsp;
can sort of look this up. Towen is a sort of open&nbsp;&nbsp;

00:25:12.080 --> 00:25:17.600
source benchmark that shows that you can evaluate&nbsp;
whether agents reach the correct final state.&nbsp;&nbsp;

00:25:17.600 --> 00:25:23.200
So a lot of agents are sort of modifying a&nbsp;
database or interacting with a user in a way&nbsp;&nbsp;

00:25:23.200 --> 00:25:28.400
where you can say the model should always get to&nbsp;
this state at the end of the process. For example,&nbsp;&nbsp;

00:25:28.400 --> 00:25:35.280
if your agent is a customer service agent for&nbsp;
airlines and the user asks to change their flight&nbsp;&nbsp;

00:25:35.280 --> 00:25:40.000
at the end of the agentic process in response to&nbsp;
that prompt, it should have changed the flight in&nbsp;&nbsp;

00:25:40.000 --> 00:25:45.120
the database. And so you can just check at the end&nbsp;
of the agentic process, was the flight changed?&nbsp;&nbsp;

00:25:45.120 --> 00:25:49.520
was this row in the database changed to a&nbsp;
different date and that can verify that the&nbsp;&nbsp;

00:25:49.520 --> 00:25:53.920
agent is working correctly. This is really robust&nbsp;
and you can use it a lot in a lot of different use&nbsp;&nbsp;

00:25:53.920 --> 00:25:58.640
cases. For example, you can check that your&nbsp;
database is updated correctly. You can check&nbsp;&nbsp;

00:25:58.640 --> 00:26:03.360
that certain files were modified, things like&nbsp;
that as a way to evaluate the final state that&nbsp;&nbsp;

00:26:03.360 --> 00:26:12.440
the agent reaches. And that's it from us. Um,&nbsp;
we're happy to take your questions. [Applause]

00:26:17.440 --> 00:26:21.920
Can you talk about building prompts for agents?&nbsp;
Are you giving it kind of long longer prompts&nbsp;&nbsp;

00:26:21.920 --> 00:26:26.080
first and then iterating or you starting kind&nbsp;
of chunk by chunk? Uh what's that look like?&nbsp;&nbsp;

00:26:26.080 --> 00:26:32.240
And can you show sort of a little bit more on that&nbsp;
thought process? That's a great question. Um can&nbsp;&nbsp;

00:26:32.240 --> 00:26:37.520
I switch back to my screen actually? I just want&nbsp;
to sort of show the demo. Thank you. Um, yeah. So,&nbsp;&nbsp;

00:26:37.520 --> 00:26:41.520
you can see this is sort of a final prompt&nbsp;
that we've arrived at, but this is not where&nbsp;&nbsp;

00:26:41.520 --> 00:26:55.120
we started. I think the answer to your question&nbsp;
is that you start with a short simple prompt.

00:26:55.120 --> 00:27:00.160
Um, and I might just say search the web&nbsp;
aentically. I'll change this to a different&nbsp;&nbsp;

00:27:00.160 --> 00:27:07.040
question. Um, how good are the Cloud 4 models&nbsp;
and then we'll just run that. And so you'll&nbsp;&nbsp;

00:27:07.040 --> 00:27:11.280
want to start with something very simple and just&nbsp;
see how it works. You'll often find that Claude&nbsp;&nbsp;

00:27:11.280 --> 00:27:16.000
can do the task well out of the box. But if you&nbsp;
have more needs and you need it to operate really&nbsp;&nbsp;

00:27:16.000 --> 00:27:21.120
consistently in production, you'll notice edge&nbsp;
cases or small flaws as you test with more use&nbsp;&nbsp;

00:27:21.120 --> 00:27:25.920
cases. And so you'll sort of add those into the&nbsp;
prompt. So I would say building an agent prompt&nbsp;&nbsp;

00:27:25.920 --> 00:27:32.400
what it looks like concretely is start simple,&nbsp;
test it out, see what happens, iterate from there,&nbsp;&nbsp;

00:27:32.400 --> 00:27:37.760
start collecting test cases where the model fails&nbsp;
or succeeds and then over time try to increase the&nbsp;&nbsp;

00:27:37.760 --> 00:27:42.160
number of test cases that pass. Um, and the way&nbsp;
to do this is by sort of adding instructions,&nbsp;&nbsp;

00:27:42.160 --> 00:27:46.320
adding examples to the prompt. But you really&nbsp;
only do that when you find out what the edge&nbsp;&nbsp;

00:27:46.320 --> 00:27:52.640
cases are. And you can see that it thinks that&nbsp;
the models are indeed good. So that's great.&nbsp;&nbsp;

00:27:53.840 --> 00:27:58.160
when I do like normal prompting and it's not&nbsp;
agentic, uh I'll often give like a few shot&nbsp;&nbsp;

00:27:58.160 --> 00:28:01.440
example of like, hey, here's like input,&nbsp;
here's output. This works really well for&nbsp;&nbsp;

00:28:01.440 --> 00:28:06.320
like classification tasks, stuff like that, right?&nbsp;
Uh is there a parallel here in this like agentic&nbsp;&nbsp;

00:28:06.320 --> 00:28:10.800
world? Are you finding that that's ever helpful&nbsp;
or should I not think about it that way? That is&nbsp;&nbsp;

00:28:10.800 --> 00:28:16.640
a great question. Yeah. So should you include&nbsp;
fewshot examples in your prompt and sort of&nbsp;&nbsp;

00:28:16.640 --> 00:28:20.240
traditional prompting techniques involve like&nbsp;
giving the saying the model should use a chain&nbsp;&nbsp;

00:28:20.240 --> 00:28:24.960
of thought and then giving a few shot examples&nbsp;
like a bunch of examples to imitate. We find&nbsp;&nbsp;

00:28:24.960 --> 00:28:30.000
that these techniques are not as effective for&nbsp;
state-of-the-art frontier models and for agents.&nbsp;&nbsp;

00:28:30.000 --> 00:28:34.240
Um the main reason for this is that if you give&nbsp;
the model a bunch of examples of exactly what&nbsp;&nbsp;

00:28:34.240 --> 00:28:38.320
process it should follow, that just limits the&nbsp;
model too much. These models are smarter than&nbsp;&nbsp;

00:28:38.320 --> 00:28:43.040
you can predict and so you don't want to tell&nbsp;
them exactly what they need to do. Similarly,&nbsp;&nbsp;

00:28:43.040 --> 00:28:46.960
chain of thought has just been trained into the&nbsp;
models at this point. The models know to think&nbsp;&nbsp;

00:28:46.960 --> 00:28:52.800
in advance. They don't need to be told like use&nbsp;
chain of thought. But what we can do here is one&nbsp;&nbsp;

00:28:52.800 --> 00:28:57.040
you can tell the model how to use its thinking.&nbsp;
So you know I talked about earlier rather than&nbsp;&nbsp;

00:28:57.040 --> 00:29:00.560
telling the model you need to use a chain of&nbsp;
thought. It already knows that. You can just&nbsp;&nbsp;

00:29:00.560 --> 00:29:05.440
say use your thinking process to plan out your&nbsp;
search or to plan out what you're going to do&nbsp;&nbsp;

00:29:05.440 --> 00:29:10.400
in terms of coding. Reme or you can tell it to&nbsp;
remember specific things in its thinking process&nbsp;&nbsp;

00:29:10.400 --> 00:29:15.040
and that sort of helps the agent stay on track.&nbsp;
As far as examples go, um you'll want to give&nbsp;&nbsp;

00:29:15.040 --> 00:29:19.280
the model examples but not too prescriptive.&nbsp;
I think we are out of time, but you can come&nbsp;&nbsp;

00:29:19.280 --> 00:29:25.440
up to me personally and I'll talk to you all&nbsp;
after. Thanks. Thank you. Thanks for coming.

