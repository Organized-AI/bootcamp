# VibeCoders Bootcamp - All Video Transcripts

Generated: 2025-08-07 22:26

## Table of Contents

1. [Building AI agents with Claude in Google Cloud's Vertex AI](#1-building-ai-agents-with-claude-in-google-cloud's-vertex-ai)
2. [Building headless automation with Claude Code](#2-building-headless-automation-with-claude-code)
3. [Claude plays Pokemon](#3-claude-plays-pokemon)
4. [Claude Code best practices](#4-claude-code-best-practices)
5. [MCP 201](#5-mcp-201)
6. [MCP at Sourcegraph](#6-mcp-at-sourcegraph)
7. [Prompting 101](#7-prompting-101)
8. [Prompting for Agents](#8-prompting-for-agents)
9. [Spotlight on Canva](#9-spotlight-on-canva)
10. [Spotlight on Databricks](#10-spotlight-on-databricks)
11. [Spotlight on Manus](#11-spotlight-on-manus)
12. [Spotlight on Shopify](#12-spotlight-on-shopify)
13. [Startups building new products with Claude](#13-startups-building-new-products-with-claude)
14. [How students build with Claude](#14-how-students-build-with-claude)
15. [Vibe coding in prod](#15-vibe-coding-in-prod)
16. [Building Blocks for Tomorrow’s AI Agents](#16-building-blocks-for-tomorrow’s-ai-agents)
17. [Taking Claude to the Next Level](#17-taking-claude-to-the-next-level)
18. [Code with Claude Opening Keynote](#18-code-with-claude-opening-keynote)

---

## 1. Building AI agents with Claude in Google Cloud's Vertex AI

**Preview:**
> Kind: captions Language: en Hello everyone. Uh thank you for joining this uh&nbsp; this session. So in this session we are going to&nbsp;&nbsp; talk about how you can build uh AI agents uh using&nbsp; uh cloud on vertex AI. So before to start let's&nbsp;&nbsp; see the uh let's set the scene. So as you probably&nbsp; know like building AI agent is very powerful.&nbsp;&nbsp; with the II agents you can build such a cool&nbsp; applications but the reality is after you start&nbsp;&nbsp; developing an...

**Full Transcript:**

Kind: captions Language: en Hello everyone. Uh thank you for joining this uh&nbsp; this session. So in this session we are going to&nbsp;&nbsp; talk about how you can build uh AI agents uh using&nbsp; uh cloud on vertex AI. So before to start let's&nbsp;&nbsp; see the uh let's set the scene. So as you probably&nbsp; know like building AI agent is very powerful.&nbsp;&nbsp; with the II agents you can build such a cool&nbsp; applications but the reality is after you start&nbsp;&nbsp; developing and you know prototyping agents and&nbsp; let's assume that you are happy with what you&nbsp;&nbsp; built it's so hard to productionalize&nbsp; these agents right and the reason are&nbsp;&nbsp; essentially three uh so first of all you need to&nbsp; because uh right now to build agent you have so&nbsp;&nbsp; many frameworks that provides you know tools&nbsp; that provides uh capabilities that you can&nbsp;&nbsp; uh that you can use to enhance your agents like&nbsp; the landscape is so fragmented. So you need&nbsp;&nbsp; to figure it out how to integrate the different&nbsp; frameworks and different tools to make the system&nbsp;&nbsp; work. So the the other the other reason is let's&nbsp; assume that you are capable of building one agent&nbsp;&nbsp; or a multi- aent system with one framework but at&nbsp; the same time you want to use different framework&nbsp;&nbsp; together. It's not easy to um like make um make&nbsp; the communication happen between these two set of&nbsp;&nbsp; uh you know different agents. And then even let's&nbsp; assume that even if you're able to you know build&nbsp;&nbsp; agents uh create this network of of agents that&nbsp; are capable of communicating between them it's so&nbsp;&nbsp; hard to manage uh them in production because you&nbsp; need to take care of all the operation around the&nbsp;&nbsp; agents and the relative governance. So all the&nbsp; monitoring capabilities the logging capabilities&nbsp;&nbsp; that you need to implement on your agent they are&nbsp; very hard to uh be managed. Uh in this sense uh&nbsp;&nbsp; let's imagine that uh you we you will be able to&nbsp; have a toolkit that will allows you to standardize&nbsp;&nbsp; and develop your agent in a very efficient way&nbsp; and then together with this toolkit you you get&nbsp;&nbsp; a set of protocols that will allows your agent to&nbsp; consume uh tool and context with the M but at the&nbsp;&nbsp; same time connect with other agent in a seamless&nbsp; way. And third you will you will get an u agent&nbsp;&nbsp; platform that will allows you to deploy at scale&nbsp; these uh agent system and you know manage all the&nbsp;&nbsp; uh operations that are around these uh this new&nbsp; kind of application. So with these challenges&nbsp;&nbsp; in mind and these three you know um three main&nbsp; um reason that we want to address that's why we&nbsp;&nbsp; define our own agent stack on Google cloud and our&nbsp; agent stack is composed by four main components.&nbsp;&nbsp; So the first one is agent development kit which is&nbsp; a an open-source code first and developer friendly&nbsp;&nbsp; uh framework that will allows you to build&nbsp; evaluate and deploy your agent uh at scale. But&nbsp;&nbsp; in order to enhance your agent you have a you need&nbsp; a way to standardize the agent communicate with&nbsp;&nbsp; different tools as I saw you before. So to address&nbsp; these challenges of protocols, one one thing that&nbsp;&nbsp; we did when we designed agent development&nbsp; kit is made and is making it compatible with&nbsp;&nbsp; um um MCP. So probably you know what is MCP uh you&nbsp; already uh heard about it but with MCP essentially&nbsp;&nbsp; you will make the agent compatible uh with uh&nbsp; several tools and in general application will be&nbsp;&nbsp; uh you will provide your context to your&nbsp; application using LLMs on top of MCP.&nbsp;&nbsp; So we also introduce like this Vert.x AI engine&nbsp; engine which is essentially a managed platform&nbsp;&nbsp; that has been designed to deploy, manage and scale&nbsp; your AI agent in production and uh it takes care&nbsp;&nbsp; of all the those operational challenges and uh&nbsp; you know possible capabilities that you need&nbsp;&nbsp; uh in order to uh deploy your agent in production.&nbsp; And finally to address the challenges of&nbsp;&nbsp; uh allow communication between different agent um&nbsp; build with different frameworks we also introduce&nbsp;&nbsp; uh agentto agent protocol. So which is essentially&nbsp; you know uh open source uh an open source protocol&nbsp;&nbsp; that will allows you to create this seamless&nbsp; communication and collaboration between agents&nbsp;&nbsp; in whatever framework you build. So with this&nbsp; talk we so today we are going to use this talk to&nbsp;&nbsp; uh build uh multi- aent systems and but before&nbsp; to do that let me to introduce myself I'm Ian&nbsp;&nbsp; Ardini I'm a developer advocate at the Google&nbsp; cloud I'm based in sunny and uh today I want to&nbsp;&nbsp; go through this journey with you and the journey&nbsp; will starts with building a very simple ADK agents&nbsp;&nbsp; uh using cloud and then we are going to enhance&nbsp; these agents using uh some uh pre-built tools and&nbsp;&nbsp; MCP and finally Finally, we will deploy&nbsp; the agents on agent engine. As a bonus,&nbsp;&nbsp; we will try to cover which I will try also to show&nbsp; you how you can connect multiple agent using agent&nbsp;&nbsp; to agent protocol. Uh but in case we are not&nbsp; we we will not able to do that. Don't worry,&nbsp;&nbsp; we are going to have a live webinar at the end&nbsp; of the month. So we will show you how to do that&nbsp;&nbsp; later. With that being said, we want to build an&nbsp; agent. But to build an agent, we need an an LLM.&nbsp;&nbsp; So let me show you how you can get access to cloud&nbsp; models on vertex AI. So cloud model cloud models&nbsp;&nbsp; on vert.xai are accessible through vert.xi model&nbsp; garden which is essentially a centralized hub&nbsp;&nbsp; where you can discover, deploy and manage a wide&nbsp; variety of foundational and open models including&nbsp;&nbsp; uh cloud. So on uh on um model garden you will&nbsp; find the latest and greatest cloud model. This&nbsp;&nbsp; morning we just rolled out uh cloud uh 4. So I&nbsp; will show you and um after you simply you know&nbsp;&nbsp; fill um you know you provide some credential and&nbsp; everything you will get access to the model and&nbsp;&nbsp; you will able to use it through API or through the&nbsp; console. So without further ado let me show you&nbsp;&nbsp; how you can get access to cloud. So let's switch&nbsp; on the yes so for people that doesn't know vertex&nbsp;&nbsp; AI this is how the vertexi console looks like.&nbsp; So you vert.xi provide a set of services to build&nbsp;&nbsp; both generative AI and predictive AI application&nbsp; and model garden as I said is a centralized app&nbsp;&nbsp; that provides you several model from different you&nbsp; know model providers including cloud in including&nbsp;&nbsp; entropic. In fact in the uh partner session you&nbsp; will find the entropy models and here you can see&nbsp;&nbsp; all the entropy models that we provide including&nbsp; the latest that we released this morning. So in&nbsp;&nbsp; order to you can use model garden to test this&nbsp; model. So uh here is u the vert.xi um vert.xi&nbsp;&nbsp; studio which is our prompt UI that you can use for&nbsp; test this model. As you see I already select cloud&nbsp;&nbsp; 3.7 sonet which is the model that we are going to&nbsp; use today to build our agent. Uh we are already&nbsp;&nbsp; integrating cloud uh 4 with ADK. So stay tuned&nbsp; in the coming weeks. But through this UI what you&nbsp;&nbsp; can do you can test the model and uh you know you&nbsp; can start you can start interacting with it and uh&nbsp;&nbsp; using the API that you can get here to integrate&nbsp; with your uh application. So with that being said&nbsp;&nbsp; now that you know more or less how you know to get&nbsp; access to cloud through vertex AI let's go back&nbsp;&nbsp; to the presentation and let's start building&nbsp; agents using this model. So for the in this&nbsp;&nbsp; uh in this um in this demo we are going to build&nbsp; a very simple agent which is a birth uh birthday&nbsp;&nbsp; a birthday birthday planner agent. So uh we will u&nbsp; we will it's an agent that essentially will allows&nbsp;&nbsp; you to organize a birthday party such as in teams&nbsp; and you know getting the guest list and so on and&nbsp;&nbsp; uh in before to start this uh before to build this&nbsp; agent you need to know some concept related to&nbsp;&nbsp; ADK. Uh just one thing I know this uh this uh this&nbsp; session is supposed to be a workshop but because&nbsp;&nbsp; all the Wi-Fi issue that you've already faced I&nbsp; will uh I know we will uh we will already give you&nbsp;&nbsp; some credits and I will share the repository with&nbsp; you. So after this session you will be able to&nbsp;&nbsp; reproduce this code I'm going to show you at home&nbsp; and if you have question you can always come back&nbsp;&nbsp; to me. Okay, with that being said, these are the&nbsp; core concept that you need to know about ADK in&nbsp;&nbsp; order to build an agent with the agent development&nbsp; kit. First of all, agent development kit provides&nbsp;&nbsp; several type of agents that you can use. Uh&nbsp; you already pre-built some you know pattern&nbsp;&nbsp; uh so aenting pattern including sequential agents&nbsp; that you can use in order to implement your&nbsp;&nbsp; application. But the simplest pattern that you can&nbsp; find is the the one that we use with the LLM agent&nbsp;&nbsp; which essentially used just an LLM to feed uh to&nbsp; you know um build to use the agent to build the&nbsp;&nbsp; agent. And so uh the this this class represent&nbsp; the brain of the agent and it supports several&nbsp;&nbsp; models including claude uh claude and essentially&nbsp; it allowed uh it requires you to set the model&nbsp;&nbsp; give it uh the agent a name some instructions&nbsp; and define the tool that you want to use and&nbsp;&nbsp; then after you have done this you get your agent&nbsp; already up and running with respect of tools you&nbsp;&nbsp; know what is a tool is it's essentially a mean&nbsp; that you can use to you know assign some skills&nbsp;&nbsp; to the agent and um uh ADK A we provide some&nbsp; pre-build tools that you can use but you also can&nbsp;&nbsp; you can also define your own tools and integrate&nbsp; with the with the framework. So you have the&nbsp;&nbsp; agents, you have the tool in ADK. You have this&nbsp; concept of runner that puts together everything&nbsp;&nbsp; and coordinates um you know execute the agents. So&nbsp; it manage the session. So the conversation state&nbsp;&nbsp; along the uh while you're running the agents and&nbsp; it is integrated with a very nice CLI that you can&nbsp;&nbsp; see here uh ADK run and ADK web that will allows&nbsp; you to interact with the agent programmatically&nbsp;&nbsp; or you know through a web UI that I will show you&nbsp; later. And then uh last last important thing that&nbsp;&nbsp; I want to mention you have this concept of session&nbsp; which essentially will allows you to store uh the&nbsp;&nbsp; conversation and interact with the agent in a&nbsp; way that you know it remembers what uh what you&nbsp;&nbsp; already discussed with him before. Okay. So with&nbsp; that being said, I told you ADK support cloud how&nbsp;&nbsp; it is support cloud with two you can use cloud&nbsp; in two ways with ADK through the LL light LLM&nbsp;&nbsp; integration which is something that I will assume&nbsp; you're familiar with or you can use the pre-build&nbsp;&nbsp; integration that we provide as a vertx team uh&nbsp; using cloud and the LLM registry which is the&nbsp;&nbsp; one that I will show you uh today. It's just a&nbsp; nice way you know to integrate the model with the&nbsp;&nbsp; with the interface. So with that being said, let&nbsp; me show you how you can build um an agent using u&nbsp;&nbsp; using ADK. So this is the repository that uh&nbsp; you will u you will get once you uh download&nbsp;&nbsp; from once you get once you clone the repo from&nbsp; GitHub. So in the repository you will have three&nbsp;&nbsp; agents. We are going to cover them uh today. And&nbsp; the first one as I said is the birthday planner.&nbsp;&nbsp; So in order to build an agent with ADK all you&nbsp; need to do is providing essentially three file&nbsp;&nbsp; uh the agent.py PI which contain the agent logics&nbsp; uh the environment variable file which contains&nbsp;&nbsp; all the environment variable that you want to use&nbsp; for your agent and an init file as you probably&nbsp;&nbsp; are familiar with. So just these three file will&nbsp; allows you to run the agent and as you can see&nbsp;&nbsp; we designed ADK to be so close uh to software&nbsp; engineering best practices. So this is something&nbsp;&nbsp; that you should be capable of running easily. With&nbsp; that being said here you can see how you can use&nbsp;&nbsp; ADK. So you need to import the LLM agent class,&nbsp; the cloud class which is going to represent the&nbsp;&nbsp; cloud model that we are going to use today and uh&nbsp; then you can introduce uh you can also use some&nbsp;&nbsp; other classes related to memory the runner that&nbsp; I already explained. But with that being said&nbsp;&nbsp; once you get this uh once you import this class&nbsp; this is all the bullet plate code that you need&nbsp;&nbsp; to write in order to create your first agent. So&nbsp; you use the LLM agent class. You define a name,&nbsp;&nbsp; the model that you want to use, in this case the&nbsp; cloth 3.7, the description, so what the agent is&nbsp;&nbsp; going to do and the instruction that you want to&nbsp; give to the agent. That's it. Once you have this,&nbsp;&nbsp; you are ready to go. So all you need to do is that&nbsp; running if you want to interact with the agent in&nbsp;&nbsp; a programmatic way, you can run ad run and then&nbsp; behind the scene it will start a session with&nbsp;&nbsp; your agent. Oh, sorry, I forgot one thing. NDK&nbsp; run birthday and then uh it will run a session&nbsp;&nbsp; uh an interactive session with your agent. So from&nbsp; here you can start interacting with your agent and&nbsp;&nbsp; you can start you know understanding how it works&nbsp; and so in this way you can iteratively develop&nbsp;&nbsp; develop the agents. So and you can improve the&nbsp; agent depending on the task that you are trying&nbsp;&nbsp; to achieve. So again three files one CLI and&nbsp; you're done and you can you can start you know&nbsp;&nbsp; uh improving your agents. So let's go back to&nbsp; the slide. Okay. So let's assume that uh you&nbsp;&nbsp; know you clone the repo, you get your agent up and&nbsp; running. Uh let's make things a little bit more&nbsp;&nbsp; complicated. So we want to extend our agents uh in&nbsp; a way that it becomes a multi- aent system. So we&nbsp;&nbsp; have this agent that it will give us suggestion&nbsp; for the birthday party. But then once we get the&nbsp;&nbsp; birthday party, we want also you know to schedule&nbsp; some time in our agenda for example for going and&nbsp;&nbsp; buy the gift for the party or you know just&nbsp; setting a reminder of the birthday day. So&nbsp;&nbsp; how you do that you do uh you introduce you know&nbsp; tools and uh the cool thing of ADK is that we we&nbsp;&nbsp; didn't want to reinvent the wheel. So we uh we by&nbsp; day zero we introduced this integration with MCP.&nbsp;&nbsp; So again I'm not going to explain you what uh it&nbsp; is MCP and the difference between you know the&nbsp;&nbsp; language specific tools or the API. The idea is&nbsp; essentially with MCP you standardize the way LLM&nbsp;&nbsp; u get access to the context not only LLM but&nbsp; also but also agents. Uh with ADK you have two&nbsp;&nbsp; ways to use uh MCP. So you can use MCP uh some MCP&nbsp; existing uh server and uh you know integrate them&nbsp;&nbsp; as a tool with ADK. This is something that we are&nbsp; going to do today. So whatever MCP server is out&nbsp;&nbsp; there you can use just uh like you can use today&nbsp; already with the ADK without you reinventing you&nbsp;&nbsp; know the wheel in that sense or if you have a ADK&nbsp; and you build some tool in ADK you can use MCP to&nbsp;&nbsp; deploy this tool and interact with other agents.&nbsp; So these are the two ways that you have uh that&nbsp;&nbsp; you can use to leverage NCP with ADK. So with that&nbsp; being said uh let me show you how you can use uh&nbsp;&nbsp; ADK with MCP. So let's go back here. Let me exit&nbsp; to this agent and then let's go to So this is the&nbsp;&nbsp; second agent. So again as I said now we want to&nbsp; what we want to do is that we want to introduce&nbsp;&nbsp; um a calendar service agent which will allows&nbsp; me to schedule some time in my in my agenda and&nbsp;&nbsp; because now we have two agents the birthday one&nbsp; and the calendar one we want to also introduce&nbsp;&nbsp; an orchestrator which route my you know request&nbsp; to the right agent depending on what uh I want to&nbsp;&nbsp; achieve. So in this particular case the birthday&nbsp; planner is exactly the same agent that we defined&nbsp;&nbsp; before except that now I want to create an IB&nbsp; system um because for example like for scheduling&nbsp;&nbsp; for some for getting some birthday idea I can use&nbsp; also a very you know I can use also a different&nbsp;&nbsp; model like Gemini but then I have these calendar&nbsp; agents that in this case we use again cloud 3.5&nbsp;&nbsp; with an NCP server to schedule some time in my&nbsp; agenda. So in order to use an MCP server with ADK,&nbsp;&nbsp; these are the two line of codes that you need&nbsp; to uh introduce. So you get um you you get to&nbsp;&nbsp; the MCP server that you already have out there&nbsp; or you already created right or deployed as a&nbsp;&nbsp; as a serverless service and then you create a&nbsp; connection with it and then what happened behind&nbsp;&nbsp; the scene when you start building your agent when&nbsp; you run this command and you start building your&nbsp;&nbsp; agent what it does it like get all the information&nbsp; all the requirements to run your MCP server it&nbsp;&nbsp; converts these MCP servers as a tool and he&nbsp; use these MCP servers as a tool of the agent&nbsp;&nbsp; That's it. But again, the the cool thing, what I&nbsp; really believe is powerful of ADK is that it will&nbsp;&nbsp; allows me with two line of codes to integrate&nbsp; any kind of NCP tool that you have already.&nbsp;&nbsp; Once you have this MCP tool, you integrate it&nbsp; as a tool again in the our agent and you're&nbsp;&nbsp; done. Same similar things. Uh so now we have the&nbsp; birthday agent, we have the calendar agent. This&nbsp;&nbsp; is how the or the organizer look like. So look&nbsp; at how easy it is to pass multiple agents in a&nbsp;&nbsp; uh in an orchestrator like this one. Again you all&nbsp; you need to do is defining a better instruction&nbsp;&nbsp; because in this case this agent is going to&nbsp; orchestrate a multi- aent system. So you will&nbsp;&nbsp; define what agent like what what each agent is&nbsp; capable of doing and then you pass all the agent&nbsp;&nbsp; as a tool in this orchestrator. So again it will&nbsp; figure it out what agent to use depending on your&nbsp;&nbsp; request. Once you have done this, you are good&nbsp; to go. So what we can do is that running u uh going back here [Music] local actually let me do this. Let me&nbsp; show you this. So before I show you how you can&nbsp;&nbsp; interact uh I can spin up an agent interactive&nbsp; programmatic programmatically. But because now&nbsp;&nbsp; this system is more complicated. We have three&nbsp; agents, right? We want something more a little&nbsp;&nbsp; bit more solid to to try to understand what is&nbsp; happening behind the scene. So in ADK you have&nbsp;&nbsp; this uh web UI which allows you to um debug and&nbsp; interact uh interact with your agent. So this is&nbsp;&nbsp; uh the web UI. So in this case this is how it&nbsp; looks like. So the web UI we select the agent&nbsp;&nbsp; that I want to run and this is uh so in this case&nbsp; it's like what we did before except that now we&nbsp;&nbsp; have the um we have the other agents we have the&nbsp; multi- aent system that is running behind the&nbsp;&nbsp; scene and as you can see here this UI will nicely&nbsp; provides you a way to see what is happening behind&nbsp;&nbsp; the scene with your agent. So while you are uh&nbsp; while you're running the conversation with it,&nbsp;&nbsp; you will see which agent is using for&nbsp; doing what. Okay, with that being said,&nbsp;&nbsp; so now you know also the web UI. Let's go back on&nbsp; the on the presentation. Thank you. So uh let's&nbsp;&nbsp; um for the last part of this presentation, I want&nbsp; to show you also how you can easily deploy uh the&nbsp;&nbsp; uh the agent on agent engine. So in order to do&nbsp; that um let me do this. Yeah. In order to do that&nbsp;&nbsp; um let me first introduce you what what is an&nbsp; a what is why why you need an agent engine like&nbsp;&nbsp; this one. Essentially when uh when you need to&nbsp; deploy agent at scale in order to do that you&nbsp;&nbsp; need to figure it out a lot of complexity right&nbsp; you need to get your agent code you need to&nbsp;&nbsp; uh um you know wrap the agent in one of those&nbsp; services like fast API or jungo you need to&nbsp;&nbsp; build your container and then you know you need&nbsp; to figure it out your environment to run it in&nbsp;&nbsp; this case it can be a GCP environment and then uh&nbsp; you need to uh handle all the operation related to&nbsp;&nbsp; infrastructure and at the same time you also also&nbsp; need to monitor this agent because at the end of&nbsp;&nbsp; the day is a is an application right. So with the&nbsp; agent engine what uh you can simply deploy the&nbsp;&nbsp; agent using a method like agent engine create and&nbsp; you will get your agent up and running as well as&nbsp;&nbsp; all these observability um all the observability&nbsp; capabilities and the monitoring that you need in&nbsp;&nbsp; order to deploy your agent. they are directly&nbsp; managed by the platform itself and uh also all&nbsp;&nbsp; the interaction that you have with the agents they&nbsp; are going to be automatically uh collected by our&nbsp;&nbsp; logging system and you will directly use them to&nbsp; run some evaluation in a way that you know you&nbsp;&nbsp; can keep improving your agent along time. So these&nbsp; are like this gives you an idea of the reason why&nbsp;&nbsp; you want to consider an agent engine and this give&nbsp; you the picture the overall picture of the agent&nbsp;&nbsp; of vertxi agent engine. So in this picture as you&nbsp; can see agent engine is capable of integrating you&nbsp;&nbsp; know any kind of agent framework uh ADK as you as&nbsp; a as I just said but if you build agent with lang&nbsp;&nbsp; graph lchain you can you can do that you can use&nbsp; those framework as well and then um with whatever&nbsp;&nbsp; tools and whatever model that you want and the&nbsp; agent engine will take care of deploying your&nbsp;&nbsp; agents and we'll enable all these observability&nbsp; uh capabilities or features that you need using&nbsp;&nbsp; some cloud tools and uh the evaluation part is&nbsp; also covered by one of our services which is the&nbsp;&nbsp; vert.x AI evaluation service. So to wrap up like&nbsp; the agent engine capabilities. So you can deploy&nbsp;&nbsp; any uh agent that like uh you can define agent&nbsp; in any framework that you want. You can use this&nbsp;&nbsp; uh manage runtime to deploy these agents and then&nbsp; you will automatically get you will automatically&nbsp;&nbsp; be able to observe the behavior of the agent. call&nbsp; the agent at scale and we uh the agent engine uh&nbsp;&nbsp; uh it also has an integration with another with&nbsp; another services that we provide on Google cloud&nbsp;&nbsp; which is a agent space which I'm not going to&nbsp; cover today but just to give an idea it's the&nbsp;&nbsp; gate that will allows your agent to go in the&nbsp; ends of business. So really you know have an&nbsp;&nbsp; impact of the agents that you're going to build in&nbsp; an enterprise context. But with that being said,&nbsp;&nbsp; uh let me jump in the last lab that we are&nbsp; going to cover today. So I already show you&nbsp;&nbsp; um how you can build the agent. So in this last&nbsp; lab what I want to show you is how you can easily&nbsp;&nbsp; deploy an agent with a few line of codes. So&nbsp; in the repository you will find this uh this&nbsp;&nbsp; uh module that essentially will allows you to&nbsp; iteratively deploy your agents. All you need to&nbsp;&nbsp; do to deploy an agent on vertxi agent engine&nbsp; is providing the base requirements that your&nbsp;&nbsp; agent needs in order to run and then as I said&nbsp; we provide already a class that will allows you&nbsp;&nbsp; to create an agent endpoint in this case on&nbsp; the agent engine. So in this class you have&nbsp;&nbsp; your agent that you define. In this case we are&nbsp; going to deploy the first agent the birth planner&nbsp;&nbsp; agent. And then here you have the requirements.&nbsp; You can provide extra packages if you want. But&nbsp;&nbsp; then again few line of codes to deploy your agent&nbsp; in in a in a manager in a managed service that is&nbsp;&nbsp; scalable and will allows you to open your agent to&nbsp; several users. So with that being said, let me run&nbsp;&nbsp; this script. So first of all, let me close this&nbsp; session. Clear. Then let me go in the repository ls ls and then here I have my module. So&nbsp; in this case I do python deploy agent. So what happened behind the scene is that it&nbsp; will start uh deploying my agent. So you can&nbsp;&nbsp; monitor the deploy on the agent directly in&nbsp; the Vert.exi console. Now this step is going&nbsp;&nbsp; to get some time as you can imagine because it's&nbsp; building the image and deploying the agents. So&nbsp;&nbsp; let me directly jump into the UI. So once you once&nbsp; the deployment of the agent will successfully run,&nbsp;&nbsp; what you will do is uh you will get an entry in&nbsp; the Vert.Ex AI agent engine UI and from this UI&nbsp;&nbsp; you will be able to monitor this agent. So the&nbsp; query that it receives uh the latency that uh it&nbsp;&nbsp; takes so how how long it takes to respond to the&nbsp; query and you will also monitor you know the CPU&nbsp;&nbsp; and the memory that the agent is using. So you&nbsp; can better understand if um you allocate enough&nbsp;&nbsp; uh resources to serve this agent at scale.&nbsp; The engine is is also manage session. So in&nbsp;&nbsp; this case I just deployed one. So we don't start a&nbsp; session yet. But here you will see the session and&nbsp;&nbsp; uh it will gives you all the information that you&nbsp; need in order you know to integrate this agent&nbsp;&nbsp; in application both in a real time or streaming&nbsp; depending on the method that you want to use and&nbsp;&nbsp; you can always check the details of the of the&nbsp; deployment. Okay. So let's go. So now you have&nbsp;&nbsp; also an idea how to deploy uh the agent. Let's&nbsp; go back to slide. Thank you. So as I said this&nbsp;&nbsp; was a bonus part. I don't think we are going to co&nbsp; we have time to cover it but what I want to tell&nbsp;&nbsp; you is that let's assume that you build your&nbsp; agent you deploy it on agent engine right and&nbsp;&nbsp; u right now we build all our agent using just ADK&nbsp; but what if you want to deploy or build your agent&nbsp;&nbsp; build and deploy your agent using lchain crew&nbsp; AI or whatever framework as I already said agent&nbsp;&nbsp; engine support this but what the the main problem&nbsp; is that you you don't have a way to connect these&nbsp;&nbsp; agents that are built with different framework&nbsp; together Right. So that's when you need a protocol&nbsp;&nbsp; to do that. So in a world where you have you&nbsp; are going to have multiple agents that they are&nbsp;&nbsp; uh they are uh built and deployed with different&nbsp; framework there is this need to find a common&nbsp;&nbsp; language between these agent to interact to&nbsp; interact and collaborate in order to achieve some&nbsp;&nbsp; task and that's why as a Google cloud we introduce&nbsp; uh agent to agent protocol. So again, it's an open&nbsp;&nbsp; protocol that has been designed to uh enhance to&nbsp; foster the agent collaboration using very simple&nbsp;&nbsp; um uh concept that I will show you in a minute.&nbsp; But the key thing that I want to share with you is&nbsp;&nbsp; that has been already designed to to be enterprise&nbsp; ready. So it has a a bunch of features that will&nbsp;&nbsp; allows you to govern and uh in a secure way&nbsp; your agents and we again also in this case we&nbsp;&nbsp; didn't invent the wheel because it's based on some&nbsp; standard protocol HTTP JSON RCP something that is&nbsp;&nbsp; common adopted in the industry. The concept that&nbsp; you need to know about is the concept of agent&nbsp;&nbsp; skills. So which essentially describe the function&nbsp; or the capability of the agents and it's it's a&nbsp;&nbsp; like a business card of your agent with respect&nbsp; to other agents and then you have u the the sorry&nbsp;&nbsp; the the agent skill describe what the agent is&nbsp; capable of doing. So uh it manage the function&nbsp;&nbsp; that the agent has and then you have the agent&nbsp; card that essentially is a a digital business&nbsp;&nbsp; card for the agent will allow other agent or other&nbsp; application to know what the skills what are the&nbsp;&nbsp; skills of the agent and how to interact with&nbsp; it. So one is describe the agent the other one&nbsp;&nbsp; describe what is the agent capable of doing to the&nbsp; other agents and then as before you have an agent&nbsp;&nbsp; executor that essentially manage the communication&nbsp; the request and and the response that this system&nbsp;&nbsp; generates between agents. So these three concept&nbsp; with this three concept you can build system like&nbsp;&nbsp; this one where you will you will essentially&nbsp; have multiple agents uh written with different&nbsp;&nbsp; framework communicating between each other in&nbsp; order to achieve a particular and more complex&nbsp;&nbsp; task rather than the one we build today of you&nbsp; know uh scheduling or buying a birthday gift. So&nbsp;&nbsp; we are not going to cover this today but again as&nbsp; I said at the beginning we are going to have a web&nbsp;&nbsp; uh live webinar at the end of the month. So I&nbsp; will share with you the QR code. So just recap,&nbsp;&nbsp; we start from these three main problems, right?&nbsp; Building agent agent that is powerful but there&nbsp;&nbsp; are several challenges when you want to put them&nbsp; in production. You have a fragmented landscape.&nbsp;&nbsp; Uh there are some integration complexity that&nbsp; you need to manage. And even if you're capable&nbsp;&nbsp; of fixing this, you have to manage all the&nbsp; operational overhead that uh you need to you&nbsp;&nbsp; need to handle in order to deploy these agents.&nbsp; And then that's when you want to enable like you&nbsp;&nbsp; want to get access to a toolkit protocols and&nbsp; engine platform that at the end it allows you&nbsp;&nbsp; to standardize the way you build your agent and&nbsp; scale them to production and to give you this&nbsp;&nbsp; kind of tool. We put together this agentic stack&nbsp; using ADK, MCP, agent engine and entway that will&nbsp;&nbsp; essentially allows you to confidently build um a&nbsp; gentic system and scale them in uh in production&nbsp;&nbsp; as uh required. Okay. So scanner alert. So uh&nbsp; please get your phone out. I'm going to share with&nbsp;&nbsp; you some useful uh uh uh circ codes. So the first&nbsp; one that I want to share with you is code. So in&nbsp;&nbsp; this in this repository you will find all the code&nbsp; related to ADK. So samples you know getting start&nbsp;&nbsp; everything you will find here. Three two one.&nbsp; Okay. And then if you want to know how if you want&nbsp;&nbsp; I mean we covered this in 30 minutes but it can&nbsp; be like a onehour workshop. So here you can find a&nbsp;&nbsp; webinar we are going to run together with entropic&nbsp; next month and where we show also the integration&nbsp;&nbsp; with gateway. So please scan this code. Three,&nbsp; two, one. Okay. And then I I mean I was fast. So&nbsp;&nbsp; I I I I assume that you have uh several questions.&nbsp; So feel free to reach out. I'm always helpful&nbsp;&nbsp; happy to answer your questions. But with that&nbsp; being said, I hope you enjoyed the session. I am&nbsp;&nbsp; just 20 seconds late. So, I hope you enjoyed and&nbsp; yeah, thank you for uh attending this [Applause]

---

## 2. Building headless automation with Claude Code

**Preview:**
> Kind: captions Language: en Good afternoon everybody. My name is Sedara. Um&nbsp; I am an engineer on the cloud code team. Um and&nbsp;&nbsp; today we're going to be talking a little bit&nbsp; about uh the cloud code SDK and uh the cloud&nbsp;&nbsp; GitHub action that was just announced today.&nbsp; Um cool. So a little bit about the agenda.&nbsp;&nbsp; uh we do a little quick start for the SDK&nbsp; uh just to give some examples of how to get&nbsp;&nbsp; started and how to use the SDK. Uh we wi...

**Full Transcript:**

Kind: captions Language: en Good afternoon everybody. My name is Sedara. Um&nbsp; I am an engineer on the cloud code team. Um and&nbsp;&nbsp; today we're going to be talking a little bit&nbsp; about uh the cloud code SDK and uh the cloud&nbsp;&nbsp; GitHub action that was just announced today.&nbsp; Um cool. So a little bit about the agenda.&nbsp;&nbsp; uh we do a little quick start for the SDK&nbsp; uh just to give some examples of how to get&nbsp;&nbsp; started and how to use the SDK. Uh we will&nbsp; then dive into uh a live demo of the GitHub&nbsp;&nbsp; action which should be fun. Uh the GitHub action&nbsp; was built on top of the SDK. So it's meant to be&nbsp;&nbsp; um a source of inspiration for the kind of things&nbsp; that you can do using the cloud code SDK. Uh we'll&nbsp;&nbsp; then dive into some uh more advanced features of&nbsp; the SDK. Um, and then we'll end with having all&nbsp;&nbsp; of you set up uh the the cloud GitHub uh GitHub&nbsp; action on your repo so you guys can start using&nbsp;&nbsp; it and build on top of it. Um, cool. Actually,&nbsp; before we get started, uh, can I get a show of&nbsp;&nbsp; hands here? How many people have used cloud code?&nbsp; Okay, it's a lot of people. And, uh, of the people&nbsp;&nbsp; who have used cloud code, uh, how many have used&nbsp; cla or know what that is? Okay. Okay. far fewer&nbsp;&nbsp; people. Uh it's good to know. Um if you guys&nbsp; don't have cloud code installed in your laptop,&nbsp;&nbsp; uh that's how you get it. Uh I'd encourage you&nbsp; to to install it in your laptops and follow&nbsp;&nbsp; along. Uh there will be parts of this uh this&nbsp; talk that will be beneficial to follow along&nbsp;&nbsp; and then if you don't want to, you don't have to.&nbsp; It's all good. Uh cool. So what is the cloud code&nbsp;&nbsp; SDK? It is a way to programmatically access the&nbsp; power of the cloud code agent in headless mode.&nbsp;&nbsp; Uh this is powerful because uh it's a new kind&nbsp; of primitive and a new kind of building block&nbsp;&nbsp; that allows you to build applications that just&nbsp; weren't possible before. Um things that you can&nbsp;&nbsp; do with with the with the SDK are like super&nbsp; simple things to get started. Like for example,&nbsp;&nbsp; you can use it like a Unix tool. Uh the Unix the&nbsp; Unixish tool philosophy is what really makes cloud&nbsp;&nbsp; code powerful because you can plug it in anywhere&nbsp; where you can run bash or a terminal. Uh so you&nbsp;&nbsp; can like use it in your in your Unix pipelines. Uh&nbsp; you can pipe stuff into it, pipe stuff out of it,&nbsp;&nbsp; have like make like complex chains out of it and&nbsp; stuff like that. You can then build CI automation&nbsp;&nbsp; on it. So you can have Claude review your code.&nbsp; Some people actually get Claude to write new&nbsp;&nbsp; llinters for them too. So like Claude can lint&nbsp; your code. If there are specific things that you&nbsp;&nbsp; can't define programmatically, you can get Claude&nbsp; code to do it. Uh and then we get into fancier&nbsp;&nbsp; applications as well. So if you want to build&nbsp; your own chatbot that's powered by cloud code,&nbsp;&nbsp; that's certainly possible. Um if you want&nbsp; cloud code to write you code in like a um a new&nbsp;&nbsp; environment or a separate remote environment, uh&nbsp; you can build those kinds of applications as well.&nbsp;&nbsp; Um and finally, the these are a few features.&nbsp; We'll talk more about the features in the coming&nbsp;&nbsp; slides. Uh and we have Python and Typescript SDKs&nbsp; or like bindings for the cloud code SDK coming up&nbsp;&nbsp; soon. Uh so that should make it much easier for&nbsp; you guys to consume it and build on top of it.&nbsp;&nbsp; Uh so let's jump into some basic examples. Uh&nbsp; calling claude the cloud code SDK is as simple&nbsp;&nbsp; as just doing claude-b and following it up with&nbsp; the string that you want to ask claude. So in&nbsp;&nbsp; this example uh I'm telling claude to write me&nbsp; a Fibonacci sequence generator. Um and if you&nbsp;&nbsp; notice I also give it a d-allow tools write which&nbsp; is uh a way for me to proactively give it access&nbsp;&nbsp; to the right tool so it can write files to my&nbsp; file system. Um, and then this is something I&nbsp;&nbsp; like doing too. Uh, piping logs to claude. So you&nbsp; can do cat log cat app.log and then pipe that into&nbsp;&nbsp; cloud-b looking at logs manually. So this is&nbsp; something I do quite often. Um, and as you can&nbsp;&nbsp; see, it does a pretty decent job of summarizing&nbsp; what the log failures were. Uh, similarly, uh, if&nbsp;&nbsp; you're anything like me, I just can't get myself&nbsp; to understand the output of if config. I still&nbsp;&nbsp; don't know what it means, but Claude does and&nbsp; Claude does it for me over here. Um, and finally,&nbsp;&nbsp; this is uh this is kind of what makes the SDK&nbsp; tick, right? It is this is a we have an output&nbsp;&nbsp; format. If you do d-output format JSON, uh, cloud&nbsp; code will actually output things or its response&nbsp;&nbsp; in JSON as opposed to plain text. And, uh, you&nbsp; can parse this JSON and build on top of it. Um, so&nbsp;&nbsp; we we'll talk more about details for how this is&nbsp; what what else you can do with this JSON, but uh&nbsp;&nbsp; I wanted to throw that example out there. Uh let's&nbsp; get into a significantly more complex example now&nbsp;&nbsp; uh which is the uh the cloud GitHub action. Uh so&nbsp; cloud GitHub action was built on top of the SDK&nbsp;&nbsp; um and it can be used to uh review code. It can&nbsp; be used to create new features. It can be used to&nbsp;&nbsp; u triage bugs and so on. Um and uh this is also&nbsp; open source. So I'll include a link at the very&nbsp;&nbsp; end of the talk. Uh so you guys can go have a look&nbsp; at the source for inspiration for how to use it.&nbsp;&nbsp; Uh but for now let's jump into a live demo on my&nbsp; laptop. So I have cloned a popular uh small like&nbsp;&nbsp; uh open- source quiz app for this for the purposes&nbsp; of this demo. Um and we are going to fire it up&nbsp;&nbsp; just to see how that works and then we will tell&nbsp; cloud to build something on top of it for us. So,&nbsp;&nbsp; I just did an npm start, which opened up my my&nbsp; shiny new quiz app. It's actually pretty nifty. It&nbsp;&nbsp; allows you to like choose a bunch of categories.&nbsp; Uh how many questions you want, difficulty,&nbsp;&nbsp; definitely easy for me. I suck at trivia. Um&nbsp; type of questions. And then there's like a&nbsp;&nbsp; countdown timer. So, we're not going to actually&nbsp; answer these unless someone feels very strongly,&nbsp;&nbsp; please shout out the answer. But I'm just going&nbsp; to just fly through these uh just to show you&nbsp;&nbsp; guys how this how this like little quiz app works.&nbsp; There we go. Uh not surprising. We got a great F,&nbsp;&nbsp; but that's okay. Um we So this was the little like&nbsp; demo quiz app that that's open sourced. And if we&nbsp;&nbsp; look at the issues for for this uh for this um uh&nbsp; repo, we see a couple very interesting ones. Um,&nbsp;&nbsp; there's one issue that says, uh, we should add&nbsp; powerups for 50/50 elimination of options and skip&nbsp;&nbsp; questions for free. Um, because I suck at trivia,&nbsp; I really like that feature and I want to build&nbsp;&nbsp; it. Uh, and I before before this presentation, I&nbsp; already installed cloud the cloud GitHub action&nbsp;&nbsp; on my repo. So, it it's already available. Um, but&nbsp; we'll go over like how to set that up uh uh later&nbsp;&nbsp; too. Um, okay. So, here's the issue. Um it has&nbsp; pretty sparse details on how to implement this.&nbsp;&nbsp; It's just literally a feature a wish list really&nbsp; like a a wish feature. It's saying uh add a power&nbsp;&nbsp; up option in the config 50/50 elimination for the&nbsp; skip question. It should avoid user points even&nbsp;&nbsp; even though the user the question was skipped. Uh&nbsp; and user should be able to configure this from the&nbsp;&nbsp; config page. So there's a lot of like creative&nbsp; room for claw to do whatever it wants to do&nbsp;&nbsp; uh in this case and I'm excited to see what it&nbsp; actually ends up building. So what I'm going to&nbsp;&nbsp; do is say at cloud please implement this feature&nbsp; and comment on it. So it usually does take a four&nbsp;&nbsp; or five seconds for it to respond. And while it's&nbsp; doing that for good measure we'll just also take&nbsp;&nbsp; this other GitHub issue. Uh this is talking about&nbsp; uh a per question timer. So we saw there was like&nbsp;&nbsp; a global timer on the quiz app but there was no&nbsp; per question timer. Uh so that's what this one's&nbsp;&nbsp; uh talking about. So let's go and say cloud please&nbsp; build this. And now we have two things building.&nbsp;&nbsp; Um cool. So now when I get back to this tab you&nbsp; see that claude responded with a comment on this&nbsp;&nbsp; GitHub issue. Uh saying that it's working. Uh&nbsp; it also has a link to the job run which is the&nbsp;&nbsp; GitHub action run. If I click into it and if&nbsp; I actually like click on the logs, I'll see&nbsp;&nbsp; that it's doing a bunch of stuff. You can see all&nbsp; this JSON being output. This is from the SDK. Um,&nbsp;&nbsp; so we won't look at the JSON too much because it's&nbsp; not much fun to parse it manually. But over here&nbsp;&nbsp; we can see that it also created a to-do list for&nbsp; us. So claw is now going to actually go through&nbsp;&nbsp; this to-do list and try to implement uh implement&nbsp; the uh powerup feature. Uh and similarly for the&nbsp;&nbsp; question timer uh it's going to do something&nbsp; something uh similar. Uh one more thing that&nbsp;&nbsp; we should do here is um there are already a&nbsp; couple pull requests that have been opened&nbsp;&nbsp; uh for this repo. Um and let's get claw to&nbsp; review it or change some of these pull requests&nbsp;&nbsp; uh just just for fun. Uh there's this one which&nbsp; is change background color to blue. All right.&nbsp;&nbsp; I I actually think I like green better. So,&nbsp; I'm just going to be like, "All right, Claude,&nbsp;&nbsp; please change this to green." Uh, this one is&nbsp; fairly easy, and I'm pretty sure Claude's going&nbsp;&nbsp; to do this, but I just wanted to show you guys&nbsp; that it can also add commits for a pull request&nbsp;&nbsp; that's already open. U Okay, so this is going to&nbsp; take a few minutes to run. And while this runs,&nbsp;&nbsp; uh, let's let's go back to the presentation,&nbsp; and then we'll check up on how this is doing,&nbsp;&nbsp; um, towards the end. Um, okay cool. So, let's do&nbsp; a little bit of a deep dive on the features of the&nbsp;&nbsp; SDK. Uh, when you call cloud-ba it has it has uh&nbsp; it has no edit or destructive permission access.&nbsp;&nbsp; Uh, which is great for safety, but it's not great&nbsp; for actually getting things done. uh which is why&nbsp;&nbsp; there is a d-allowed tools option which allows you&nbsp; to to preconfigure cloud with uh any permissions&nbsp;&nbsp; that you think it might need in the future for for&nbsp; for your given task. Uh so in this case the first&nbsp;&nbsp; example you see that I've given it permissions&nbsp; bash permissions to uh npm run build npm test&nbsp;&nbsp; and the right tool uh which is a good set of&nbsp; uh permissions because this allows cloud to uh&nbsp;&nbsp; self-verify what's what it's writing uh and build&nbsp; uh build build your project and test and then&nbsp;&nbsp; continue writing. Um similarly for MCP if you have&nbsp; MCP servers configured um you can allow list those&nbsp;&nbsp; MCP tools as well. So it's it's a very similar uh&nbsp; very similar process. Uh then structured output&nbsp;&nbsp; uh we already saw an example of structured output&nbsp; both from the GitHub actions logs and uh also the&nbsp;&nbsp; the little screenshot I showed you earlier. But&nbsp; there's two there's two modes here. There's stream&nbsp;&nbsp; JSON and JSON. Um it it's it does exactly what&nbsp; it sounds like. If you if you select stream JSON,&nbsp;&nbsp; it'll actually stream messages to you as and&nbsp; when they're available versus JSON will just&nbsp;&nbsp; give you one giant blob of JSON at the end. U&nbsp; and parsing this JSON and building on top of it&nbsp;&nbsp; is really how you can make use of the cloud&nbsp; code SDK um and and create features for for&nbsp;&nbsp; your users. Um and then you can also configure&nbsp; the system prompt. So you can do d-system prompt&nbsp;&nbsp; talk like a pirate and you can get cloud code&nbsp; to talk like a pirate for the rest of your day.&nbsp;&nbsp; uh which is actually quite fun. If you haven't&nbsp; done it, I'd encourage you to try it out. Um we also have uh a few user interact interaction&nbsp; features built into the SDK. Um and what that&nbsp;&nbsp; means is like the first one is uh is resuming&nbsp; session state. So um when you uh when you call&nbsp;&nbsp; cloud-p in in structured output or JSON mode, it's&nbsp; going to return a session ID. Uh and this session&nbsp;&nbsp; ID is useful because you can then reference the&nbsp; session ID to go back to the same context state&nbsp;&nbsp; that that claude had when it finished that&nbsp; process. Uh so by preserving these session&nbsp;&nbsp; IDs and keeping track of them, you can enable or&nbsp; like build user interactive features where like&nbsp;&nbsp; you the user says something, you pass that on to&nbsp; claude, uh claude returns a response and now you&nbsp;&nbsp; want the user to give feedback on that response&nbsp; and that's how this kind of enables you to to&nbsp;&nbsp; build those types of interactions in your apps.&nbsp; Um, and then the last one, and this one's actually&nbsp;&nbsp; pretty interesting, and it's it's fairly recent,&nbsp; too. Um, it's it's d-permission prompt tool. Um,&nbsp;&nbsp; we talked a little bit about how to give cloud&nbsp; permissions using the allowed tools flag. And that&nbsp;&nbsp; requires you to to preconfigure them in advance.&nbsp; Uh, but what if you didn't want to do them because&nbsp;&nbsp; you don't know what tools cloud wants to what tool&nbsp; tools cloud would want to use in the future? Um,&nbsp;&nbsp; in that case, you can use the d-p permission&nbsp; prompt tool and offload the permission management&nbsp;&nbsp; to an to an MCP server. U so you can ask users&nbsp; in real time for whether they want to accept a&nbsp;&nbsp; tool or reject a tool and you can have an&nbsp; MCP server kind of handle that for you as&nbsp;&nbsp; opposed to trying to predict which tools are&nbsp; okay and which tools are not. Um, so this is&nbsp;&nbsp; uh this is fairly recent and we'd love to get&nbsp; feedback on this if you guys end up trying it out. Uh, okay. Let's let's go back to our demo uh and&nbsp; see what Claude's done. Um, all right. So, this is&nbsp;&nbsp; the power up issue. Uh, we can see that Claude&nbsp; has actually gone through his to-do list. Um, okay. I'm going to open a PS. There's a&nbsp; there's a link over here to create a PR.&nbsp;&nbsp; And I'm going to click that uh and see&nbsp; what that gives us. I'll actually create&nbsp;&nbsp; the pull request too so it's easier&nbsp; for us to review. Um I don't really&nbsp;&nbsp; know how this codebase works but we'll&nbsp; still eyeball it just to see if you know&nbsp;&nbsp; it's doing the right thing. U so you see&nbsp; some set power up stuff seems all right. Okay. There's like some configuration in the&nbsp; main component. Um all right. Right. I think&nbsp;&nbsp; what we should do and what would make this fun&nbsp; is we should just get this branch locally and&nbsp;&nbsp; see what Claude did because there's no way&nbsp; that we can actually figure out what it did&nbsp;&nbsp; in the short amount of time that we have. So, I'm&nbsp; going to go back to my terminal, do a good fetch, check out the branch that Claude just&nbsp; created, and restart our process. Okay.&nbsp;&nbsp; Uh, awesome. Uh, it looks like we have a powerup&nbsp; section now, uh, at the bottom of our config page.&nbsp;&nbsp; And it's a little check box. I like that touch.&nbsp; Uh, we'll keep both of them on. And, uh, no, let's&nbsp;&nbsp; select general knowledge. Uh, let's start playing&nbsp; this game. Let's see what it did. Oh, sweet. So,&nbsp;&nbsp; we You see it has like this little 50/50 button&nbsp; on the bottom left and a skip questions button on&nbsp;&nbsp; the right. Um, I'm just going to with 50/50&nbsp; because I have no idea what the answer this&nbsp;&nbsp; is. Does anybody know what that is? D. Okay,&nbsp; there we go. That makes sense. Cadbury. Yeah. Uh, all right. I'm going to&nbsp; skip this one and then let's&nbsp;&nbsp; just breeze through the other ones&nbsp; for for sake of time. Um, all right. All right. Still got a still got an F, but&nbsp; we got we got one one correct answer. Uh,&nbsp;&nbsp; which is better than zero correct&nbsp; answers. Um, and yeah, uh, I guess uh, yeah, it tricked us. Uh, that&nbsp; was a good one. Um, but yeah, I mean,&nbsp;&nbsp; it it seems like it worked. I think there's&nbsp; definitely more we could do here. We could&nbsp;&nbsp; like show how the power like which questions&nbsp; we we use the power upon over here. And there's&nbsp;&nbsp; like definitely more we can do. But at the most&nbsp; basic level, I think uh Claude was able to do&nbsp;&nbsp; u do the task that we assigned it to do. Um&nbsp; which is exciting. Like this is kind of the&nbsp;&nbsp; power of the GitHub action because you didn't&nbsp; really have to run this on your own infra. You&nbsp;&nbsp; can just literally comment on a thread saying&nbsp; please build this for me. It uses your your&nbsp;&nbsp; uh GitHub action runners and just like does the&nbsp; thing. Um we let's also look at the PR that we&nbsp;&nbsp; told it to change from blue to green. Uh it's&nbsp; all hex code. So let's just see what it did in&nbsp;&nbsp; the commits. So you see there's two commits and uh&nbsp; Claude has added this last one to switch it from&nbsp;&nbsp; blue to green. And it did it for all three of the&nbsp; places where uh we uh where the color was defined,&nbsp;&nbsp; which is which is awesome. Um okay. Uh I'm not&nbsp; going to go over the last one, the question timer,&nbsp;&nbsp; because we we might run out of time. Um, but&nbsp; this hopefully gives you uh insight into what&nbsp;&nbsp; the cloud GitHub action can can do for you. Um,&nbsp; let's let's go back to the presentation now. Okay. So, just as a recap, uh, the cloud&nbsp; GitHub action um, as it's implemented today,&nbsp;&nbsp; uh, is able to to read your code. It's able to&nbsp; create PRs for you from GitHub issues like we&nbsp;&nbsp; just saw. It's able to create commits for you.&nbsp; So if you already have a PR and you commit or&nbsp;&nbsp; you comment on it, it can add a commit to an&nbsp; existing branch or an existing PR. Um it can&nbsp;&nbsp; answer questions. It doesn't have to do something.&nbsp; It can just literally answer questions for you.&nbsp;&nbsp; If you don't understand something, you can be&nbsp; like, "Hey Claude, how does this work?" And you&nbsp;&nbsp; can get it to answer questions and it can of&nbsp; course review your code. Uh the best part of&nbsp;&nbsp; all of this is that you don't have to take care&nbsp; of the infra. It runs on existing GitHub runners&nbsp;&nbsp; which almost everyone has configured if you're&nbsp; using GitHub actions. Um, so that that's kind&nbsp;&nbsp; of the really nice thing about this is you&nbsp; don't have to worry about any of the infra. Okay. Um, so how were actions built, right? Um,&nbsp; I think I I may have mentioned that these actions&nbsp;&nbsp; were built on top of the SDK. Um, we so the SDK&nbsp; does form the foundation of how these actions were&nbsp;&nbsp; built and then we have two other actions on top.&nbsp; We have the cloud code base action. Uh this is a&nbsp;&nbsp; thin layer that just uh uh it it just implements&nbsp; the piece which talks to cloud code and returns&nbsp;&nbsp; the response from cloud code. Um and then we have&nbsp; another action on top of this which is called the&nbsp;&nbsp; PR action. And this action is responsible for&nbsp; all the fancy things that you saw um on the&nbsp;&nbsp; PR. So it's responsible for making comments, for&nbsp; the to-do list, for rendering it the right way,&nbsp;&nbsp; uh for adding the PR links and things like that.&nbsp; So um it it's kind of three layers uh in which&nbsp;&nbsp; it's built. Both the base action and the PR action&nbsp; are open sourced. Uh so I would encourage you guys&nbsp;&nbsp; to go have a look uh you know take inspiration&nbsp; from how that works and maybe that inspires more&nbsp;&nbsp; ideas. Um yeah. Um yeah and then uh finally uh we&nbsp; also um uh you guys can install the cloud GitHub&nbsp;&nbsp; actions today. Uh the easiest way to do this is&nbsp; to open up cloud code in a terminal uh in the repo&nbsp;&nbsp; that you want to install it in. Uh and once you&nbsp; open up cloud code, just do slashinstall github&nbsp;&nbsp; action. Um and that is going to present you with a&nbsp; nice flow which uh guides you through configuring&nbsp;&nbsp; your GitHub action as well as merging it. So the&nbsp; end the end result of this would be a PR which&nbsp;&nbsp; would be a YAML file for your GitHub action. Um,&nbsp; and once you merge that in and you configure your&nbsp;&nbsp; API keys and things like that, uh, you're off&nbsp; to the races and you can you can go ahead and&nbsp;&nbsp; start tagging Cloud and using Claude, uh, like&nbsp; we just did right now. Um, uh, small caveat,&nbsp;&nbsp; if you're if you're a Bedrock or Vertex user, uh,&nbsp; the instructions are a little bit different and&nbsp;&nbsp; a tiny bit more manual. So, uh, please have&nbsp; a look at the docs. the docs uh are pretty&nbsp;&nbsp; comprehensive in uh in in in helping you set up&nbsp; uh the GitHub action for both bedrock and Vortex. Uh cool. Finally, uh resources. Uh these are&nbsp; resources for things that we've talked about&nbsp;&nbsp; today. If you want to snap a picture, uh go&nbsp; ahead. Um the the the open source repos for&nbsp;&nbsp; both the base action and the cloud code action&nbsp; are are here. Um and we we absolutely love your&nbsp;&nbsp; feedback as well. So if you guys have any feedback&nbsp; on the SDK, on the GitHub action or on cloud code,&nbsp;&nbsp; uh please go to our um public cloud code&nbsp; GitHub repo and file an issue there and&nbsp;&nbsp; someone will have a look um and and get&nbsp; back to you. Um cool. That's all I have&nbsp;&nbsp; for today. Uh thanks for joining me and I&nbsp; hope you guys have a good rest of the day.

---

## 3. Claude plays Pokemon

**Preview:**
> Kind: captions Language: en All right, welcome everybody. I was asked to talk&nbsp; about uh tool use and some of them changes to our&nbsp;&nbsp; new models that have made them better at using&nbsp; tools. And so I decided to uh use this opportunity&nbsp;&nbsp; to instead talk about QuadPlace's Pokemon uh which&nbsp; uh I made. So I'm David. I'm on our team here. Uh&nbsp;&nbsp; I'm the creator of QuadPlays Pokemon. And what&nbsp; we are going to do today is relaunch the stream&nbsp;&nbsp; live t...

**Full Transcript:**

Kind: captions Language: en All right, welcome everybody. I was asked to talk&nbsp; about uh tool use and some of them changes to our&nbsp;&nbsp; new models that have made them better at using&nbsp; tools. And so I decided to uh use this opportunity&nbsp;&nbsp; to instead talk about QuadPlace's Pokemon uh which&nbsp; uh I made. So I'm David. I'm on our team here. Uh&nbsp;&nbsp; I'm the creator of QuadPlays Pokemon. And what&nbsp; we are going to do today is relaunch the stream&nbsp;&nbsp; live together. We're going to talk about it. Uh&nbsp; and we're going to have a good f time. Could you&nbsp;&nbsp; flip over to my demo screen, please? All right.&nbsp; Uh I have a button queued up to my machine in my&nbsp;&nbsp; house in my basement in Seattle, which is running&nbsp; this. And I am going to hit enter. Uh but I need&nbsp;&nbsp; help from the crowd to countdown from 10 for me to&nbsp; windmill slam this enter button. So, I'm going to&nbsp;&nbsp; start it, but I need all of you to participate.&nbsp; It's very important for the vibes to run. 10 9 8&nbsp;&nbsp; 7 6 5 4 3 2 1. Let's go. All right. Uh, so I'm&nbsp; here to talk about Quadplays Pokemon because&nbsp;&nbsp; there's some new stuff that our models can do that&nbsp; is really exciting. um and makes the models better&nbsp;&nbsp; at Pokemon, makes the models better at a lot of&nbsp; things, makes the models better at being agents.&nbsp;&nbsp; What this actually going to practically look like,&nbsp; and I'll show some examples later in some slides I&nbsp;&nbsp; have after we're done, is that our uh models will&nbsp; like learn and adapt and think differently. So,&nbsp;&nbsp; in a minute, we're going to get to the name&nbsp; entry screen. One of my favorite examples I&nbsp;&nbsp; show later is that uh the name entry screen, a&nbsp; notoriously challenging place for Quad. Uh Quad&nbsp;&nbsp; does not quite understand how cursors move around&nbsp; and it gets a little bit lost. it's a grid. It&nbsp;&nbsp; gets confused. Uh, one of the things that it's&nbsp; really good at with this extended thinking mode&nbsp;&nbsp; is sort of like building a full plan of where it&nbsp; needs to move, what it needs to do, that thing&nbsp;&nbsp; between tool calls. So, in the past, it might not&nbsp; sort of use that extended thinking, reconsider&nbsp;&nbsp; its assumptions, question itself, figure out the&nbsp; right answer. Uh, in Quad 4, uh, you will see that&nbsp;&nbsp; happen. Another feature we added in this uh new&nbsp; version of quad that uh we have gotten asked for&nbsp;&nbsp; a lot is the ability to call multiple tool calls&nbsp; at once. So parallel tool calling is the other&nbsp;&nbsp; name. Uh in quad 3.7 it was like frustratingly bad&nbsp; at this. It would only call one tool at any given&nbsp;&nbsp; time. And what that means for you is practically&nbsp; like when you're building an agent it'll call one&nbsp;&nbsp; tool and then it will like wait. You'll have&nbsp; to make a whole another generation. and you'll&nbsp;&nbsp; get a whole time to first token hit between uh&nbsp; between generations. With the new models, they are&nbsp;&nbsp; like much more keen to call multiple tools. Uh we&nbsp; actually saw it right at the beginning. I haven't&nbsp;&nbsp; seen it since, but it will like do things in&nbsp; Pokemon like take an action and update its memory&nbsp;&nbsp; at the same time. Uh which essentially just like&nbsp; saves us tokens when we're building agents. The&nbsp;&nbsp; model is going to take more actions more quickly&nbsp; and not need to sort of like go through the plan&nbsp;&nbsp; uh plan act loop as often. Tool use has evolved&nbsp; rapidly uh in the last year. Uh when I first&nbsp;&nbsp; started helping customers with tool use last year,&nbsp; a lot of it was like let's uh use a calculator&nbsp;&nbsp; give the model a calculator tool so it can do math&nbsp; because the model's bad at math so it has this&nbsp;&nbsp; ability to spill over. These days that is not what&nbsp; tool use gets used for. Tool use is the driver of&nbsp;&nbsp; agents. Uh when people build with tools, they give&nbsp; models full suites of tools that enable agent or&nbsp;&nbsp; models to take long agentic actions and uh and&nbsp; move forward. And so in that the agentic tool or&nbsp;&nbsp; loop, it is really revolves around tools. In an&nbsp; agentic loop, uh the model will plan in action,&nbsp;&nbsp; act on whatever that plan was, learn something&nbsp; from what it saw, and then repeat that until it's&nbsp;&nbsp; accomplished its goals. In Pokemon, it might say,&nbsp; "I'm going to try to talk to my mom right now."&nbsp;&nbsp; The way it would do that is I'm going to press A.&nbsp; Uh, and then it will reflect, see the dialogue box&nbsp;&nbsp; come up and see it worked and keep going with its&nbsp; job to play Pokemon. So, let's talk about those&nbsp;&nbsp; two big improvements we talked about uh with&nbsp; tool calling in Cloud 4. The first is improved&nbsp;&nbsp; planning by being able to use extended thinking&nbsp; mode between tool calls. uh you are now able to&nbsp;&nbsp; uh see the model actually break down, build plans,&nbsp; step back, reflect, question its assumptions&nbsp;&nbsp; between tool calls. And by calling multiple tools&nbsp; at once, the models will be more efficient when&nbsp;&nbsp; acting as agents. Uh this has a practical impact&nbsp; in Pokemon that we also didn't get to see. Uh so I&nbsp;&nbsp; want to talk a little bit about this interleaf&nbsp; thinking, thinking between tool calls because&nbsp;&nbsp; there's some some clear examples in Pokemon of how&nbsp; this works out. Uh in the past when you launched&nbsp;&nbsp; it, we actually saw this is the one thing we&nbsp; actually did saw is when you hit run on a model,&nbsp;&nbsp; uh it would build the whole plan for how it was&nbsp; going to beat Pokemon in its first message. Uh&nbsp;&nbsp; this was a terrible plan it would write. It would&nbsp; say, I'm going to write my name down as Claude&nbsp;&nbsp; and I'm going to give my Pokemon nicknames and I'm&nbsp; going to go beat Pokemon. And that's the extent of&nbsp;&nbsp; its planning. Uh and then it would hit uh a really&nbsp; big horrible challenge which is the name entry&nbsp;&nbsp; screen and everything would fall apart. Uh, it&nbsp; would occasionally hit left to move the cur cursor&nbsp;&nbsp; to a new letter and accidentally wrap around to&nbsp; the other side of the screen and think, "How the&nbsp;&nbsp; heck did my cursor end up on the right side of&nbsp; the screen? The game must be bugged. Everything&nbsp;&nbsp; is terrible." Uh, and now with the ability&nbsp; to do extended thinking between tool calls,&nbsp;&nbsp; you'll see the model actually sort of catch these&nbsp; errors more often, adjust, adapt it thinking,&nbsp;&nbsp; and and uh come up with a better plan. So in this&nbsp; example, this is an actual trace from Quad 4 Opus&nbsp;&nbsp; where it says, "I'm really stumped. The cursor&nbsp; went right instead of left when I hit left. What&nbsp;&nbsp; happened?" And then it will actually say, "Wait,&nbsp; like let's step through. What actually happened?&nbsp;&nbsp; Where did the cursor go? It was at this letter,&nbsp; this letter, this letter. Actually, what I think&nbsp;&nbsp; happened is like the cursor spilled over uh and&nbsp; wrapped around to the other edge. Everything's&nbsp;&nbsp; okay. I understand how this works now. Let's&nbsp; keep going with name entry." And it can sort of&nbsp;&nbsp; like pick that up and learn it. And that ability&nbsp; to adapt on the fly is really meaningful as you&nbsp;&nbsp; build agents that are kind of expected to take&nbsp; in tons of new information as they're building. Similarly, we have parallel tool calling. Uh&nbsp; parallel tool calling more of an efficiency game.&nbsp;&nbsp; In the past, when you're sitting there waiting&nbsp; to talk to mom, the model would press a talk and&nbsp;&nbsp; then if it wanted to update its knowledge base&nbsp; to keep track of where it found mom in the past,&nbsp;&nbsp; it would have to take a whole another&nbsp; action. can call out to call it again,&nbsp;&nbsp; wait for the time to first token hit, make&nbsp; that change. Uh, with parallel tool calling,&nbsp;&nbsp; it can do both things at once. Basically, it&nbsp; can say, "I'm going to talk to mom. I'm going&nbsp;&nbsp; to update my knowledge base. I want to advance&nbsp; the dialogue six times by pressing A six times&nbsp;&nbsp; because I'm bored talking to mom." Uh, this&nbsp; saves you time, saves your customers time,&nbsp;&nbsp; it speeds up how agents work. Um, and it will&nbsp; make agents work more effectively uh for your&nbsp;&nbsp; customers that won't have to wait around for sort&nbsp; of redundant tool calling and calls to cloud. And so what this means and what's next is that uh&nbsp; models are getting better at being agents. This is&nbsp;&nbsp; obvious, we knew this, but this is one of the core&nbsp; things we work on at Enthropic. We find ways to&nbsp;&nbsp; make models smarter when they're acting over long&nbsp; time horizons and uh solving complex problems.&nbsp;&nbsp; Extended thinking between tool calls is an example&nbsp; of this. It's something we've seen make a real&nbsp;&nbsp; impact on how effective agents are in the real&nbsp; world. But I also want to talk about like quad is&nbsp;&nbsp; being trained to be a useful agent and an easier&nbsp; one to build with. When we build our models,&nbsp;&nbsp; we try to listen to developers. We understand what&nbsp; it means to give quad the capabilities that make&nbsp;&nbsp; it work more easily, more seamlessly, better&nbsp; for users and we train those into our models&nbsp;&nbsp; too. Things like parallel tool calling that&nbsp; we want to uh hear feedback and improve our&nbsp;&nbsp; models on model over model. I will uh let some&nbsp; people ask questions. We can chat about this a&nbsp;&nbsp; little bit. Yeah, over here. Hey y uh hi. Uh&nbsp; thanks cloud cloudbased Pokemon is awesome.&nbsp;&nbsp; Uh so one question I had was so you have many&nbsp; low-level actions right which is like click&nbsp;&nbsp; button A click button B and then you also have&nbsp; some highle actions like go to go to this point&nbsp;&nbsp; that you previously visited that's one of the high&nbsp; level actions are all these in the same hierarchy&nbsp;&nbsp; of tools or how do you think of because when&nbsp; you're building any agent like you know you have&nbsp;&nbsp; some sort of zoomed out view action that you want&nbsp; to take and then some zoomed in click a button&nbsp;&nbsp; action y how do you think of this and should&nbsp; it be flat should it be like a hierarchy how&nbsp;&nbsp; do you think of this thanks I think like designing&nbsp; any agent, designing tools tends to be the thing&nbsp;&nbsp; that actually matters the most. Um, and this&nbsp; is like the most simple set of tools. In fact,&nbsp;&nbsp; like I've aimed for simplicity with Pokemon. It's&nbsp; somewhat a bad example in this sense. But what&nbsp;&nbsp; really matters is being clear like separating the&nbsp; concerns of what tool should be used when, giving&nbsp;&nbsp; good examples of what tools should be used when&nbsp; and how to do that. And so in the case of Pokemon,&nbsp;&nbsp; I have this tool that allows the model to navigate&nbsp; to a specific place. Um, and then you have to just&nbsp;&nbsp; be very clear to it that it should use that when&nbsp; it's trying to move around in the overworld. Uh,&nbsp;&nbsp; it's like I have I watched Claude play a&nbsp; bunch and I found out that uh the model&nbsp;&nbsp; was quite bad at moving around in the overworld&nbsp; its own. And so just basically telling it, hey,&nbsp;&nbsp; you're not good at this when you're trying to do&nbsp; this set of tasks. This is the right tool to use.&nbsp;&nbsp; You'll have a better outcome versus if you're&nbsp; in a battle uh just pressing buttons directly.&nbsp;&nbsp; You're perfectly capable of it's easier for you.&nbsp; that's a good way to do it. And so I think about&nbsp;&nbsp; sort of like the loop of building these learn&nbsp; like watch the model, see where it struggles,&nbsp;&nbsp; try to design and build tools that will help some&nbsp; of the places it struggles and then write clear&nbsp;&nbsp; descriptions that help the model understand what&nbsp; you have seen like what its shortcomings are, what&nbsp;&nbsp; it why it might need this tool, what scenarios&nbsp; and and equip it with that knowledge. Uh I think&nbsp;&nbsp; there's it might come here. Yes, there there's a&nbsp; pattern uh in which if you have like a bunch of&nbsp;&nbsp; tool functions and you don't want to necessarily&nbsp; like clutter your current context with like a&nbsp;&nbsp; whole huge list, you adopt a helper which kind of&nbsp; acts like a proxy where the model can say hey I&nbsp;&nbsp; want to accomplish this and then okay so you know&nbsp; what I'm talking about. Have the dynamics of that&nbsp;&nbsp; particular pattern use changed at all with the new&nbsp; model? Uh I don't think we like I have not studied&nbsp;&nbsp; and I don't think we really know how that will&nbsp; break down with the new model. Um my expectation&nbsp;&nbsp; is that like telling the smarter a model gets the&nbsp; more I trust it with the full context and to make&nbsp;&nbsp; complex decisions. So my gut with building&nbsp; with opus would be or sonnet really like the&nbsp;&nbsp; quad four models is giving it the full list and&nbsp; again maybe you're guessing about like context&nbsp;&nbsp; clutter and just like avoiding if the tools&nbsp; are too long or maybe double click yeah follow your uh I think we've pretty confidently seen the&nbsp; model be able to navigate order of like 50 to 100&nbsp;&nbsp; tools. It's just a question of def definition&nbsp; though. Like the more as a human who writes&nbsp;&nbsp; prompts and writes tools out, the more that you&nbsp; uh more tools you write, the less likely it is&nbsp;&nbsp; that you're going to be precise enough and where&nbsp; and how you can actually define those tools to the&nbsp;&nbsp; model and sort of like divide the lines between&nbsp; them. And so from my perspective, it's a little&nbsp;&nbsp; bit like welldesigned that's possible. if it gets&nbsp; complicated or nuanced or the too much overlap,&nbsp;&nbsp; I think that's where you need to start figuring&nbsp; out like patterns to delegate larger chunks of&nbsp;&nbsp; work or things like that. Uh so when you say that&nbsp; uh we should give clear descriptions of what tools&nbsp;&nbsp; should be used when uh does that belong in&nbsp; the prompt or does that belong in the tool&nbsp;&nbsp; uh description? I ask because I've been uh working&nbsp; on agentic features myself and I find that if I&nbsp;&nbsp; pass in a JSON schema where I tell it about every&nbsp; field and description in a way that's opinionated&nbsp;&nbsp; about what it's going to do, like that's generally&nbsp; worked better for me. But on the other hand,&nbsp;&nbsp; I see these architecture advancements with&nbsp; remote MCP servers where tools can be defined&nbsp;&nbsp; once and used in many other use cases. So I'm&nbsp; I'm not really sure what to do. Yeah. Uh it's&nbsp;&nbsp; a great question. I um my lean is often to put&nbsp; things in a tool description, but honestly I&nbsp;&nbsp; think you can do both. I mean when you the way&nbsp; that our prompt gets rendered when you provide&nbsp;&nbsp; uh tools in a tool description is it just&nbsp; renders the tools in the system prompt. And&nbsp;&nbsp; so mechanically the the gap in text between if&nbsp; you would write it in the tool description and&nbsp;&nbsp; just below it in the system prompt is not that&nbsp; much. And I think it matters more to just have&nbsp;&nbsp; queer descriptions and to be clear about what it&nbsp; is. I think the thing that's nice about putting&nbsp;&nbsp; in a tool description is you're sort of like&nbsp; separating what tool you're talking about when&nbsp;&nbsp; more like the way that we've trained it.&nbsp; We're sort of like guaranteeing that the&nbsp;&nbsp; syntax that is used for the model to like read&nbsp; and understand a tool description is something&nbsp;&nbsp; it's seen before. Whereas if you venture off&nbsp; that path, there's a risk that you're going&nbsp;&nbsp; to do something that's not as easy for the&nbsp; model to understand. Um, but I think like if&nbsp;&nbsp; you write a really strong prompt, it should work&nbsp; similarly well in both situations, I'd expect. Uh, so 3.5 Sonic got stuck in Mount Moon for a&nbsp; while. It did. Can it Can it make it out? Uh,&nbsp;&nbsp; it will make it out. This is okay. Let's talk&nbsp; a little bit about Claude performance. This is&nbsp;&nbsp; a good chance to ramble here. Uh, Claude, this&nbsp; is Opus is significantly better at Pokemon. Uh,&nbsp;&nbsp; but the ways that it's better are not the most&nbsp; satisfying ways if you want to see uh, Pokemon&nbsp;&nbsp; get beat. It's like roughly as enabled to see the&nbsp; Pokemon the Game Boy screen as it was before. So,&nbsp;&nbsp; we didn't like I don't know. I didn't go&nbsp; to research and ask them to to make the&nbsp;&nbsp; model better at Game Boy screens. That's not&nbsp; what our customers are asking for. It might&nbsp;&nbsp; be my favorite thing, but uh, wouldn't be a good&nbsp; reflection. So, uh, it still struggles with some&nbsp;&nbsp; like navigation challenges and stuff like that&nbsp; where it's just like not sure what it's seeing.&nbsp;&nbsp; its ability to plan and execute on a plan is&nbsp; like miles ahead of where it was in the past. My&nbsp;&nbsp; favorite example of this that I've seen uh after&nbsp; you get the third badge to go to Rock Tunnel,&nbsp;&nbsp; you need to get Flash the the HM. To do that, you&nbsp; need to go catch at least 10 species of Pokemon&nbsp;&nbsp; and then like find some dude in a random building.&nbsp; It found the dude in a random building. It found&nbsp;&nbsp; out it needed to catch 10 Pokemon and it went like&nbsp; on a 24-hour grind session finding 10 Pokemon like&nbsp;&nbsp; uninterrupted. didn't get distracted, didn't do&nbsp; anything else, catch 10 to Pokemon, like wander&nbsp;&nbsp; back, get flash, go straight to Rock Tunnel.&nbsp; And it's like this ability to sort of plan and&nbsp;&nbsp; execute, like build a plan, and then like actually&nbsp; track and execute against that over in this case&nbsp;&nbsp; like a 100 million tokens worth of information was&nbsp; like by far the best I've ever seen from a model.&nbsp;&nbsp; So, uh, in this playthrough, as you watch at home,&nbsp; as you watch on the demo thing, I think you'll&nbsp;&nbsp; see it gets stuck in Mount Moon for probably a&nbsp; similar amount of time, if I had to guess, but,&nbsp;&nbsp; uh, you'll see it do some, uh, like miles more&nbsp; intelligent things in the process of getting&nbsp;&nbsp; there. Funny how it works. Yeah. Hey, uh, I just&nbsp; have a question about parallel tool calling. First&nbsp;&nbsp; time I've ever Is this state-of-the-art? I haven't&nbsp; uh no models should be able to do this. I think&nbsp;&nbsp; like frankly like I wish 3.7 could have done this.&nbsp; I don't think this is like an insane capability&nbsp;&nbsp; but it matters like it's just a useful thing for&nbsp; people to be able to do. So just under the hood in&nbsp;&nbsp; your messages array that you're p interacting&nbsp; with the model are you just doing some magic&nbsp;&nbsp; on your end to kind of pre it's kind of like on&nbsp; the model to say hey I'm done. I I've described&nbsp;&nbsp; a set of tool calls I want to make and I'm done&nbsp; or not. So the model in the past would just like&nbsp;&nbsp; make one tool call and say I'm want to wait for&nbsp; the result of this. The model now is more likely&nbsp;&nbsp; to understand that in some cases I actually know&nbsp; two or five or eight tool calls that I want to&nbsp;&nbsp; make right now and it will describe all of those&nbsp; and then the object you get back in the API is&nbsp;&nbsp; has eight tool use blocks that say here are the&nbsp; eight tools I want to use and then you're asked&nbsp;&nbsp; to go sort of like render those. Awesome. Thank&nbsp; you. Yeah. So I'm I'm particularly interested with&nbsp;&nbsp; the idea, right? So with parallel tool calls, it&nbsp; there are some cases where it's obvious that all&nbsp;&nbsp; the tool calls can actually happen in parallel,&nbsp; but then there's like more of a planning sense&nbsp;&nbsp; where you showed like press A, press A, press A.&nbsp; And so of course I'm like thrown back to being&nbsp;&nbsp; six in my mom's minivan and remembering when I&nbsp; restarted a really long conversation because I was&nbsp;&nbsp; spamming A. Yep. And so I'm just like I'm nerdily&nbsp; curious if that if it's ever done that where it's&nbsp;&nbsp; like impatiently restarted a conversation all the&nbsp; time. But I think that that also like scratches&nbsp;&nbsp; out a deeper thing of like is there ever such&nbsp; a thing as too too much planning and do you see&nbsp;&nbsp; it like being too opinionated about following&nbsp; the plan and not updating with new information&nbsp;&nbsp; like the conversation has ended. I think this&nbsp; is like the range for good prompting honestly.&nbsp;&nbsp; Um the so the reason that it actually hits many&nbsp; buttons is you'll see its thought process say&nbsp;&nbsp; I'm going to hit a whole bunch of buttons and&nbsp; I'll stop whenever it's done but it doesn't&nbsp;&nbsp; like quite have the sense of time like we do. So&nbsp; if it says I want to hit A 500 times, it's like,&nbsp;&nbsp; oh, don't worry. I'll be I'll know when I have&nbsp; finished the dialogue and then I'll stop. But&nbsp;&nbsp; it doesn't quite understand that it doesn't get&nbsp; to see in between each one by default because&nbsp;&nbsp; like the the nature I don't know that's a very LLM&nbsp; problem that you have to register 500 buttons and&nbsp;&nbsp; then close your eyes and then come back and find&nbsp; out what happened. Um, but you can actually get&nbsp;&nbsp; around that just like with prompting and helping&nbsp; the model understand what is happening, what are&nbsp;&nbsp; its limitations and uh what and how should it act.&nbsp; So like in the system profit quad plays Pokemon,&nbsp;&nbsp; I just have to tell it when you register a&nbsp; sequence of buttons. You don't get to see&nbsp;&nbsp; like you're not going to see and so there could&nbsp; be side effects. Uh going restarting the dialogue&nbsp;&nbsp; is a simple version, but you can actually do like&nbsp; much worse things in Pokemon. Like I've seen it&nbsp;&nbsp; overwrite one of its moves accidentally when it&nbsp; was learning a new move in a way that was like&nbsp;&nbsp; quite bad for for making progress in the game.&nbsp; Um and so this is like I think the space where&nbsp;&nbsp; uh someone building agents, you have a lot of room&nbsp; to sort of see how models make mistakes like that,&nbsp;&nbsp; help them understand why and what's going on and&nbsp; sort of like build that into how you prompt them.&nbsp;&nbsp; uh prompt agents and that's a lot of how I think&nbsp; about sort of like iterating on agent design. So&nbsp;&nbsp; in our production agent we saw that in 3.7&nbsp; there was uh some not very good consistency&nbsp;&nbsp; with calling about 18 tools versus like if&nbsp; you were to just pass the model a single tool&nbsp;&nbsp; um and then just have the exact same prompt and&nbsp; and have it call that. And you mentioned before&nbsp;&nbsp; that the four models are able to handle like&nbsp; over a hundred tools. Are there any changes&nbsp;&nbsp; or differences you're seeing in in how you get&nbsp; consistent performance among so many tools? Uh,&nbsp;&nbsp; I think these models we've pretty clearly seen&nbsp; are much better at precise instruction following.&nbsp;&nbsp; It's going to be a double-edged sword. Like if&nbsp; you're imprecise with the instructions you write,&nbsp;&nbsp; they'll they'll readily follow or get confused by&nbsp; contrasting instructions sometimes. But I think&nbsp;&nbsp; the key is with like very good tool design and&nbsp; very crisp prompting, we've seen that these models&nbsp;&nbsp; are like much more capable at following a pretty&nbsp; long set of different and complex instructions and&nbsp;&nbsp; being able to use that to execute. So I think the&nbsp; key is there's more room to hill climb on a prompt&nbsp;&nbsp; maybe is what I would say with these models which&nbsp; is to say as you are making more and more precise&nbsp;&nbsp; uh descriptions of your tools there's more room&nbsp; to get better and better across a wider range&nbsp;&nbsp; of tools and sort of like reach that same level&nbsp; of performance you'd expect on a single tool. I&nbsp;&nbsp; think I am at time. I have successfully gave&nbsp; a very different talk than I expected. But I&nbsp;&nbsp; appreciate you all for being here and it&nbsp; was fun to chat with you all. [Applause]

---

## 4. Claude Code best practices

**Preview:**
> Kind: captions Language: en Let's get started. Welcome everyone to Cloud&nbsp; Code best practices. In this talk, I'm going&nbsp;&nbsp; to talk about kind of what cloud code is at a high&nbsp; level. Then we'll peer under the hood a little bit&nbsp;&nbsp; to kind of understand how cloud code works. And&nbsp; then knowing that because it's useful to kind of&nbsp;&nbsp; know how your tools work. We're going to talk&nbsp; about good use cases for cloud code and also&nbsp;&nbsp; best practices we've...

**Full Transcript:**

Kind: captions Language: en Let's get started. Welcome everyone to Cloud&nbsp; Code best practices. In this talk, I'm going&nbsp;&nbsp; to talk about kind of what cloud code is at a high&nbsp; level. Then we'll peer under the hood a little bit&nbsp;&nbsp; to kind of understand how cloud code works. And&nbsp; then knowing that because it's useful to kind of&nbsp;&nbsp; know how your tools work. We're going to talk&nbsp; about good use cases for cloud code and also&nbsp;&nbsp; best practices we've figured out both internally&nbsp; and from our users uh for getting the most out of&nbsp;&nbsp; this tool. Uh but before I get started, I'd like&nbsp; to introduce myself real quick and talk about how&nbsp;&nbsp; I ended up on the stage. So, my name's Cal and&nbsp; I joined Enthropic about a year and a half ago&nbsp;&nbsp; uh to help start up a team we call applied&nbsp; AI. And it's the applied AI's kind of mission,&nbsp;&nbsp; our team's mission is to help our customers and&nbsp; partners build great products and features on&nbsp;&nbsp; top of Claude. So what that really means is&nbsp; I spend a lot of my day prompting Claude to&nbsp;&nbsp; get the absolute best outputs out of these&nbsp; models. That said, I also love to code and&nbsp;&nbsp; I'm definitely one of those coders that like&nbsp; starts a lot of projects, has some crazy idea,&nbsp;&nbsp; and then just never finishes them. So, I have&nbsp; this graveyard of just like code that I started,&nbsp;&nbsp; never really finished. Um, but I'm always&nbsp; spinning new things up. And late last year,&nbsp;&nbsp; I was in Slack and I was hearing about this new&nbsp; tool that a few people are using. They were saying&nbsp;&nbsp; it was really cool. And so, on a Friday night, I&nbsp; downloaded the tool that would become Cloud Code.&nbsp;&nbsp; And I threw it at this kind of new notetaking&nbsp; app that I wanted to build. And like that whole&nbsp;&nbsp; weekend just kind of totally changed the way that&nbsp; I code and think about software engineering. I was&nbsp;&nbsp; carrying around my laptop with me all weekend. I&nbsp; was super addicted to just watching Claude Code&nbsp;&nbsp; work and I would press enter and I'd switch over&nbsp; to my browser and refresh and I watched this huge&nbsp;&nbsp; powerful application come together in front of my&nbsp; eyes. And I got way farther into this thing than&nbsp;&nbsp; I ever would have on my own. And it just blew my&nbsp; mind. And while I was doing this, I was a little&nbsp;&nbsp; worried. I was like, you know, I you know, I kind&nbsp; of know how these things work. So I'm like, man,&nbsp;&nbsp; I'm using a lot of tokens. I hope I don't get in&nbsp; trouble or anyone like notices. I'm not really&nbsp;&nbsp; contributing to anthropic code. Um, but what&nbsp; I didn't know is that the claude code team had&nbsp;&nbsp; built this internal like leaderboard tracking&nbsp; how much all the anthropic employees were using&nbsp;&nbsp; this. And over the weekend, I had shot to the&nbsp; top. And so through that, I got to meet Boris&nbsp;&nbsp; and Cat and some of the early cloud code team.&nbsp; And I was able to start talking to them and say,&nbsp;&nbsp; "Hey, I love this tool. I also know a lot about&nbsp; prompting. Can I help you all out?" And so through&nbsp;&nbsp; that I got involved and now I'm one of the core&nbsp; contributors on the team and I do a lot of I work&nbsp;&nbsp; a lot on the prompting the system prompts how the&nbsp; tools work the tool descriptions and tool results&nbsp;&nbsp; as well as I work on how we evaluate this tool.&nbsp; So when we think about changing the prompts how&nbsp;&nbsp; do we make how do we know we made things better or&nbsp; the same and we didn't totally ruin cloud code. So&nbsp;&nbsp; with that said let's kind of dive in. So, here's&nbsp; my current mental model of Claude Code and how I&nbsp;&nbsp; describe it to people when people ask me. Claude&nbsp; Code is like that co-worker that does everything&nbsp;&nbsp; on the terminal. It's the sort of person that just&nbsp; never touches the guey. They're a whiz. I think&nbsp;&nbsp; of when I was a junior engineer, I had this mentor&nbsp; and I would walk over to his desk and I would say,&nbsp;&nbsp; "Hey, Tony, can you help me with this bug?" and&nbsp; he would whipping it open his terminal and he'd&nbsp;&nbsp; be like doing all these crazy bash commands&nbsp; and changing things around in Vim and I'd&nbsp;&nbsp; always walk away thinking, "Wow, that was crazy.&nbsp; I should learn how to do that." Um, I never did.&nbsp;&nbsp; But having Claude Code on your computer is kind&nbsp; of like having Tony next to you all the time. So, how does Claude code kind of work under the&nbsp; hood? At Anthropic, we try to always do what&nbsp;&nbsp; we call the simple thing that works. And what&nbsp; that means for Cloud Code is it's what we would&nbsp;&nbsp; consider a very pure agent. And anthropic, when&nbsp; we talk about agents, what we really mean is some&nbsp;&nbsp; instructions, some powerful tools, and you let&nbsp; the model just run in a loop until it decides it's&nbsp;&nbsp; done. And that's really what Cloud Code is. So&nbsp; it's tools, powerful tools, and the tools that you&nbsp;&nbsp; know someone that was really good at a terminal&nbsp; would be able to use tools to create and edit&nbsp;&nbsp; files to use the terminal. And then you can also&nbsp; do things like pull in other things with MCP. Now,&nbsp;&nbsp; on top of that, there's how Claude understands&nbsp; the codebase. And if you're going to build a&nbsp;&nbsp; coding agent or a coding tool a year ago, you'd&nbsp; probably have ideas like, well, okay, I'm going to&nbsp;&nbsp; get this user message about something about this&nbsp; codebase and I'll need to figure out which files&nbsp;&nbsp; are relevant. So maybe I'll like index the whole&nbsp; codebase and embed it and do this fancy like kind&nbsp;&nbsp; of rag retrieval thing. That is not how cloud code&nbsp; works. We don't do any sort of indexing. Instead,&nbsp;&nbsp; claude kind of explores and understands the&nbsp; codebase. how you if you were new to a team&nbsp;&nbsp; and new to a codebase would explore a codebase&nbsp; and that is through a gentic search is the same&nbsp;&nbsp; sort of search tools you or I would use things&nbsp; like glob and gp and find and it can work its way&nbsp;&nbsp; through a codebase and understand what's going on&nbsp; and when we talk about a gentic search that really&nbsp;&nbsp; means the model can go do some searches and then&nbsp; it can look at the results and can say hm maybe I&nbsp;&nbsp; need to figure out a few more things I'm going to&nbsp; go do some more searching and then come back and&nbsp;&nbsp; then on top of these primitives. On top of this&nbsp; agent, we have a few things. We have a very nice&nbsp;&nbsp; light UI layer where you get to watch Claude&nbsp; code work. You see all the text fly by and we&nbsp;&nbsp; have this nice permission system that allows the&nbsp; agent to work and allows and kind of forces the&nbsp;&nbsp; human to butt in when the agent is doing something&nbsp; dangerous. And then on top of that, we also care a&nbsp;&nbsp; lot about security in this tool. And so because&nbsp; quad code is just such a lightweight kind of&nbsp;&nbsp; layer on top of the model and the fact that our&nbsp; model is available not just behind anthropic APIs&nbsp;&nbsp; but also with our cloud providers AWS and GCP it's&nbsp; very easy and native to point cloud code at one of&nbsp;&nbsp; these other services if you feel more comfortable&nbsp; consuming cloud that way. Now a lot of people ask&nbsp;&nbsp; me hey Cal what can I use cloud code for? Like&nbsp; what is it good at? where is it interesting? And&nbsp;&nbsp; the reality is it's kind of great at everything.&nbsp; So let's start with discovery. Often times in your&nbsp;&nbsp; career, you will be dropped into a new codebase.&nbsp; Whether that means you're switching teams,&nbsp;&nbsp; you're switching companies, I don't know,&nbsp; you're starting to work on some sort of&nbsp;&nbsp; open source project. And probably when you're&nbsp; first getting started and getting familiar,&nbsp;&nbsp; you're not very productive because you're just&nbsp; trying to figure out where things are in the&nbsp;&nbsp; codebase, what patterns kind of the team is using,&nbsp; things like that. And Claude code can kind of help&nbsp;&nbsp; supercharge that onboarding process. You can ask&nbsp; claude, hey, where is this feature implemented?&nbsp;&nbsp; Or since it's great at the terminal, you can&nbsp; say, hey, look at this file and look at the git&nbsp;&nbsp; history and just kind of tell me a story about how&nbsp; this code has changed over the past couple weeks.&nbsp;&nbsp; One thing you can use cloud code for, and I think&nbsp; this is underrated, is instead of just diving in&nbsp;&nbsp; and starting to work, you can use cloud code as a&nbsp; thought partner. So oftent times when I'm working&nbsp;&nbsp; with Claude and I want to implement a feature or&nbsp; we're going to change something up, I'll open up&nbsp;&nbsp; Claude and I'll say, "Hey Claude, you know,&nbsp; I'm thinking about implementing this feature,&nbsp;&nbsp; can you just kind of like search around and kind&nbsp; of figure out how we would do it and maybe report&nbsp;&nbsp; back with like two or three different options.&nbsp; Don't start working. don't start writing any&nbsp;&nbsp; files writing any files yet and claude will go&nbsp; off and use those agentic search capabilities&nbsp;&nbsp; and come back with a few ideas and then I could&nbsp; work with claude to kind of validate things and&nbsp;&nbsp; then we can jump into the project of course cloud&nbsp; code is great at building and writing code and&nbsp;&nbsp; I would say this in on two different fronts one&nbsp; it can do the zero to one sort of stuff you drop&nbsp;&nbsp; it in an empty directory and you say hey build me&nbsp; an app build me a game that demos where very well&nbsp;&nbsp; it's very fun to do it's very grat gratifying.&nbsp; Of course, in reality, what really matters is is&nbsp;&nbsp; cloud code good working in existing code bases.&nbsp; And this is primarily what we focus on. Um,&nbsp;&nbsp; on the cloud code team, we have in our codebase&nbsp; abnormally high, I would say, unit test coverage.&nbsp;&nbsp; And that's because cloud code makes it so easy&nbsp; and just straightforward to add unit tests. So, we&nbsp;&nbsp; have great code coverage. And then the other thing&nbsp; we have in cloud code in our own codebase is we&nbsp;&nbsp; have great commits and PR messages because when we&nbsp; finish working we'll just say hey cloud write the&nbsp;&nbsp; commit for me write the PR message for me. We also&nbsp; see great opportunities to use cloud code in kind&nbsp;&nbsp; of the deployment like deployments and in other&nbsp; parts of the life cycle. And this is a few other&nbsp;&nbsp; people have talked about this but this is using&nbsp; the cloud code SDK. So using it headlessly, using&nbsp;&nbsp; it programmatically, being able to sprinkle in a&nbsp; coding agent agent anywhere. And so that's things&nbsp;&nbsp; like sprinkling it into CI/CD to use it in GitHub&nbsp; for instance to help people um programmatically.&nbsp;&nbsp; And then finally, it's great kind of with support&nbsp; and scale. It can help you debug errors faster.&nbsp;&nbsp; One thing that we saw when we started giving cloud&nbsp; code to customers and talking to them about it,&nbsp;&nbsp; we didn't totally predict this was a lot of&nbsp; customers or potential customers said, "Hey,&nbsp;&nbsp; we've been me we've been kind of putting off this&nbsp; like large codebase migration. People that are on&nbsp;&nbsp; old versions of Java trying to get to a new one&nbsp; or a team that's on PHP and they're trying to&nbsp;&nbsp; get to React or Angular. We've talked to multiple&nbsp; teams like this and having a tool like Cloud Code&nbsp;&nbsp; makes projects like that a little more digestible.&nbsp; when you go to your team and you say, "Hey, we're&nbsp;&nbsp; going to spend a month, you know, refactoring or&nbsp; rewriting large parts of the codebase." And then&nbsp;&nbsp; on top of that, and this kind of matters across&nbsp; all these, is once again remember Claude is great&nbsp;&nbsp; at the terminal. And that means it's going&nbsp; to be great at all those different CLI tools,&nbsp;&nbsp; things like Git, Docker, Big Query, things like&nbsp; that. I never have to worry about, oh, I'm going&nbsp;&nbsp; to get myself, how do I get myself out of this&nbsp; sticky rebase? I'll just fire up cloud code and&nbsp;&nbsp; tell it the situation and be like, "Hey, can you&nbsp; fix this for me?" It's incredible. Now, let's&nbsp;&nbsp; talk about best practices. And the first one is&nbsp; not going to be a surprise, but the first one is&nbsp;&nbsp; use claw.md files. So, remember that cloud code,&nbsp; like I said, is an agent and it has some tools,&nbsp;&nbsp; has some lightweight instructions in the prompt,&nbsp; but it doesn't really have memory. And so the main&nbsp;&nbsp; way we share state across kind of sessions&nbsp; or across our team when we fire up cla code&nbsp;&nbsp; in the same codebase over and over again is this&nbsp; cloud.mmd file. So when we start cla what happens&nbsp;&nbsp; is if there's this claw.md file in the working&nbsp; directory it's just plopped into context. It's&nbsp;&nbsp; plopped into the prompt. And basically what it&nbsp; says is hey claude by the way these are important&nbsp;&nbsp; instructions the developer left for you. Be&nbsp; sure to pay close attention to this. And there's&nbsp;&nbsp; various places you can put the cloudmd file. You&nbsp; can put it in a project and check it in so all&nbsp;&nbsp; your teammates share it. You could put one in your&nbsp; home directory if there's things you just want&nbsp;&nbsp; claude to always know about regardless of what&nbsp; you're working on. And the things you put in here&nbsp;&nbsp; are things like, hey, by the way, maybe this is&nbsp; how you run the unit tests. Or just so you know,&nbsp;&nbsp; to make kind of your searching and life easier,&nbsp; here's like just like an overview of kind of how&nbsp;&nbsp; this project is laid out, where the tests live,&nbsp; what different modules are, things like that.&nbsp;&nbsp; or here's our style guide. All sorts of things&nbsp; like that to just make Claude's life a bit easier.&nbsp;&nbsp; And you can build these things up over time.&nbsp; The other thing you can do, which is important,&nbsp;&nbsp; is permission management. When you're running&nbsp; Cloud Code, there's all sorts of different kind&nbsp;&nbsp; of permission things flying by. Kind of out of the&nbsp; box, what happens when you start our tool is for&nbsp;&nbsp; read actions. If Claude is searching or reading,&nbsp; we just let it go. But once it starts writing or&nbsp;&nbsp; running bash commands or doing things that could&nbsp; change change stuff on your machine potentially,&nbsp;&nbsp; that's when we kick in this UI and it says&nbsp; something like yes, yes, always allow this or no,&nbsp;&nbsp; um, I want to do something else. And using that&nbsp; permission management and being smart about it&nbsp;&nbsp; can help you work faster. So there's something&nbsp; called autoaccept mode where if you're working&nbsp;&nbsp; with cloud code and you press shift tab, claude&nbsp; will just start working. There's things you can&nbsp;&nbsp; do like you can configure claude in the settings&nbsp; where specific commands like on bash like if you&nbsp;&nbsp; just are like tired of saying yes run npm run test&nbsp; you can just always approve that. So fiddling with&nbsp;&nbsp; your permission management is a great way to kind&nbsp; of speed up your workflow integration setup. So,&nbsp;&nbsp; one thing that is going to help you get the most&nbsp; out of cloud code is remember that it's great at&nbsp;&nbsp; the terminal. And if there's applications that&nbsp; you use which have kind of a way to access them&nbsp;&nbsp; through CLI and GitHub is a great example of&nbsp; that. They have a powerful tool called GH you&nbsp;&nbsp; can basically give more work to cloud code and&nbsp; you can do that either by just installing more&nbsp;&nbsp; CLI tools or you can attach more MCP servers. Um,&nbsp; I would say just through experience that if you're&nbsp;&nbsp; using something like um, a CLI tool that's well&nbsp; known and well documented and you're trying to&nbsp;&nbsp; choose between the CLI tool and just installing&nbsp; it on your machine and grabbing an MCP server,&nbsp;&nbsp; I would recommend using the CLI tool. Um, and&nbsp; then also if you internally have your own tools&nbsp;&nbsp; at Anthropic, we have something called coup that&nbsp; does a whole bunch of stuff for us. You can also&nbsp;&nbsp; tell Claude about that and you pro that's the sort&nbsp; of thing you'd put in claude.mmd and then context&nbsp;&nbsp; management. So remember that claude is an agent&nbsp; and when it's an a what what it does it's calls&nbsp;&nbsp; these tools and the context builds up and up over&nbsp; time and at least for anthropic our models have a&nbsp;&nbsp; context window of 200,000 tokens and you can max&nbsp; this thing out. So you kind of have two options&nbsp;&nbsp; when you're in a long session with claude and&nbsp; you're working and you're going back and forth.&nbsp;&nbsp; You'll see in the bottom right you'll start to&nbsp; get this little warning that'll say hey you're&nbsp;&nbsp; starting to fill up the context window and kind of&nbsp; depending on what's going on you have two options.&nbsp;&nbsp; You can run slashcle and just start over and that&nbsp; clears everything out except for for instance&nbsp;&nbsp; claw.mmd or you can run slash compact and what'll&nbsp; happen is basically it's like a user message is&nbsp;&nbsp; inserted and it just says something like hey&nbsp; I need to go summarize everything we've been&nbsp;&nbsp; up to. I'm going to give this to another developer&nbsp; and they're going to pick up where I left off. And&nbsp;&nbsp; then that summary is what kind of seeds the next&nbsp; session. You can go from there. We spend a lot of&nbsp;&nbsp; time tuning this kind of compact functionality&nbsp; so that as you max out the context window and&nbsp;&nbsp; then run compact, you can start back over and&nbsp; keep going efficient workflows. What can you do&nbsp;&nbsp; with cloud code? And how do you get the most out?&nbsp; So using planning and to-dos. talked a little bit&nbsp;&nbsp; about this before, but one of the best things you&nbsp; can do is when you open up Cloud Code, instead of&nbsp;&nbsp; saying, "Hey, I need you to fix this bug," you can&nbsp; say, "Hey, I have this bug. Can you search around,&nbsp;&nbsp; figure out what's causing it, and just like tell&nbsp; me a plan how we're going to fix it?" And this can&nbsp;&nbsp; save you a lot of time because you can verify,&nbsp; you can read Claude's plan, and you can verify&nbsp;&nbsp; what it's going to do. And then the other thing&nbsp; that we have is we have this to-do list feature.&nbsp;&nbsp; So often when Claude's working on a big task,&nbsp; it'll create a to-do list. And if you're kind of&nbsp;&nbsp; paying attention, you can kind of watch this to-do&nbsp; list, and if you see anything kind of weirder in&nbsp;&nbsp; there or something that doesn't make sense, that's&nbsp; when you can press escape and say, "Hey Claude,&nbsp;&nbsp; let's change the to-do list. I think you're&nbsp; on the wrong path." Smart vibe coding. So it's&nbsp;&nbsp; very tempting and it's very powerful to just let&nbsp; Claude work and press enter and see what happens&nbsp;&nbsp; at the end. I think there's a few things that can&nbsp; help make this better. And there's I think a talk&nbsp;&nbsp; later today about just this for 30 minutes. But&nbsp; doing things like having test-driven development,&nbsp;&nbsp; having Claude make small changes, run the tests,&nbsp; make sure they pass, always having Claude do&nbsp;&nbsp; things like check the TypeScript and the linting,&nbsp; and then commit regularly so that if it's kind of&nbsp;&nbsp; going off the rails, you can always fall back and&nbsp; try again. You can use screenshots to guide and&nbsp;&nbsp; debug. So Claude is built on top of our models&nbsp; which are multimodal. You can always just grab&nbsp;&nbsp; a screenshot, paste it in, or if you have a file&nbsp; somewhere that's an image, you can just say, "Hey,&nbsp;&nbsp; Claude, look at this mock.png and then build the&nbsp; website for me or whatever." And then advanced&nbsp;&nbsp; techniques. So, as you're getting used to using&nbsp; Claude, what are some things you can think about&nbsp;&nbsp; uh to kind of push things to the next level? And&nbsp; one of the things we see both internally and with&nbsp;&nbsp; customers is when you've started to use this tool&nbsp; for a while, it's going to be very tempting to use&nbsp;&nbsp; multiple clouds at once. And so I know people at&nbsp; Anthropic and a few customers that run four clouds&nbsp;&nbsp; at the same time. There's various ways to do this.&nbsp; You can have it in T-Mox or just different tabs,&nbsp;&nbsp; all sorts of crazy things. So I would challenge&nbsp; you to try getting multiple clubs running at once&nbsp;&nbsp; and kind of be orchestrating all these things.&nbsp; It's quite fun. I can only do two, but I know&nbsp;&nbsp; people that do four. Use escape. So, escape&nbsp; is your best friend. While Claude is working,&nbsp;&nbsp; you can kind of keep an eye on what it's up to,&nbsp; and you can press escape to stop it and interject&nbsp;&nbsp; and say, "Hey, I think you're going on the wrong&nbsp; path, or I want you to do something else." Knowing&nbsp;&nbsp; when the right time to press escape is versus&nbsp; just letting Claude figure it out, is key to&nbsp;&nbsp; getting the most out of the tool. And there's a&nbsp; hidden feature. Not too many people know about it,&nbsp;&nbsp; but if you press escape twice, you can actually&nbsp; jump back in your conversation. You can go back&nbsp;&nbsp; and you can kind of reset tool expansion in MCP.&nbsp; So, this is taking it to the next level. If you&nbsp;&nbsp; feel like with bash and with the tools that cloud&nbsp; has that it still can't do something, this is&nbsp;&nbsp; when you should start looking at MCP servers. And&nbsp; then headless automation. And I think this is the&nbsp;&nbsp; thing we're most excited about, but also we are&nbsp; still trying to wrap our heads around internally,&nbsp;&nbsp; which is how can we use Claude programmatically.&nbsp; We have that in GitHub actions. We want to figure&nbsp;&nbsp; out other creative places we can start using&nbsp; it. I would challenge you all to do the same.&nbsp;&nbsp; So, with that said, uh I'm going to jump over&nbsp; to my computer because there's one other best&nbsp;&nbsp; practice, which is it's always good to stay on&nbsp; top of everything that's new. So, we're shipping&nbsp;&nbsp; super fast. I'm going to throw I'm just going to&nbsp; go over a few things that are new as of today. Um,&nbsp;&nbsp; one thing is when you're in Cloud Now and you fire&nbsp; it up, you can do slashmodel. You can see what&nbsp;&nbsp; model you're running on. I'm on default, which&nbsp; happens to be Sonnet. We can jump over to Opus.&nbsp;&nbsp; You can do the same thing in slashconfig. Switch&nbsp; it here. So that's new. Make sure you're running&nbsp;&nbsp; the model that works for you. There's another&nbsp; thing that's new about these models which is&nbsp;&nbsp; you can say something like um can you figure out&nbsp; what's in this project? And for a long time for a&nbsp;&nbsp; while we've had this like think hard or extended&nbsp; thinking. Now this is great but with our past&nbsp;&nbsp; models the we wouldn't let our model think between&nbsp; tool calls and that's probably when the thinking&nbsp;&nbsp; matters most. So starting with cloud 4 they can&nbsp; now our models now think between tool calls and&nbsp;&nbsp; we can watch this happen. So we have Claude in&nbsp; this project. There's a few different files in&nbsp;&nbsp; here and I'm just going to tell it to think hard&nbsp; and figure out what's in this project and we can&nbsp;&nbsp; watch Claude start to work. And so the way you&nbsp; know you triggered thinking is you'll see kind&nbsp;&nbsp; of this lighter gray text and then it'll call&nbsp; some file, it'll call some tools, it'll read&nbsp;&nbsp; some stuff, and then we see some more thinking.&nbsp; And this is awesome. Um, so I encourage you when&nbsp;&nbsp; you're working on tasks and solving bugs, throw&nbsp; a think hard in there. And then the other thing,&nbsp;&nbsp; and you know what, we'll just throw it up real&nbsp; quick, is I have this in VS Code, but of course&nbsp;&nbsp; this is in Jet Brains as well, but we have these&nbsp; new great integrations with VS Code and and Jet&nbsp;&nbsp; Brains. Um, we can do things like Claude's going&nbsp; to know what file I'm in. What file am I in? That is not what I meant to say,&nbsp;&nbsp; but Claude's going to figure it out.&nbsp; And you can do things like this. So these are the sort of things I would&nbsp; encourage you to stay on top of. We have a public&nbsp;&nbsp; uh kind of GitHub project called Claude Code&nbsp; under Enthropic. You can post issues there,&nbsp;&nbsp; but we also post our change log there. And so I&nbsp; check this once a week and make sure that I'm on&nbsp;&nbsp; top of all the new stuff we're shipping because&nbsp; even I can't keep up with it. So, with that said,&nbsp;&nbsp; we have like four minutes left. I'm happy to&nbsp; answer questions about anything cloud code&nbsp;&nbsp; related. We have it here. I can live demo some&nbsp; stuff if you're interested. Um, let's do a few. Thanks. Real quick, this might be obvious, but&nbsp; multiple cloud MD files in a project. I presume&nbsp;&nbsp; that's possible and it just figures it out or no?&nbsp; So, there's a few options, of course, like in the&nbsp;&nbsp; same directory. You couldn't um but you could have&nbsp; one here and one in a subdirectory. And I think we&nbsp;&nbsp; changed this so that all the subdirectory ones&nbsp; aren't read in because like Anthropic, we have a&nbsp;&nbsp; monor repo and people would open it at the top and&nbsp; blow up their context with all the claud MDs. So,&nbsp;&nbsp; we encourage Claude when it's searching around and&nbsp; it discovers claw.md files in um child directories&nbsp;&nbsp; that are relevant to be sure to read them. But by&nbsp; default, it just reads the cloud MD file in the&nbsp;&nbsp; current working directory when you fire it up.&nbsp; And then also you can set one in like your home&nbsp;&nbsp; directory. Um there are things you can do though.&nbsp; We have this new thing like in your cloud MD you&nbsp;&nbsp; can start referencing other files. So you could&nbsp; for instance um do something like this with an&nbsp;&nbsp; at sign um if you have other cloud MD files that&nbsp; you just kind of know you always want to read in&nbsp;&nbsp; um to do something like that. Hi. Okay. I um have&nbsp; not had luck getting Claude to respect my Claude&nbsp;&nbsp; MD. Like there's one thing particular. Yes. where&nbsp; I'll ask it to refactor something and then it&nbsp;&nbsp; will leave inline comments explaining the like the&nbsp; what of it is and it's like like something that's&nbsp;&nbsp; extremely obvious and so I'll tell it like go and&nbsp; remove any inline comments that describe the what&nbsp;&nbsp; of what's happening and then it will remove it&nbsp; and then immediately do it again and like the same&nbsp;&nbsp; pass. So do you have any strategies for dealing&nbsp; with that? So there's kind of two things that&nbsp;&nbsp; fix that. So that was actually kind of a model&nbsp; problem. There's nothing in the prompt. We have&nbsp;&nbsp; actually a lot in the prompt for 37 that said,&nbsp; "Whoa, do not leave comments." And despite that,&nbsp;&nbsp; the model just loves to leave comments. Um, so it&nbsp; doesn't surprise me that your cloud MD didn't help&nbsp;&nbsp; much either. We already did a lot I did a lot of&nbsp; work to try to tamp it down from what happens out&nbsp;&nbsp; of the box. So we mostly fixed that in Cloud 4.&nbsp; Now there might be some new weird behavior quirks,&nbsp;&nbsp; but the other thing we made better in Cloud 4&nbsp; is it's just better at following instructions.&nbsp;&nbsp; Um, and we've gotten a lot of feedback from&nbsp; early testers that, uh, all of a sudden, whoa,&nbsp;&nbsp; my cloud MD is being followed way more closely.&nbsp; Um, and it might be a good chance to go look in&nbsp;&nbsp; your CloudMD and decide, do I still need this&nbsp; stuff? Maybe I can take some of it out. Maybe&nbsp;&nbsp; I need to add a few new things. So, moving&nbsp; over to the new models might be a good time&nbsp;&nbsp; to take another look at what's in there and&nbsp; see what you need and what maybe can go. Uh,&nbsp;&nbsp; for the record, I'm trying to think of something&nbsp; that you might not have thought of. We're doing&nbsp;&nbsp; multi- aent execution and parallelization.&nbsp; Can you make it so that for four agents,&nbsp;&nbsp; say agents two and three use the context from&nbsp; agent one, maybe agent four uses the context from&nbsp;&nbsp; agent two at a certain point. Yeah. Um yeah,&nbsp; etc. That's interesting. We're trying to So,&nbsp;&nbsp; kind of like I said at the beginning, we're&nbsp; trying to do the simple thing that works,&nbsp;&nbsp; which is just one agent that's great at coding&nbsp; and does everything. Um I think we want to figure&nbsp;&nbsp; that out. Probably what's going to happen is if&nbsp; you wanted to do that, you would ask all your&nbsp;&nbsp; agents to probably like write to a shared markdown&nbsp; file or something like that so they can all kind&nbsp;&nbsp; of like check in and communicate. Um, sometimes&nbsp; like I'll be working with cloud.md or claude and&nbsp;&nbsp; I'll just say like, "Hey, I need you to write some&nbsp; stuff in like ticket.md for another developer and&nbsp;&nbsp; then I'll fire up another cloud code and I'll be&nbsp; like, hey, read ticket.md like another developer&nbsp;&nbsp; left this note for you. Like this is what you're&nbsp; going to work on." So, I would think about trying&nbsp;&nbsp; to write that state to a file and then just kind&nbsp; of like count on the model's ability to just like&nbsp;&nbsp; read files and make sense them um is probably the&nbsp; best you can do today. And maybe we'll figure out&nbsp;&nbsp; clever ways to expose that uh in the product as&nbsp; something more native. Cool. All right. And with&nbsp;&nbsp; that said, I have some rare clawed code stickers&nbsp; that I found in my backpack. So, come find me.&nbsp;&nbsp; I'll be hanging out over there or something.&nbsp; Um, happy to share them. Thank you. [Applause]

---

## 5. MCP 201

**Preview:**
> Kind: captions Language: en [Music] [Applause] Well, hello. Uh, my name&nbsp; is David. I'm a member of technical staff at&nbsp;&nbsp; Anthropic and one of the co-creators of uh, MCP.&nbsp; And today I'm going to tell you a little bit more&nbsp;&nbsp; about the protocol and the things you can do&nbsp; um just to give you an understanding of um what&nbsp;&nbsp; there's more to the protocol than what most people&nbsp; use it for at the moment which would be tools. So&nbsp;&nbsp; really the goal to...

**Full Transcript:**

Kind: captions Language: en [Music] [Applause] Well, hello. Uh, my name&nbsp; is David. I'm a member of technical staff at&nbsp;&nbsp; Anthropic and one of the co-creators of uh, MCP.&nbsp; And today I'm going to tell you a little bit more&nbsp;&nbsp; about the protocol and the things you can do&nbsp; um just to give you an understanding of um what&nbsp;&nbsp; there's more to the protocol than what most people&nbsp; use it for at the moment which would be tools. So&nbsp;&nbsp; really the goal today is to showcase you what&nbsp; the protocol is capable of and how you can use&nbsp;&nbsp; it in ways to build richer interactions with MCP&nbsp; clients. um that goes beyond the tool call tool&nbsp;&nbsp; calling that most people are used to. And I will&nbsp; first go through all the different like what we&nbsp;&nbsp; call primitives like ways for the servers to&nbsp; expose information to a client before we go&nbsp;&nbsp; into some of the bit more lesser known aspects of&nbsp; the protocol and then I want to talk a little bit&nbsp;&nbsp; about like how to build a really rich interaction&nbsp; before we take a little stab of what's coming next&nbsp;&nbsp; for MCP and how we bring MCP to the web. But to&nbsp; just get you started, I want to talk about one of&nbsp;&nbsp; the MCP primitives um that servers can expose to&nbsp; MCP clients that very few people know. And those&nbsp;&nbsp; are called prompts. And what a prompts are really&nbsp; are predefined templates for AI interactions.&nbsp;&nbsp; And that's to say it's a way for an MCP server&nbsp; to expose a set of text, you know, like a prompt&nbsp;&nbsp; in a way um that allows um users to directly um&nbsp; add this to the context window and see how they&nbsp;&nbsp; would use for example the MCP uh server uh you're&nbsp; building. And there are really the two main use&nbsp;&nbsp; cases here is for you as an MCP server author to&nbsp; provide an example for um that you can showcase&nbsp;&nbsp; to the user so that they know how to use the MCP&nbsp; server in the best way because realistically you&nbsp;&nbsp; are the one who has built it. You are the one&nbsp; who knows how to use it in the best possible&nbsp;&nbsp; way and probably at the time you would release&nbsp; it are the one who has used it the most time.&nbsp;&nbsp; But since MCP uh prompts are also dynamic in&nbsp; a way, they're just code under the hood that&nbsp;&nbsp; are executed in MCP server, they allow you to do&nbsp; even richer things than that. What you can do and&nbsp;&nbsp; I want to showcase this in this scenario is an MCP&nbsp; prompt that a user invokes um in this uh Z editor&nbsp;&nbsp; here that will fetch directly GitHub comments&nbsp; that um into my context window. And so what you&nbsp;&nbsp; see me here doing is just basically um put into&nbsp; the context window um the comments from my pull&nbsp;&nbsp; request that is that you know I've written so that&nbsp; I can go and interact with it and have then the&nbsp;&nbsp; model go and help me you know apply the changes&nbsp; that has been requested to me or whatever I want&nbsp;&nbsp; to do. And so this is really a way for exposing&nbsp; things that the user should directly interact&nbsp;&nbsp; and the user should directly wants to put into the&nbsp; context window before it interacts with the yellow&nbsp;&nbsp; lamb. So it's different from that from tools where&nbsp; the model decides when to do it. This is what that&nbsp;&nbsp; the user decides um I want to add this um to&nbsp; the context window. And if you look carefully,&nbsp;&nbsp; there's one additional thing that very very few&nbsp; people know um that you can do and that is prompt&nbsp;&nbsp; completion. So if you have looked carefully, there&nbsp; was a way where it showcased quickly a popup of me&nbsp;&nbsp; selecting the poll requests that are available&nbsp; to myself. And that is a way that you can that&nbsp;&nbsp; is a thing that you can provide as an MCP server&nbsp; author to build richer parameterized templates&nbsp;&nbsp; for example. And this is exceptionally easy to&nbsp; do in the code. Like if you're in Typescript,&nbsp;&nbsp; building a prompt that provides users with um&nbsp; like such a template and have parameters for it&nbsp;&nbsp; and like autocomp completion is nothing more than&nbsp; a few lines of code that cloud code together with&nbsp;&nbsp; cloud 4 can most of the time write basically&nbsp; for you. And it's just that simple. It's a&nbsp;&nbsp; function for the completion. It's a function for&nbsp; generating the prompt. And so this is already like&nbsp;&nbsp; one of these primitives you can use to build&nbsp; an interaction for users with an MCP server,&nbsp;&nbsp; but it's just a little bit more richer than a tool&nbsp; call. And a second one of these is something that&nbsp;&nbsp; we call resources. It's another primitive than&nbsp; an MCP server can expose to an MCP client. And&nbsp;&nbsp; while prompts are really focused on text snippets&nbsp; that a user can add uh into the context window,&nbsp;&nbsp; resources are about exposing raw data or content&nbsp; from a server. And why would you want to do this?&nbsp;&nbsp; There are two ways why you want to do this. So&nbsp; one thing is most of the clients today would&nbsp;&nbsp; allow you to add this raw content directly to the&nbsp; context window. So in that way they're not that&nbsp;&nbsp; different from context uh from prompts but it also&nbsp; allows application to do additional things to that&nbsp;&nbsp; raw data and that could be for example building&nbsp; embeddings around this data the server exposes and&nbsp;&nbsp; then do retrieval augmented uh generation um by&nbsp; adding to the context window the most appropriate&nbsp;&nbsp; things. And so this is an area that at the moment&nbsp; I feel is a bit underexplored. And I just want&nbsp;&nbsp; to quickly showcase you uh how resources work.&nbsp; In this case, this is again uh one of these um&nbsp;&nbsp; uh ways where an MCP client exposes a resource um&nbsp; as a file like object. And in this scenario here,&nbsp;&nbsp; we are exposing the database schema for apostra&nbsp; database um as resources and then you can add them&nbsp;&nbsp; in cloud desktop just like files and that way&nbsp; you can tell claude this is the tables I care&nbsp;&nbsp; about and now please go ahead and visualize them.&nbsp; And so in this scenario, what you're going to see&nbsp;&nbsp; is Claude is going to go uh and write a beautiful&nbsp; diagram that visualizes the database schema for&nbsp;&nbsp; me. And I've exposed the schema via resources.&nbsp; There's a lot of unexport space still here. Again,&nbsp;&nbsp; if you go beyond just like adding it file and&nbsp; think about like retrieval augmentation or any&nbsp;&nbsp; other thing the application might want to do. And&nbsp; so those are two primitives. one is prompts again&nbsp;&nbsp; the things that the user interacts with there's&nbsp; the second one is resources that the application&nbsp;&nbsp; interact with then of course there should be a&nbsp; third one that you all are very familiar with&nbsp;&nbsp; um that I don't want to get into too much depth&nbsp; because if you have built an MCP server you&nbsp;&nbsp; probably have built it for exposing a tool and&nbsp; so tools are really these actions of course that&nbsp;&nbsp; can be invoked that's like one of the I think&nbsp; most magical moment I feel when you build an&nbsp;&nbsp; MCP server is when the model for the first&nbsp; time invokes something that you care about&nbsp;&nbsp; that you have built for and has this little&nbsp; impact on you know it might be like quing a&nbsp;&nbsp; database for you or whatever it might be. Um but&nbsp; this is again the thing that the model decides&nbsp;&nbsp; when to call to an action. And so these are three&nbsp; very basic primitives that the protocol exposes. And if you think carefully about these three&nbsp; primitives that I just showcased you to to you,&nbsp;&nbsp; there's a little bit of overlap about like&nbsp; how do you use like they could like when do&nbsp;&nbsp; you use what really and so there's something&nbsp; that very that we don't talk enough about and&nbsp;&nbsp; it's somewhere buried in the specification&nbsp; language of the model context protocol is&nbsp;&nbsp; what I call the interaction model and I think&nbsp; showcasing it hopefully makes clear when you use&nbsp;&nbsp; What? Because the interaction model is built in&nbsp; such a way that you can expose the same data in&nbsp;&nbsp; three different ways depending on when you&nbsp; want to have it show up. And prompts again&nbsp;&nbsp; are these userdriven things. It's the thing&nbsp; the user invokes adds to the context window.&nbsp;&nbsp; And the most common scenario where how you see&nbsp; these pop up is a slash command, an add command,&nbsp;&nbsp; something like that. Resources on the other hand&nbsp; are all applicationdriven. The application that&nbsp;&nbsp; uses the LLM be it cloud desktop be it VS code&nbsp; something like that fully is decides what it&nbsp;&nbsp; wants to do with that. And then lastly tools&nbsp; are driven by the model. in between, you know,&nbsp;&nbsp; an AI application using a model and a user, we&nbsp; have all three parts that we eventually cover&nbsp;&nbsp; using these three basic primitives. And that&nbsp; allows you already to go to a little bit of a&nbsp;&nbsp; richer application and experience than what most&nbsp; people can currently do with tools because you&nbsp;&nbsp; just have a way to interact with the user a bit&nbsp; more nuanced um than if you just wait for this&nbsp;&nbsp; model to call the tool. But we can even go beyond&nbsp; that because while these basic primitives get us&nbsp;&nbsp; a little bit further than what we see most MCP&nbsp; servers do at the moment, there are even richer&nbsp;&nbsp; interactions that we want to enable. And to make&nbsp; this a bit more understandable, here's a really&nbsp;&nbsp; an example I want to give you um that showcases&nbsp; this problem. So how can you build an MCP server&nbsp;&nbsp; for example that summarizes a discussion from an&nbsp; issue tracker? So on one hand side I can build&nbsp;&nbsp; an MCP server that exposes this kind of data very&nbsp; simple and that's quite clear but how do I do the&nbsp;&nbsp; summarization step because for the summarization&nbsp; step I obviously need a model and so there in one&nbsp;&nbsp; way to go and build this is you can build an&nbsp; MCP server that is this issue tracker server&nbsp;&nbsp; and you have a choice here you can bring your own&nbsp; SDK and call the model have the model summarizes&nbsp;&nbsp; But then there's a little problem to that and the&nbsp; problem is that the client has a model selected&nbsp;&nbsp; be it like clawed or whatever else but the server&nbsp; the MCP server that you've built doesn't know what&nbsp;&nbsp; model the client has configured and so you bring&nbsp; your own SDK like of of a model provider and be&nbsp;&nbsp; it the anthropic SDK you still need then like&nbsp; a API key that this user needs to provide and&nbsp;&nbsp; gets very quickly very awkward and so MCP has&nbsp; a little hidden feature or a little primitive&nbsp;&nbsp; called sampling that allows a server to request&nbsp; a completion from the client. What does this&nbsp;&nbsp; mean? It means that the server can use a model&nbsp; independently from like don't having to provide an&nbsp;&nbsp; SDK itself but in but asks the client which model&nbsp; you have configured and the client is the one&nbsp;&nbsp; providing the completion to the server. And what&nbsp; does this do? It does two things. First of all,&nbsp;&nbsp; it allows the client to get full control over&nbsp; the security, privacy and the cost. So instead of&nbsp;&nbsp; having to provide an additional API key, you might&nbsp; tap into the subscription that your client might&nbsp;&nbsp; already have. But it allows also um a second part&nbsp; which is that if you take multiple um if you chain&nbsp;&nbsp; MCP servers in an interesting way, it makes this&nbsp; whole pattern very recursive. And what do I mean&nbsp;&nbsp; by that? It's a bit abstract. You can take an MCP&nbsp; server that exposes a tool, but during the tool&nbsp;&nbsp; execution, you might want to use more MCP servers&nbsp; downstream. And somewhere downstream in this like&nbsp;&nbsp; system, there might be then your Asia Tracker&nbsp; server that wants to go and have a completion.&nbsp;&nbsp; But using sampling you can bubble up the requests&nbsp; such that the client always stays in full control&nbsp;&nbsp; over the cost of the subscription whatever&nbsp; you want to use. It stays in full control of&nbsp;&nbsp; the privacy over the cost of this interaction and&nbsp; basically manages every interaction um that an MCP&nbsp;&nbsp; server wants to do with a model. And that allows&nbsp; for very powerful chaining and it allows for like&nbsp;&nbsp; more complex patterns that go already into like&nbsp; ways of how you can build little MCP agents. But that's sampling. Sampling at the moment is&nbsp; sadly I think one of the more exciting features&nbsp;&nbsp; but also one of the features that's the least&nbsp; supported in clients. for our first party projects&nbsp;&nbsp; uh products. We will bring um sampling somewhere&nbsp; this year. Um and so then you can hopefully start&nbsp;&nbsp; building more exciting MCP servers. And then&nbsp; there's the last primitive that I want to touch&nbsp;&nbsp; on that's also a bit more interesting and it's&nbsp; one of these things that in retrospective as one&nbsp;&nbsp; of the person who has built the protocol um I've&nbsp; probably named terribly to be fair I'm not a very&nbsp;&nbsp; not not very good at naming and you will see this&nbsp; throughout the talk probably but there's a thing&nbsp;&nbsp; called roots and roots is also an interesting&nbsp; aspect because let's imagine I want to build today&nbsp;&nbsp; an MCP server that deals with my git commands I&nbsp; don't want to deal with git. I don't want to do&nbsp;&nbsp; source control commands. I don't remember any of&nbsp; that. I want to have MCP deal like an MCP server&nbsp;&nbsp; deal with this. So now I'm going to hook up an&nbsp; MCP server into my favorite IDE. But how does the&nbsp;&nbsp; IDE know how does the MC sorry, how does the MCP&nbsp; server know what are the open projects in the IDE?&nbsp;&nbsp; Because obviously I want to run the git commands&nbsp; in the workspaces I have open, right? And so roots&nbsp;&nbsp; is a way for the server to inquire from the client&nbsp; such as VS code for example what are the projects&nbsp;&nbsp; you have open so that I can operate within only&nbsp; those directories that the server has opened and I&nbsp;&nbsp; know where I want to execute my git commands and&nbsp; this again is a feature that's not that widely&nbsp;&nbsp; used but for example VS code currently does&nbsp; support this and so these These are, you know,&nbsp;&nbsp; just all the big primitives that MCP offers. So,&nbsp; we have five primitives, three on the server side,&nbsp;&nbsp; two on the client side. But how you put it all&nbsp; together to actually build a rich interaction&nbsp;&nbsp; because that's what we want. We want to build&nbsp; something for users that's a bit richer than&nbsp;&nbsp; just tool calling. And so, let's take a look at&nbsp; how you will build a hypothetical MCP server that&nbsp;&nbsp; interacts with your favorite chat application, be&nbsp; that Discord, be it Slack. You could use prompts&nbsp;&nbsp; to give examples to users such as like summarize&nbsp; this discussion and you can use completions with&nbsp;&nbsp; recent threads, users or whatever you want them&nbsp; to expose. You can have additional prompts like&nbsp;&nbsp; what's new, what happened since yesterday. And so&nbsp; that's one way the user can just kickstart right&nbsp;&nbsp; away into using the server you have provided and&nbsp; get the ideas. um that you how you intended it to&nbsp;&nbsp; be used and then you can use resources to directly&nbsp; list the channels to expose recent threats that&nbsp;&nbsp; happened in the in the you know chat application&nbsp; such that the MCP client can index it deal with&nbsp;&nbsp; it in ways um that it that it wants. And then&nbsp; of course last but not least we still have our&nbsp;&nbsp; tools. We have search, we have read channels, we&nbsp; have reading a threads and we would use sampling&nbsp;&nbsp; to summarize a thread for example and really&nbsp; expose this. And that's a way to really build a&nbsp;&nbsp; much much much richer experience with MCP to use&nbsp; the full power that the protocol has to offer. But this is just the the beginning because most&nbsp; of these experiences if we build MCP servers so&nbsp;&nbsp; far have been experiences that stayed local. Out&nbsp; of the 10,000 MCP servers the community has built&nbsp;&nbsp; over the last six to seven months. The vast&nbsp; majority are local experiences. But I think we&nbsp;&nbsp; can take the next step and I think this is MCP's&nbsp; really big next thing is bringing MCP servers away&nbsp;&nbsp; from the local experience to the web. And so what&nbsp; does this mean? It means that instead of having&nbsp;&nbsp; an MCP server that is, you know, a Docker&nbsp; container or some form of local executable,&nbsp;&nbsp; it is nothing else but a website that your client&nbsp; can connect to and exposes MCP and you talk to. But for that we need two critical components.&nbsp; We need authorization and we need scaling. And in the most recent specification of&nbsp; MCP, we made a ton of changes towards&nbsp;&nbsp; this from the lessons we've learned and&nbsp; the feedback we honestly got from the&nbsp;&nbsp; community as well as like key partners.&nbsp; And we work closely for example with like&nbsp;&nbsp; people in the industry that worked on&nbsp; and other aspects to get this right. And so with authorization in MCP, what you can do&nbsp; is you can basically provide the private context&nbsp;&nbsp; of a user that might be behind an online&nbsp; account or something directly to the LLM&nbsp;&nbsp; application. And it really enables MCP authors to&nbsp; bind the capability of the MCP servers to a user,&nbsp;&nbsp; an online account, something like that. And in&nbsp; order to do that, the way this currently has to&nbsp;&nbsp; work in MCP is that you do this by providing&nbsp; OOTH as the authorization layer. And the MCP&nbsp;&nbsp; specification basically says you need to do O 2.1.&nbsp; And that's a bit daunting because very few people&nbsp;&nbsp; know what OOTH 2.1 is. But OA 2.1 is usually&nbsp; just O 2.0 with all the basic things you would&nbsp;&nbsp; do anyway. all these security considerations that&nbsp; people that have done a wall telling you anyway to&nbsp;&nbsp; do. So it's just OS 2.0 zero a little bit cleaned&nbsp; up and you're probably already doing it if you're&nbsp;&nbsp; doing a wall and if you do implement this wall&nbsp; flow you get two interesting patterns out of that&nbsp;&nbsp; and the first one is the scenario of an MCP&nbsp; server in the web and a good example of this&nbsp;&nbsp; is if you for example a payment provider and you&nbsp; have you know website payment.com and I as a user&nbsp;&nbsp; have an online account there now I as the payment&nbsp; provider can expose mcp.payment.com that the user&nbsp;&nbsp; can put into an MCP client and the MCP client will&nbsp; do the or flow. I log in as my account and I know&nbsp;&nbsp; this is payment.com. I know this is the the person&nbsp; that is my online account with the provider that I&nbsp;&nbsp; trust. I don't trust some random Docker container&nbsp; running locally built by a third party developer&nbsp;&nbsp; anymore. I trust the person I already trust with&nbsp; the data anyway and their developers. And on the&nbsp;&nbsp; their development side, they can just ex like&nbsp; ex like update this server as they want and they&nbsp;&nbsp; don't have to wait for me to download a new like&nbsp; docker image. And so this is I think will be a&nbsp;&nbsp; really really big step for enabling MCP servers to&nbsp; be exposed on the web and MCP clients to interact&nbsp;&nbsp; basically with all the online interactions&nbsp; that you already have. And here's just a small&nbsp;&nbsp; little example of this. In this scenario, we use&nbsp; Cloud AAI integrations which we launched earlier&nbsp;&nbsp; um this month to connect to a remote server and&nbsp; use this oath flow to log in our user to then have&nbsp;&nbsp; tools available that are aware of my data that I&nbsp; care about it are for it is for me. But it enables&nbsp;&nbsp; another aspect. It enables enterprises to smoothly&nbsp; integrate MCP into their ecosystem how they&nbsp;&nbsp; usually build applications. And what does this&nbsp; mean? It means that internally they can deploy an&nbsp;&nbsp; MCP server to their internet or whatever like they&nbsp; use and use an identity provider like Azure ID or&nbsp;&nbsp; Octar or whatever that central identity provider&nbsp; that you usually use for single sign on and you&nbsp;&nbsp; can have that still exist and it will be the one&nbsp; that um gives you the tokens that you require to&nbsp;&nbsp; interact with the MCP server. And that has a lot&nbsp; to say that what it ends up with is a very smooth&nbsp;&nbsp; integration. You as a development team internally,&nbsp; you're going to build an MCP server that you&nbsp;&nbsp; control that you could control the deployment and&nbsp; the user just logs in in the morning with their&nbsp;&nbsp; normal SSO like you always would do and anytime&nbsp; they use an MCP server from them on on out,&nbsp;&nbsp; they will just be logged in and have access to the&nbsp; data that you know the that is their data that the&nbsp;&nbsp; company has for them. And so this I think enables&nbsp; a new way that I've already seen some of the big&nbsp;&nbsp; enterprises do to build really vast systems of&nbsp; MCP servers that allow um part of the company&nbsp;&nbsp; to build a server um while the other part deals&nbsp; about the integrations really nicely separates&nbsp;&nbsp; uh integration builder and platform builders. And&nbsp; then the second part that we require is scaling.&nbsp;&nbsp; And we just added a new thing called streamable&nbsp; HTTP which is just to say it's a lot of lot of&nbsp;&nbsp; words to say basically we want MCP servers&nbsp; to scale similar to normal APIs. And it's as&nbsp;&nbsp; simple as that. you have as a server author you&nbsp; can choose to either return results directly as&nbsp;&nbsp; you would be in in a REST API except that it's not&nbsp; quite just REST or if you need to you can open a&nbsp;&nbsp; stream and get richer interactions. So in the most&nbsp; simple way, you just want to provide a tool call&nbsp;&nbsp; result. You get a request, you return application&nbsp; JSON and off you go. End of the story. You close&nbsp;&nbsp; the connection and the next connection come in&nbsp; and uh you know get served by yet another Lambda&nbsp;&nbsp; function. But if you need richer interactions&nbsp; such as notification or features we talked&nbsp;&nbsp; about like sampling, a request comes in, you&nbsp; start a stream, the client accepts the stream,&nbsp;&nbsp; and now you're being able to send additional&nbsp; things to the client before you're returning&nbsp;&nbsp; finally the result. And those authorization&nbsp; and scaling together is really the foundation&nbsp;&nbsp; to make MCP go from this local experience now to&nbsp; be truly a standard for how LLM applications will&nbsp;&nbsp; interact with the web. And just to finish it all&nbsp; up, I just want to show you quickly about like&nbsp;&nbsp; what's coming for MCP in the next few months&nbsp; of some of the the most important highlights.&nbsp;&nbsp; And the most important part is that um we are&nbsp; starting to think more and more about agents and&nbsp;&nbsp; there's a lot to do there. There are a synchronous&nbsp; task that you of course want to run things that&nbsp;&nbsp; are longer running that are not just like a minute&nbsp; long but maybe a few few like hours long task that&nbsp;&nbsp; an agent takes and that eventually I want to&nbsp; have a result for. So we think a lot about&nbsp;&nbsp; that and we're going to work to build primitives&nbsp; for that into MCP in the near future. The second&nbsp;&nbsp; part of that is elicitation. So really MCP server&nbsp; authors being able to ask for input from the user&nbsp;&nbsp; and that is something that's going to land just&nbsp; about today or on Monday in the in the protocol.&nbsp;&nbsp; And then we're doing two additional things. We&nbsp; first and foremost going to build an official&nbsp;&nbsp; registry to make sure there's a central place&nbsp; where you can find and publish to MCP servers.&nbsp;&nbsp; um so that we can really have one common place&nbsp; where we're going to look for these servers but&nbsp;&nbsp; also allow agents to dynamically download servers&nbsp; and install them and use them and then of course&nbsp;&nbsp; we're thinking more about multimodality and&nbsp; that can be for example streaming of results&nbsp;&nbsp; but that can have other aspects that I just don't&nbsp; want to go into details yet and that's just the&nbsp;&nbsp; specification part on the e on the ecosystem part&nbsp; we going and having a more things to go that we're&nbsp;&nbsp; doing at the moment. We're adding a Ruby SDK that&nbsp; is uh donated by Shopify uh in the next few weeks&nbsp;&nbsp; and the Google uh folks, the Google Go team is&nbsp; currently building an official Go SDK for MCP. And&nbsp;&nbsp; so I just hope that I was able to give you a bit&nbsp; of a more in-depth view of what you could do with&nbsp;&nbsp; MCP if you use the full power of the protocol.&nbsp; And with that, I think I'm bit low on time,&nbsp;&nbsp; so I can't ask question. We can't ask questions.&nbsp; We can't do Q&amp;A, but just grab me afterwards&nbsp;&nbsp; and I can happy to answer on the hallway any&nbsp; questions you might have. So, thank you so much. [Music]

---

## 6. MCP at Sourcegraph

**Preview:**
> Kind: captions Language: en All right. How's everyone doing today? Cool.&nbsp; Awesome. So, my name is uh Byang. I'm the CTO&nbsp;&nbsp; and co-founder of a company called SourceCap.&nbsp; Uh there might be some of you who have used our&nbsp;&nbsp; products inside your development organizations. We&nbsp; build developer tools for professional engineers&nbsp;&nbsp; working in large complex code bases. And for those&nbsp; of you who haven't heard of us, we actually serve&nbsp;&nbsp; uh I think it'...

**Full Transcript:**

Kind: captions Language: en All right. How's everyone doing today? Cool.&nbsp; Awesome. So, my name is uh Byang. I'm the CTO&nbsp;&nbsp; and co-founder of a company called SourceCap.&nbsp; Uh there might be some of you who have used our&nbsp;&nbsp; products inside your development organizations. We&nbsp; build developer tools for professional engineers&nbsp;&nbsp; working in large complex code bases. And for those&nbsp; of you who haven't heard of us, we actually serve&nbsp;&nbsp; uh I think it's like seven of the top 10 software&nbsp; engineering companies by market cap and six of&nbsp;&nbsp; the top 10 US banks and just multitudes of&nbsp; companies building software and writing code&nbsp;&nbsp; uh across basically every industry vertical. And&nbsp; today I'm here to talk about uh our journey with&nbsp;&nbsp; uh MCP and in particular uh you know how we're&nbsp; integrating the model context protocol deeply&nbsp;&nbsp; into the fabric of our uh architecture. So our&nbsp; journey actually began uh quite some time ago.&nbsp;&nbsp; It was actually summer of last year when this&nbsp; fell uh David uh at Enthroic uh maybe you've&nbsp;&nbsp; heard of him. He's now one of the the co-creators&nbsp; of MCP. he reached out and said, "Hey, uh, I heard&nbsp;&nbsp; you you guys are doing a lot with uh, you know,&nbsp; retrieval augmented code generation and fetching&nbsp;&nbsp; the appropriate context in the context window&nbsp; and getting models to perform better on coding&nbsp;&nbsp; and technical question answering. We're working on&nbsp; a thing that uh, you might find interesting." And&nbsp;&nbsp; we're like, "Huh, that sounds interesting. What is&nbsp; it?" And he was like, "It's kind of like LSP uh,&nbsp;&nbsp; but for model context." And we're like, "Wow,&nbsp; that does sound interesting." And so we started&nbsp;&nbsp; chatting and we ended up becoming one of the early&nbsp; design partners uh for the the the MCP protocol&nbsp;&nbsp; um and and gave a lot of feedback and it was&nbsp; really a privilege to work with David and the team&nbsp;&nbsp; uh to to kind of guide the evolution of that&nbsp; protocol and in in in these conversations as&nbsp;&nbsp; we're playing more around with the protocol and&nbsp; experimenting with tools in conjunction with the&nbsp;&nbsp; uh you know developer tools that we're building&nbsp; uh we soon came to this realization that holy&nbsp;&nbsp; crap like you know AI is changing everything&nbsp; but everything is about to change again. Um&nbsp;&nbsp; and specifically we felt that uh tool&nbsp; calling models in conjunction with MCP&nbsp;&nbsp; uh was going to lead to another paradigm shift in&nbsp; the standard like AI application architecture and&nbsp;&nbsp; I think we've been through uh I would say like&nbsp; three waves of AI application architecture uh so&nbsp;&nbsp; far the first wave was sort of the co-pilot wave&nbsp; where uh basically the the architecture of those&nbsp;&nbsp; applications was dictated by the capabilities of&nbsp; the first LMS that were pushed into production.&nbsp;&nbsp; So if you go back to the ancient year of uh&nbsp; 2022 and remember what AI was like back then,&nbsp;&nbsp; uh all the models then hadn't yet been tuned&nbsp; to respond in a chat fashion or use tools.&nbsp;&nbsp; They were just these kind of like text completion&nbsp; models. And so all the big applications that you&nbsp;&nbsp; saw built on top of AI followed this paradigm. You&nbsp; know, the human would type some stuff and then the&nbsp;&nbsp; model would complete the next couple of tokens and&nbsp; the human would type some more. And that was kind&nbsp;&nbsp; of the interaction paradigm. And then chat GBT&nbsp; came along and that ushered in a new modality. So&nbsp;&nbsp; uh everyone soon realized like wow like being&nbsp; able to chat with this thing and make explicit&nbsp;&nbsp; asks is really powerful. And one of the things&nbsp; that we soon realized in that world was like&nbsp;&nbsp; hey if you copy and paste relevance code snippets&nbsp; into the context window and then ask it to answer&nbsp;&nbsp; the question. It gets a lot better in terms of&nbsp; quality and usefulness on production code bases.&nbsp;&nbsp; And that's what I like to call the kind of like&nbsp; rag chat era of AI. And I think you know a lot&nbsp;&nbsp; of folks are still living uh in this world to&nbsp; a certain extent but you all the folks at this&nbsp;&nbsp; conference uh I think we are all aware that we're&nbsp; kind of entering have already entered into a new&nbsp;&nbsp; era which is the era of agents and just as the&nbsp; two generations before uh this era is really being&nbsp;&nbsp; dictated by the capabilities at the model layer&nbsp; and so when we took a look at our tooling suite&nbsp;&nbsp; that we had built so far um the more we looked&nbsp; the more we realized like my gosh like the a&nbsp;&nbsp; lot of the underlying assumptions of building on&nbsp; top of LMS have changed with tool calling agents&nbsp;&nbsp; and MCP we might have to rethink you know this&nbsp; application uh from the ground up uh to to to&nbsp;&nbsp; build truly agentically and so that's essentially&nbsp; what we did we built a completely new coding agent&nbsp;&nbsp; called AMP from the ground up uh and I'll show uh&nbsp; off to you uh what it's able to do and I think the&nbsp;&nbsp; best way to talk about um how we're uh how how&nbsp; we've been building with uh the tool calling uh&nbsp;&nbsp; models anthropics have been shipping shipping&nbsp; in conjunction with the model context protocol&nbsp;&nbsp; is to show you AMP in action and show you it&nbsp; using a bunch of tools to complete tasks. So&nbsp;&nbsp; uh this demo I think is going to be I've been&nbsp; watching all the talks and I think this is this&nbsp;&nbsp; might be the longest uh live demo of the day. So&nbsp; everything that you all the prayers that you said&nbsp;&nbsp; for Brad and other folks please uh say for me this&nbsp; is a live AI demo. What we're going to do is we're&nbsp;&nbsp; going to make a live change to AMP itself. So I&nbsp; have sort of described the change I want uh in&nbsp;&nbsp; this linear issue. Um it's simple because this is&nbsp; a demo and I want to make it easy to gro. Um and&nbsp;&nbsp; basically all we're going to do is we're going to&nbsp; change the background panel of uh AMP to the color&nbsp;&nbsp; red. I've described it in that linear issue. I've&nbsp; given some instructions just as I would in like a&nbsp;&nbsp; uh an actual linear issue that I handed off to an&nbsp; engineering uh member of my team and uh I'm just&nbsp;&nbsp; going to have it implement the issue. So implement&nbsp; I'll just paste in the URL of the issue and the&nbsp;&nbsp; first thing it does is it actually uses a linear&nbsp; tool that it has provided through an MCP server&nbsp;&nbsp; to fetch the contents of that linear issue. So the&nbsp; issue that we're just looking at now the model has&nbsp;&nbsp; access to. I didn't have to atmention it or prompt&nbsp; it in any special way. It just knew to use that&nbsp;&nbsp; tool to fetch uh that piece of context. And it's&nbsp; going to do a couple more agentic steps here to&nbsp;&nbsp; find the appropriate context within the codebase.&nbsp; But I just want to point out that you know this&nbsp;&nbsp; linear uh tool is it's not a firstparty tool. It&nbsp; is actually the official linear MCP server which I&nbsp;&nbsp; currently think is one of the best MCP servers out&nbsp; there. Um the way we've implemented it is you just&nbsp;&nbsp; plug in the the URL. Uh we actually have this uh&nbsp; remote MCP proxy uh that I'll talk about a little&nbsp;&nbsp; bit later that kind of secures the connection&nbsp; and handles the the the secret exchange uh with&nbsp;&nbsp; linear or whatever upstream service you're talking&nbsp; to. Uh and that's how we're integrating uh this&nbsp;&nbsp; capability into our coding agent. So it's going&nbsp; to do a bunch more things. It's going to search&nbsp;&nbsp; around. We'll just let it go uh for a bit. In the&nbsp; meantime, I thought what we could do is talk a&nbsp;&nbsp; little bit more about the AMP architecture and how&nbsp; MCP plays into that. And I could just, you know,&nbsp;&nbsp; share some slides about how that architecture&nbsp; looks, but I thought maybe let's just use AMP to&nbsp;&nbsp; tell you about the AMP architecture. So, here I'm&nbsp; going to open up the AMP CLI. Uh, this has access&nbsp;&nbsp; to all the same tools. It integrates MCP servers&nbsp; in the same way as uh the editor integration that&nbsp;&nbsp; you just saw. And let me just ask it, you know,&nbsp; what are the main arc uh textural components of&nbsp;&nbsp; AMP? How does MCP play into this? So, we're going&nbsp; to let that run. And then while that's running, uh&nbsp;&nbsp; I actually just became aware of this phenomenon.&nbsp; I don't know u if any of you are aware that like&nbsp;&nbsp; there's this thing people do now where they watch&nbsp; like two unrelated uh YouTube videos side by side.&nbsp;&nbsp; It's like a Gen Z thing maybe. Sometimes it's like&nbsp; a Tik Tok, sometimes like free videos. I thought&nbsp;&nbsp; like what is the coding equivalent of that that&nbsp; like agents unlock? And so I was thinking, okay,&nbsp;&nbsp; instead of playing a game or watching a video, why&nbsp; don't we just make a game on the side while, uh,&nbsp;&nbsp; we're figuring out all that other stuff out. So,&nbsp; uh, let's use MCP again and say, you know, find&nbsp;&nbsp; the linear issue about 3D flappy bird. I wrote a&nbsp; very detailed spec ahead of ahead of time. Uh, and&nbsp;&nbsp; we're going to we're going to we're going to try&nbsp; to vibe code 3D Flappy Bird on the side while all&nbsp;&nbsp; this happens. So, over here, uh, we got a pretty&nbsp; detailed, uh, textual explanation of how MCP,&nbsp;&nbsp; uh, is integrated into AMP. Um, but a picture&nbsp; is worth many more words than just text. So why&nbsp;&nbsp; don't we ask it uh can you draw a diagram showing&nbsp; me how these components connect and communicate and we'll have it do that in the meantime. Let's&nbsp; check back over here to see uh how our agent is&nbsp;&nbsp; doing. Um it's making some changes to the code&nbsp; still. One of the things I want to point out&nbsp;&nbsp; here is that another tool that it's integrating.&nbsp; So it's it's using a lot of tool calls along the&nbsp;&nbsp; way. Uh each one of these long text things is&nbsp; a tool call. It's interacting with the browser.&nbsp;&nbsp; It actually uses uh this other MCP server uh the&nbsp; playright MCP server to interact with the browser&nbsp;&nbsp; and take screenshots. And it's going to use that&nbsp; as part of its feedback loop to verify that it&nbsp;&nbsp; actually made uh the change that uh we're telling&nbsp; it to make. So let me just reload the window here&nbsp;&nbsp; to see if it's done yet. Nope. Still working&nbsp; on that. Um, here. Let me actually just restart&nbsp;&nbsp; this. Sometimes it gets confused. Uh, but the&nbsp; beauty of agents is if it messes up, it's not&nbsp;&nbsp; it's most of the times it's not worth it diving&nbsp; in to see where it kind of screwed up. I just ask&nbsp;&nbsp; it to rerun and most of the time it just works.&nbsp; Uh, okay. Okay, over here. Let's take a look at&nbsp;&nbsp; the architectural diagram that it generated&nbsp; and see how MCP relates to these components. If this were in the editor, it would just show&nbsp; this. And uh the text is a little bit hard to&nbsp;&nbsp; read, but as you can see, it's it's got the&nbsp; core pieces of the the AMP architecture here.&nbsp;&nbsp; um everything's routed through this&nbsp; kind of like servers thread component&nbsp;&nbsp; uh which talks to the MCP integration and&nbsp; which in turn talks to all the services that&nbsp;&nbsp; we're integrating through the model context&nbsp; protocol. Uh so that's that's pretty cool,&nbsp;&nbsp; right? Um streamline onboarding makes it&nbsp; really easy to gro what's happening in in&nbsp;&nbsp; a large scale codebase. Um okay, now let's&nbsp; see if 3D Flappy Bird is done here. So,&nbsp;&nbsp; it looks like it got as far as running uh&nbsp; uh Python web server. So, let's go over uh and copy this port. All right. So, we got 3D Flappy Bird. Um that's&nbsp; pretty cool. Successfully, uh wrote an app on&nbsp;&nbsp; the side. And then let's finally check back in&nbsp; on the first thing, which is did it turn the&nbsp;&nbsp; thing red? Uh, and there we go. So, uh, while&nbsp; I was explaining to you how AMP worked, AMP,&nbsp;&nbsp; uh, use tools and MCP servers to basically make&nbsp; a change to itself, explain its architecture, and&nbsp;&nbsp; code up a miniame on on the side. Um, so yeah. Oh,&nbsp; and and also it marked the linear issue as done,&nbsp;&nbsp; which is cool. use that MCP server again. So, uh&nbsp; that's just kind of a brief illustration of the&nbsp;&nbsp; power of MCP and and the sort of results we're&nbsp; getting in building this coding agent. Uh can&nbsp;&nbsp; we go back to the slides? And we've really&nbsp; incorporated MCP in a very uh deep way. So,&nbsp;&nbsp; um one of the things about AMP's architecture&nbsp; is if you look at the different components,&nbsp;&nbsp; you know, we have an AMP client and there's an AMP&nbsp; server and then there's these external services&nbsp;&nbsp; and local tools that we want to talk to. It turns&nbsp; out uh the way to effectively connect to all these&nbsp;&nbsp; different types of tools and services uh it can be&nbsp; done through MCP. So when we're talking to local&nbsp;&nbsp; tools like playright or postgress uh that's going&nbsp; over MCP or standard IO uh when we're talking&nbsp;&nbsp; to external services uh whether it be firstparty&nbsp; services like source graph or code search engine&nbsp;&nbsp; which is really good at searching over you know&nbsp; uh large scale code bases uh or other services&nbsp;&nbsp; such as your issue tracker or your observability&nbsp; tool uh that talks MCP as well and part of the&nbsp;&nbsp; work that we've done well which I'll touch upon&nbsp; in a little bit is also connect uh these MCP&nbsp;&nbsp; connections through a way that securely handles uh&nbsp; secrets and uh forwards the identity of the user&nbsp;&nbsp; uh to the appropriate external services. One of&nbsp; the things that we've realized in in building&nbsp;&nbsp; this is that there is kind of like a new emerging&nbsp; recipe for AI applications or AI agents. Um, so&nbsp;&nbsp; just like you know, rag chat was kind of the model&nbsp; for the previous era, uh, I think this is the the&nbsp;&nbsp; rough formula for the new era. And we actually&nbsp; wrote a blog post about how this is not like,&nbsp;&nbsp; uh, arcane magic. Really, anyone can write an&nbsp; agent, uh, probably in the time it takes to&nbsp;&nbsp; listen to this talk. And so, uh, if any any of&nbsp; you want try your own hand at writing a simple&nbsp;&nbsp; coding agent, just go to that blog post and it&nbsp; shows you how. But the recipe we found is you&nbsp;&nbsp; need maybe like four things. Uh, one is you&nbsp; need a really strong tool use LLM, which the&nbsp;&nbsp; latest cloud model provides. We're really excited&nbsp; about Cloud 4 cap cloud 4's capabilities. We've&nbsp;&nbsp; been playing around with it for the past couple&nbsp; weeks. Uh, all the stuff that I just demoed was&nbsp;&nbsp; running off of Cloud4. Um, in conjunction uh&nbsp; with that tool calling model, you need a way&nbsp;&nbsp; to provide a bunch of tools. Uh, and MCP just so&nbsp; happens to be the perfect solution for that. Uh,&nbsp;&nbsp; so tool use LLM MCP. And then when it comes down&nbsp; to thinking about the the actual user experience,&nbsp;&nbsp; what we found is really important is to really&nbsp; focus on the feedback loops. So as you saw with&nbsp;&nbsp; the uh AMP agent as it was like making a change&nbsp; to itself, what it was doing was it was using&nbsp;&nbsp; the Playright MCP server to take screenshots&nbsp; uh of changes it was making to the app along&nbsp;&nbsp; the way and using those screenshots to validate&nbsp; whether it was doing uh the thing it was doing.&nbsp;&nbsp; And that change was actually pretty non-trivial&nbsp; because the component hierarchy has a lot of like&nbsp;&nbsp; containers and sometimes the change looks right&nbsp; but doesn't actually change the colors uh uh in&nbsp;&nbsp; the actual application. And so that feedback cycle&nbsp; is is really essential to making agents work in&nbsp;&nbsp; practice. Um and part of this too is if you design&nbsp; the feedback loops properly um our thesis is that&nbsp;&nbsp; the UX becomes a lot more imperative. So I think&nbsp; like the previous era of AI applications there&nbsp;&nbsp; were a lot of like there was a lot of UX Chrome&nbsp; and a lot of UI built around figuring out how&nbsp;&nbsp; to invoke uh the the chatbased models sort of in&nbsp; situ in the right situation in the application.&nbsp;&nbsp; and a lot of like manual context selection um&nbsp; tool calling uh LMS plus tool use uh it it's a&nbsp;&nbsp; really powerful paradigm and often times the the&nbsp; best interaction we found is just ask the agent&nbsp;&nbsp; to do it and then refine the feedback loops so&nbsp; that it's able to get those things done reliably in terms of tool usage in AMP some of our top uh&nbsp; tools are provided through MCP servers so our most&nbsp;&nbsp; popular tools are probably the ones I listed&nbsp; above there's some local ones like playright&nbsp;&nbsp; um and Postgress. There's also uh a great tool to&nbsp; uh integrate web search. Uh so you can either use&nbsp;&nbsp; uh Anthropics web search API. There's also Brave&nbsp; web search which is really nice. Uh context 7 is&nbsp;&nbsp; a popular MCP server that pulls in different&nbsp; documentation uh corpuses. Uh and of course&nbsp;&nbsp; linear which I just showed which uh allows you&nbsp; to do this kind of like issue to PR uh workflow.&nbsp;&nbsp; Uh and then finally I want to call out Sentry as&nbsp; being just a really strong MCP server. they really&nbsp;&nbsp; uh focused in on the quality of the description of&nbsp; the tools and that ends up being really essential&nbsp;&nbsp; to making MCP servers work well in practice. Uh&nbsp; and that actually leads me to one of the pitfalls&nbsp;&nbsp; that we found in uh integrating MCP servers into&nbsp; our agent which is um one of the the the the traps&nbsp;&nbsp; that we see some people fall into is what I like&nbsp; to call tool magdon. So it's this practice of like&nbsp;&nbsp; you know MCP MC MCP MCP everyone's excited about&nbsp; MCP right now. So you just want to go plug in like&nbsp;&nbsp; two dozen MCP servers each of which provides like&nbsp; a dozen tools. Um, and that sometimes like when&nbsp;&nbsp; you think about how it's implemented underneath&nbsp; the hood, each one of those tool descriptions gets&nbsp;&nbsp; shoved into the context window and can confuse uh&nbsp; the model. And the model's always getting better,&nbsp;&nbsp; but the more irrelevant stuff that goes into&nbsp; context, the less intelligent it is about making&nbsp;&nbsp; selection among those tools and uh it is about&nbsp; sort of like general reasoning and and getting the&nbsp;&nbsp; job done. And so in terms of how we've baked MCP&nbsp; into our application architecture, uh in certain&nbsp;&nbsp; cases, we've actually limited uh the set of tools&nbsp; that a particular MCP server provides to uh a&nbsp;&nbsp; smaller subset that we think are really essential&nbsp; to the workflows that we want to enable. And and&nbsp;&nbsp; roughly speaking, there's kind of three buckets of&nbsp; tools we find really useful. There's the ones that&nbsp;&nbsp; are devoted to finding relevant context. There's&nbsp; the ones that uh can provide high quality feedback&nbsp;&nbsp; such as like invoking unit tests or invoking the&nbsp; compiler. Uh and then finally the ones that are&nbsp;&nbsp; involved in like submitting uh done or declaring&nbsp; success like marking the issue as done uh or or&nbsp;&nbsp; pinging a user to say uh hey I'm ready to uh uh&nbsp; for for your feedback on this. I also want to talk&nbsp;&nbsp; a little bit about securing MCP. So that's a high&nbsp; priority for us given how many of our customers&nbsp;&nbsp; are in kind of like large scale production code&nbsp; bases. So the original MCP spec didn't have&nbsp;&nbsp; anything around O. They've since integrated OOTH 2&nbsp; as the kind of designated authentication protocol,&nbsp;&nbsp; which I think was a really smart decision. It's&nbsp; what we've used in the past to integrate a lot of&nbsp;&nbsp; external services, but that's just the protocol,&nbsp; right? There's still the implementation and you&nbsp;&nbsp; still have to worry about like where you store the&nbsp; secrets. And I think a lot of what you see in the&nbsp;&nbsp; wild in terms of how people are integrating MCP&nbsp; servers. I think the vast majority of tools out&nbsp;&nbsp; there are still using largely like MCP or standard&nbsp; IO even uh with the existence of like remote MCP&nbsp;&nbsp; servers. Uh there someone made like this npm uh&nbsp; plugin that just converts a remote MCP uh to a&nbsp;&nbsp; local MCP. So it feels like uh the application&nbsp; feels like it's still talking over standard IO&nbsp;&nbsp; uh and it handles the off handshake, but as&nbsp; a consequence it just like shoves the secrets&nbsp;&nbsp; uh like your secret tokens to your other services&nbsp; in some like random plain text directory. Uh and&nbsp;&nbsp; that's that's kind of like a no-go for a lot of&nbsp; our customers. And so as part of this handshake,&nbsp;&nbsp; we actually implemented a secure secret store&nbsp; where the AMP server takes care of the OOTH&nbsp;&nbsp; handshake and it proxies the MCP connection&nbsp; from the client to these external services&nbsp;&nbsp; and ensures that no secret ever gets stored&nbsp; unencrypted uh on your local machine. Okay,&nbsp;&nbsp; in the last couple minutes I just want to you&nbsp; know take take some time and speculate about&nbsp;&nbsp; where the future is headed. So there's been&nbsp; a lot of like great talk at this conference&nbsp;&nbsp; about you know what's next after tool calling.&nbsp; Tool calling is such a powerful paradigm. Uh,&nbsp;&nbsp; one of the things that we think a lot about is,&nbsp; you know, the extent to which sub agents can be&nbsp;&nbsp; the way that a lot of tools are implemented.&nbsp; So, um, I didn't point this out earlier,&nbsp;&nbsp; but the way that AMP was actually gathering&nbsp; context about the the the codebase was actually&nbsp;&nbsp; a sub agent. It wasn't just a deterministic uh,&nbsp; search tool. It was actually like its own mini&nbsp;&nbsp; agent that's going in and invoking different uh&nbsp; low-level search tools and iterating on itself,&nbsp;&nbsp; reasoning about the context of gatherers, refining&nbsp; queries uh to to gather the appropriate context.&nbsp;&nbsp; And we found that approach works just super super&nbsp; well. And uh I think the the uh the potential for&nbsp;&nbsp; sub agents to become really good tools were&nbsp; we're only scratching the surface right now.&nbsp;&nbsp; Um there's also this notion of like you know what&nbsp; does it mean to dynamically synthesize a tool and&nbsp;&nbsp; I think uh we touched upon this in some of the&nbsp; earlier talks when we were talking about code&nbsp;&nbsp; execution. So uh right now the the tool calling&nbsp; paradigm is largely you have a static list of&nbsp;&nbsp; tools and the model will go and invoke each one&nbsp; one by one look at the output and then decide&nbsp;&nbsp; what to do next. Um, a lot of people, not just&nbsp; us, have pointed out, hey, it might be useful&nbsp;&nbsp; if we incorporated a notion of output schema into&nbsp; MCP, which, you know, that just got merged in. Uh,&nbsp;&nbsp; and then the model can sort of like plan out&nbsp; how to invoke these tools and compose them and&nbsp;&nbsp; chain them in different ways. And if you squint&nbsp; at some point, you're basically programming,&nbsp;&nbsp; right? Like you have these functions, you have&nbsp; these tools, you're composing them, uh, you're&nbsp;&nbsp; combining them in interesting ways. And so, uh,&nbsp; I think it's it's a really good time to revisit,&nbsp;&nbsp; uh, code interp code interpreters. um which was&nbsp; a thing that you know at first was a thing in in&nbsp;&nbsp; 2023 but uh sort of went away for a little bit.&nbsp; I think with the advent of tool calling agents&nbsp;&nbsp; there's a lot more potential to explore there&nbsp; and something that we're actively considering as&nbsp;&nbsp; well. I was talking with uh David from Enthropic&nbsp; earlier this week. He stopped by our office and&nbsp;&nbsp; we're talking about a lot of the parallels&nbsp; between uh tool calling LLMs and agents and&nbsp;&nbsp; uh the kind of discourse that uh was active when&nbsp; highle programming languages first became a thing.&nbsp;&nbsp; So before you know programming languages settled&nbsp; on uh you know the the the abstractions that kind&nbsp;&nbsp; of dominate today there's a lot of discussion&nbsp; around like what's the proper abstraction for a&nbsp;&nbsp; subruine is it a function is it message passing&nbsp; uh do uh is there a way to manage concurrent&nbsp;&nbsp; communication uh effectively there's different&nbsp; models for that and I think we're just going to&nbsp;&nbsp; revisit all of that now because the analogies to&nbsp; uh you know programming languages now with you&nbsp;&nbsp; know agents and sub aents and how they interact&nbsp; with each other and also with deterministic&nbsp;&nbsp; systems uh it's just very strong and then the last&nbsp; point here is um I think the way that most people&nbsp;&nbsp; integrate MCP right now if you look at the part&nbsp; of the spec that the vast majority of MCP clients&nbsp;&nbsp; implement versus the protocol itself the entire&nbsp; protocol I think the protocol designers here were&nbsp;&nbsp; very very forward thinking there is a lot baked&nbsp; into the protocol like stateful session management&nbsp;&nbsp; two-way communication um and uh sampling which&nbsp; is when the MCP server calls the client to do&nbsp;&nbsp; LM inference that's frankly just not being used.&nbsp; Like the vast majority of MCP connections right&nbsp;&nbsp; now are stateless uh tool calls that don't&nbsp; even take advantage of streaming. Uh and so&nbsp;&nbsp; this is such early days like we're we're literally&nbsp; scratching the tip of the iceberg for what we can&nbsp;&nbsp; do with tool calling uh LLM and and tools provided&nbsp; through the model context protocol. And uh I don't&nbsp;&nbsp; know the next year I think it's it's going to get&nbsp; really weird. Um so I I guess you know we're super&nbsp;&nbsp; excited to be here today. Uh it's been a great&nbsp; partnership uh with Enthropic uh for the past&nbsp;&nbsp; I'd say uh almost like three years now. I think&nbsp; we're one of the earliest adopters of Claude for&nbsp;&nbsp; coding purposes. I think we we started using it in&nbsp; January 2023. It's been an amazing journey. Um and&nbsp;&nbsp; we're we're trying to build AMP from the ground&nbsp; up into this kind of like tool calling native&nbsp;&nbsp; uh coding agent. And so if that's of interest&nbsp; of you, check us out. and uh we look forward&nbsp;&nbsp; to all the things that people kind of build on&nbsp; top of it, build with it. Uh and also a lot of&nbsp;&nbsp; the MCP servers that people will build that we&nbsp; can then integrate. So if you're doing that,&nbsp;&nbsp; please get in touch with us as well. We'd love to&nbsp; hear from you and figure out how this can fit into&nbsp;&nbsp; the kind of new way of doing software development&nbsp; that we're all discovering together. That do I&nbsp;&nbsp; have time for questions or one question? Any&nbsp; questions? I'll kick us off. Um you have source&nbsp;&nbsp; Kodi as well. So how are you thinking about Kodi&nbsp; and AMP uh playing together? Yeah, that's a great&nbsp;&nbsp; question. So um our we we also have this AI coding&nbsp; assistant called Kodi that was one of the first&nbsp;&nbsp; uh rag chat uh contextaware uh coding assistants.&nbsp; Recall the earlier picture where it was kind of&nbsp;&nbsp; like the three eras of AI. I think Kodi was close&nbsp; to uh Cody was an awesome AI application built for&nbsp;&nbsp; the rag chat era of models and I think there's&nbsp; still a lot of organizations that will find a&nbsp;&nbsp; lot of value from that paradigm and still a lot&nbsp; of workflows that can benefit from that. Um,&nbsp;&nbsp; but because the underlying assumptions&nbsp; of what LLMs can do have changed so much,&nbsp;&nbsp; we think that the best uh sort of like user&nbsp; experience for the agentic world is going to come&nbsp;&nbsp; from an application that was designed from the&nbsp; ground up to take advantage of uh tool calling in&nbsp;&nbsp; MCP. And that's why we built AMP as a separate uh&nbsp; sort of like application and thing uh from Kodi.&nbsp;&nbsp; And if I'm being like a little bit snarky, I think&nbsp; like if you're not rethinking that architecture,&nbsp;&nbsp; I think as an application developer, you're at&nbsp; risk of falling behind and missing the kind of&nbsp;&nbsp; like next wave of AI development. All right,&nbsp; I think that's my time. Thank you so much.

---

## 7. Prompting 101

**Preview:**
> Kind: captions Language: en Hi everyone. Thank you for joining us today for&nbsp; prompting 101. Uh my name is Hannah. I'm part&nbsp;&nbsp; of the applied AI team here at Anthropic. And&nbsp; with me is Christian, also part of the applied&nbsp;&nbsp; AI team. And what we're going to do today is&nbsp; take you through a little bit of prompting best&nbsp;&nbsp; practices. And we're going to use a real world&nbsp; scenario and build up a prompt together. Uh so&nbsp;&nbsp; a little bit about what pr...

**Full Transcript:**

Kind: captions Language: en Hi everyone. Thank you for joining us today for&nbsp; prompting 101. Uh my name is Hannah. I'm part&nbsp;&nbsp; of the applied AI team here at Anthropic. And&nbsp; with me is Christian, also part of the applied&nbsp;&nbsp; AI team. And what we're going to do today is&nbsp; take you through a little bit of prompting best&nbsp;&nbsp; practices. And we're going to use a real world&nbsp; scenario and build up a prompt together. Uh so&nbsp;&nbsp; a little bit about what prompt engineering is. uh&nbsp; prompt engineering. You're all probably a little&nbsp;&nbsp; bit familiar with this. This is the way that we&nbsp; communicate with a language model and try to get&nbsp;&nbsp; it to do what we want. So, this is the practice of&nbsp; writing clear instructions for the model, giving&nbsp;&nbsp; the model the context that it needs to complete&nbsp; the task, and thinking through how we want to&nbsp;&nbsp; arrange that information in order to get the&nbsp; best result. Um, so there's a lot of detail here,&nbsp;&nbsp; a lot of different ways you might want to think&nbsp; about building out a prompt. Um, and as always,&nbsp;&nbsp; the best way to learn this is just to practice&nbsp; doing it. Um, so today we're going to go through&nbsp;&nbsp; a hands-on scenario. Uh, we're going to use an&nbsp; example inspired by a real customer that we worked&nbsp;&nbsp; with. So, we've modified what the actual customer&nbsp; asked us to do, but this is a really interesting&nbsp;&nbsp; case of trying to analyze some images and get uh&nbsp; factual information out of the images and have&nbsp;&nbsp; Claude make a judgment about what content it finds&nbsp; there. And I actually do not speak the language&nbsp;&nbsp; that this content is in, but luckily Christian and&nbsp; Claude both do. Uh so I'm going to pass it over&nbsp;&nbsp; to Christian to talk about the scenario and the&nbsp; content. So for this example that we have here,&nbsp;&nbsp; it's uh intended so so to set the stage, imagine&nbsp; you're working for a Swedish insurance company&nbsp;&nbsp; and you deal with uh car insurance claims on a&nbsp; daily manner. Um and the purpose of this is that&nbsp;&nbsp; you have two pieces of information. Um we're going&nbsp; to these in detail as well, but visually you can&nbsp;&nbsp; see on the left hand side we have a car accident&nbsp; report form. um just detailing out what transpired&nbsp;&nbsp; before the action accident actually took place.&nbsp; And then finally, we have a sort of human drawn&nbsp;&nbsp; um sketch of how the accident took place as well.&nbsp; So these two pieces of information is what we're&nbsp;&nbsp; going to try to pass on to cloud. And to begin&nbsp; with, we could just take these two and throw them&nbsp;&nbsp; into a console and just see what what happens.&nbsp; So if we transition over to console as well,&nbsp;&nbsp; we can actually do this in a real manner. And&nbsp; in this case here, you can see we have our&nbsp;&nbsp; shiny beautiful entropic console. We're using the&nbsp; new claw for solid model as well. In this case,&nbsp;&nbsp; setting temperature zero and having a a huge max&nbsp; token budget as well. Just helping us make sure&nbsp;&nbsp; that there's no limitations to what CL can do. In&nbsp; this case, you can see I have a very simple prompt&nbsp;&nbsp; just setting the stage of what Cloud's supposed&nbsp; to do. in this case mentioning that this is&nbsp;&nbsp; um intend to review a an accident report form uh&nbsp; and eventually also determine um what happened&nbsp;&nbsp; in an accident and who's at fault. So you can&nbsp; see here with this very simple prompt if I just&nbsp;&nbsp; run this let me go to preview. Uh we can see here&nbsp; that Claude thinks that this is in relation to a&nbsp;&nbsp; skiing accident that happened on a street called&nbsp; Chappangan. It's a very common street in Sweden.&nbsp;&nbsp; Um and in many ways you can sort of understand&nbsp; this innocent mistake in the sense that in our&nbsp;&nbsp; prompt we actually haven't done anything to set&nbsp; the stage on what is actually taking place here.&nbsp;&nbsp; So this sort of first guess is not too bad but&nbsp; we still notice a lot of intuition that we can&nbsp;&nbsp; bake into cloud. So if we switch back to the&nbsp; slides you can see here that um in many ways&nbsp;&nbsp; prompt engineering is a very iterative empirical&nbsp; science. Uh in this case here, we could almost&nbsp;&nbsp; have a test case where Claude is supposed to make&nbsp; sure it understands it's in a car or vehicular&nbsp;&nbsp; environment, nothing to do with skiing. Uh and in&nbsp; that way, you iteratively build upon your prompt&nbsp;&nbsp; to make sure it's actually tackling the problem&nbsp; you're intending to solve. Um and to do so,&nbsp;&nbsp; we'll go through some best practices of how we we&nbsp; at Anthropic break this down internally and how we&nbsp;&nbsp; recommend others to do so as well. So, we're going&nbsp; to talk about some best practices for developing&nbsp;&nbsp; a great prompt. Uh, first we want to talk a&nbsp; little bit about what a great prompt structure&nbsp;&nbsp; looks like. So you might be familiar with kind of&nbsp; interacting with a chatbot with Claude going back&nbsp;&nbsp; and forth having a more kind of conversational&nbsp; style interaction. When we're working with a task&nbsp;&nbsp; like this, we're probably using the API and we&nbsp; kind of want to send one single message to Claude&nbsp;&nbsp; and have it nail the task the first time around&nbsp; without needing to uh kind of move back and forth.&nbsp;&nbsp; Uh, so the kind of structure that we recommend is&nbsp; setting the task description up front. So telling&nbsp;&nbsp; Claude, "What are you here to do? What's your&nbsp; role? What task are you trying to accomplish&nbsp;&nbsp; today?" Then we provide content. So in this&nbsp; case, it's the images that Christian was showing,&nbsp;&nbsp; the form and the drawing of the accident and how&nbsp; they occurred. That's our dynamic content. This&nbsp;&nbsp; might also be something you're retrieving from&nbsp; another system, depending on what your use case&nbsp;&nbsp; is. We're going to give some detailed instructions&nbsp; to Claude, so almost like a step-by-step list of&nbsp;&nbsp; how we want Claude to go through the task and how&nbsp; we want it to um tackle the reasoning. We may give&nbsp;&nbsp; some examples to Claude. Here's an example of some&nbsp; piece of content you might receive. Here's how you&nbsp;&nbsp; should respond when given that content. And at&nbsp; the end, we usually recommend repeating anything&nbsp;&nbsp; that's really important for Claude to understand&nbsp; about this task. Kind of uh reviewing the&nbsp;&nbsp; information with Claude, emphasizing things that&nbsp; are extra critical and then telling Claude, "Okay,&nbsp;&nbsp; go ahead and do your work." So, here's another&nbsp; view. This has a little bit more detail, a little&nbsp;&nbsp; bit more of a breakdown, and we're going to walk&nbsp; through each of these 10 points individually and&nbsp;&nbsp; show you how we build this up, um, in the console.&nbsp; So, the first couple things, um, Christian's&nbsp;&nbsp; going to talk about the task context and the tone&nbsp; context. Perfect. So, yeah, if we begin with the&nbsp;&nbsp; task context, as you realized when I went through&nbsp; a little demo there, um, we didn't have much&nbsp;&nbsp; elaborating what what scenario Chlo was actually&nbsp; working within. And because of that, you can also&nbsp;&nbsp; tell that claw doesn't necessarily need to guess a&nbsp; lot more on what you actually want from it. So in&nbsp;&nbsp; our case, we really want to break that down, make&nbsp; sure we can give more clear-cut instructions. Um,&nbsp;&nbsp; and also make sure we understand what's the&nbsp; task that we're asking Claw to do. Um, secondly,&nbsp;&nbsp; as well, we also make sure we add a little bit of&nbsp; tone into it all. Um, key thing here is we want&nbsp;&nbsp; Claw to stay factual and to stay confident. So if&nbsp; uh, Claw can't understand what it's looking at,&nbsp;&nbsp; we don't want to guess and just sort of mislead&nbsp; us. We want to make sure that any assessment&nbsp;&nbsp; and in our case we want to make sure that we can&nbsp; understand who's at fault here. We want to make&nbsp;&nbsp; sure that assessment is as clear and as confident&nbsp; as possible. If not, we're sort of losing track of&nbsp;&nbsp; what we're doing. So if we transition back to the&nbsp; the console, um we can jump to a V2 that we have&nbsp;&nbsp; here. So I'll just navigate to V2. And you can see&nbsp; here um I'll also just illustrate the data because&nbsp;&nbsp; we didn't really do that last time around just&nbsp; to really highlight what we're looking at. So,&nbsp;&nbsp; what we're seeing here, this is the car accident&nbsp; report form, and it's just 17 different checkboxes&nbsp;&nbsp; going through what actually happened. You&nbsp; can see there's a vehicle A and vehicle B,&nbsp;&nbsp; both on the left and right hand side. And the main&nbsp; purpose of this is that we want to make sure that&nbsp;&nbsp; Claude can understand this manually generated data&nbsp; to assess what's actually going on. And that is&nbsp;&nbsp; uh corroborated by if I navigate back here to this&nbsp; sketch that we can highlight here as well. In this&nbsp;&nbsp; case, the form is just a different um data point&nbsp; for the same scenario. Um and in this case here, I&nbsp;&nbsp; want to bake in more information into our version&nbsp; two. Uh and by doing so, I'm actually elaborating&nbsp;&nbsp; a lot more on what's going on. So, you can see&nbsp; here I'm specifying that uh this AI assistant is&nbsp;&nbsp; supposed to help a human's claim claims adjuster&nbsp; that's reviewing car accident report forms in&nbsp;&nbsp; Swedish as well. Um, you can see here we're also&nbsp; elaborating that it's a human-driven sketch of&nbsp;&nbsp; the incident and that you should not um make an&nbsp; assessment if it's not actually fully confident.&nbsp;&nbsp; And that's really key because if we run this,&nbsp; you'll see that and you can see it's the same&nbsp;&nbsp; settings as well. Clo my new shiny model zero&nbsp; temperature as well. If we run this, we can see&nbsp;&nbsp; here what actually happens in this case. Um, CL is&nbsp; able to pick up that uh now it's relating to car&nbsp;&nbsp; accidents, not skiing accidents, which is great.&nbsp; We can see it's able to pick up that vehicle A was&nbsp;&nbsp; marked on on checkbox one and then vehicle B was&nbsp; on 12. Um, and if we scroll down though, we can&nbsp;&nbsp; still tell that there's some information missing&nbsp; for claw to make a fully confident determination&nbsp;&nbsp; of who's at fault here. And this is great. This&nbsp; is pertaining to a task set. Make sure you don't&nbsp;&nbsp; make anything any claims that aren't um uh factual&nbsp; and make sure you you only sort of assess things&nbsp;&nbsp; when you're when you're confident. But there's&nbsp; a lot of information we're still missing here.&nbsp;&nbsp; um regarding the form uh what the form actually&nbsp; entails and a lot of that information is what&nbsp;&nbsp; we want to want to bake into this LM application&nbsp; as well and the best way of doing so is actually&nbsp;&nbsp; adding it to the system prompt which Hannah will&nbsp; elaborate on. Um so back in the slides uh we have&nbsp;&nbsp; the next item we're going to add to the prompt&nbsp; and this is um background detail data documents&nbsp;&nbsp; and images and here as Christian was saying we&nbsp; actually know a lot about this form. the form is&nbsp;&nbsp; going to be the same every single time. The form&nbsp; will never change. And so this is a really great&nbsp;&nbsp; type of information to provide to Claude to tell&nbsp; Claude, here's the structure of the form you'll&nbsp;&nbsp; be looking at. We know that will not ever alter&nbsp; between different queries. The way the form is&nbsp;&nbsp; filled out will change, but the form itself is not&nbsp; going to change. And so this is a great type of&nbsp;&nbsp; um information to put into the system prompt. Also&nbsp; a great thing to use prompt caching for if you're&nbsp;&nbsp; considering using prompt caching. This will always&nbsp; be the same. And what this will help Claude do is&nbsp;&nbsp; spend less time trying to figure out what the form&nbsp; is the first time it sees the form each time. And&nbsp;&nbsp; it's going to do a better job of reading the form&nbsp; because it already knows um what to expect there.&nbsp;&nbsp; So another thing I want to touch on here is how we&nbsp; like to organize information in prompts. So Claude&nbsp;&nbsp; really loves structure, loves organization.&nbsp; That's why we recommend following kind of a&nbsp;&nbsp; standard structure in your prompts. And there's&nbsp; a couple other tools you can use to help Claude&nbsp;&nbsp; understand the information better. I also just&nbsp; want to mention all of this is in our docs with a&nbsp;&nbsp; lot of really great examples. So definitely take&nbsp; pictures, but if you forget to take a picture,&nbsp;&nbsp; don't worry. All of this content is online with&nbsp; lots of examples and definitely encourage you&nbsp;&nbsp; guys to check it out there too. Um anyway the uh&nbsp; so some things you can use delimiters like XML&nbsp;&nbsp; tags also markdown is pretty useful to Claude&nbsp; but XML tags are nice because you can actually&nbsp;&nbsp; specify what's inside those tags. So we can tell&nbsp; Claude here's here's user preferences. Now you're&nbsp;&nbsp; going to read some content and these XML tags are&nbsp; letting you know that everything wrapped in those&nbsp;&nbsp; tags is related to the user's preferences and&nbsp; it helps Claude refer back to that information&nbsp;&nbsp; maybe at later points in the prompt. Um, so I&nbsp; want to show in the back in the console how we&nbsp;&nbsp; actually do this in this case. And Christian's&nbsp; going to pull up our version three. So we're&nbsp;&nbsp; keeping everything about the other part of the&nbsp; user prompt the same. And we've decided in this&nbsp;&nbsp; case to put this information in the system prompt.&nbsp; You could try this different ways. Uh, we're doing&nbsp;&nbsp; it in the system prompt here. And we're going&nbsp; to tell Claude everything it needs to know about&nbsp;&nbsp; this form. So this is a Swedish car accident&nbsp; form. The form will be in Swedish. It'll have&nbsp;&nbsp; this title. It'll have two columns. The columns&nbsp; represent different vehicles. We'll tell Claude&nbsp;&nbsp; about each of the 17 rows and what they mean.&nbsp; You might have noticed when we ran it before,&nbsp;&nbsp; Claude was reading individually each of the lines&nbsp; to understand what they are. We can provide all of&nbsp;&nbsp; that information up front. And we're also going&nbsp; to give Claude a little bit of information about&nbsp;&nbsp; how this form should be filled out. This is also&nbsp; really useful for Claude. We can tell it things&nbsp;&nbsp; like, you know, humans are filling this form&nbsp; out basically. So, it's not going to be perfect.&nbsp;&nbsp; People might put a circle. They might scribble.&nbsp; They might not put an X in the box. There could&nbsp;&nbsp; be many types of markings that you need to look&nbsp; for when you're reading this form. Uh we can also&nbsp;&nbsp; give Claude a little bit of information about how&nbsp; to interpret this or what the purpose or meaning&nbsp;&nbsp; of this form is. And all of this is context&nbsp; that is hopefully really going to help Claude&nbsp;&nbsp; um do a better job analyzing the form. So if&nbsp; we run it, everything else is still the same.&nbsp;&nbsp; So we've kept the same user prompt down here.&nbsp; Oh, your scroll is backwards from mine. Uh,&nbsp;&nbsp; the we have the same user prompt here. Still&nbsp; asking Claude to do the same task, same context.&nbsp;&nbsp; And we'll see here that it's spending less time.&nbsp; It's kind of narrating to us a little bit less&nbsp;&nbsp; about what the form is because it already knows&nbsp; what that is. And it's not concerned with kind&nbsp;&nbsp; of bringing us that information back. It's going&nbsp; to give us a whole list of what it found to be&nbsp;&nbsp; checked, what the sketch shows. And here Claude&nbsp; is now becoming much more confident with this&nbsp;&nbsp; additional context that we gave to Claude. Claude&nbsp; now feels it's appropriate to say vehicle B was&nbsp;&nbsp; at fault in this case based on this drawing and&nbsp; based on this sketch. So already we're seeing some&nbsp;&nbsp; improvement in the way Claude is analyzing these.&nbsp; I think we could probably all agree if we looked&nbsp;&nbsp; at the drawing and at the list that vehicle&nbsp; B is at fault. Um so we'd like to see that.&nbsp;&nbsp; Uh so we're going to go back to the slides and&nbsp; talk about a couple of other items that we're not&nbsp;&nbsp; really using in this prompt um but can be really&nbsp; helpful to building up uh building up your prompt&nbsp;&nbsp; and making it work better. Exactly. I think um&nbsp; one thing that we really highlight is examples.&nbsp;&nbsp; I think examples or few shot is a mechanism that&nbsp; really is powerful in steering cloud. So you can&nbsp;&nbsp; imagine this um in in quite a non-trivial way as&nbsp; well. So imagine you have scenarios, situations,&nbsp;&nbsp; even in this case concrete accidents that have&nbsp; happened that are um tricky for claw to get right.&nbsp;&nbsp; But you with your human intuition and your human&nbsp; label data um is able to actually get to the right&nbsp;&nbsp; conclusion. Then you can bake that information&nbsp; into the system problem itself by having clear-cut&nbsp;&nbsp; examples of a the data that that it's supposed&nbsp; to look at. So you can have visual examples.&nbsp;&nbsp; you can just base 64 encode a a an image and have&nbsp; that as part of the data that you're passing along&nbsp;&nbsp; into the examples and then on top of that you can&nbsp; have the sort of depiction or description rather&nbsp;&nbsp; of how to break that down and understand it. This&nbsp; is something we really highlight and and emphasize&nbsp;&nbsp; in how you can sort of push the limits of your&nbsp; LLM application is by baking in these examples&nbsp;&nbsp; into system prompt. And this again is sort of the&nbsp; empirical science of prompt engineering that you&nbsp;&nbsp; sort of always want to push the limits of your&nbsp; application and get that feedback loop in where&nbsp;&nbsp; it's going wrong and try to add that into system&nbsp; prompt so that next time when example that sort&nbsp;&nbsp; of mimics that u takes place it's able to actually&nbsp; reference it in its example set. You can see here&nbsp;&nbsp; as well, this is just a little example of how we&nbsp; do this. Again, really emphasizing the sort of XML&nbsp;&nbsp; structure that we we um we enjoy. It's it gives a&nbsp; lot of structure to the clone. It's what it's been&nbsp;&nbsp; fine-tuned on as well. Um and it works perfectly&nbsp; well for this example. And in our case, we're not&nbsp;&nbsp; doing this just because it's a simple demo,&nbsp; but you can realistically imagine if you were&nbsp;&nbsp; building this for an insurance company, you'd have&nbsp; tens, maybe even hundreds of examples are quite&nbsp;&nbsp; difficult, maybe in the gray, that you'd like to&nbsp; make sure that Claude actually has some basis in&nbsp;&nbsp; to make the verdict next time. Um, another topic&nbsp; we really want to highlight, which we're not doing&nbsp;&nbsp; in this demo, is conversation history. It's in the&nbsp; same vein as examples. uh we use this to make sure&nbsp;&nbsp; that the enough context rich information is at&nbsp; close disposal when it when when closing on on on&nbsp;&nbsp; your behalf. Um in our case now this isn't really&nbsp; a userfacing LLM application. It's more something&nbsp;&nbsp; happening in the background. You can imagine for&nbsp; this insurance company they have this automated&nbsp;&nbsp; system some data is generated out of this and then&nbsp; you might have a human in the loop at towards the&nbsp;&nbsp; end. If you were have to build something much more&nbsp; userf facing where you'd have a long conversation&nbsp;&nbsp; history that would be um relevant to bring in this&nbsp; is a perfect place in the system prompt to include&nbsp;&nbsp; because it enriches the context that Claude works&nbsp; within. Um in our case we haven't done so but what&nbsp;&nbsp; we do is and the next step is try to make sure&nbsp; we give a concrete reminder of the task at hand.&nbsp;&nbsp; So, now we're going to build out the&nbsp; final part of this prompt for Claude,&nbsp;&nbsp; and that's coming back to the reminder of what&nbsp; the immediate task is and giving Claude a reminder&nbsp;&nbsp; about any important guidelines that we want it&nbsp; to follow. Some reasons that we may do this are&nbsp;&nbsp; a preventing hallucinations. Um, so we want Claude&nbsp; to uh not invent details that it's not finding in&nbsp;&nbsp; this prompt, right? Or not finding in the data.&nbsp; If Claude can't tell which form is checked,&nbsp;&nbsp; we don't want Claude to take its best guess or&nbsp; invent the idea that a box might be checked when&nbsp;&nbsp; it's not. If the sketch is unintelligible, the&nbsp; person did a really bad job drawing this drawing&nbsp;&nbsp; and even a human would not be able to figure it&nbsp; out. We want Claude to be able to say that. And&nbsp;&nbsp; so these are some of the things we'll include in&nbsp; this final reminder and kind of wrap up step for&nbsp;&nbsp; Claude. Uh remind it to do things like answer only&nbsp; if it's very confident. We could even ask it to&nbsp;&nbsp; refer back to what it has seen in the form anytime&nbsp; it's making a factual claim. So if it wants to say&nbsp;&nbsp; vehicle B turned right, it should say I know this&nbsp; based on the fact that box two is clearly checked&nbsp;&nbsp; or whatever it might be. We can kind of give&nbsp; Claude some guidelines about that. So if we go&nbsp;&nbsp; back to the console, we can see the next version&nbsp; of the prompt and we're going to keep uh we're&nbsp;&nbsp; going to keep everything the same here in the&nbsp; system prompt. So, we're not changing any of that&nbsp;&nbsp; background context that we gave to Claude about&nbsp; the form, about how it's going to fill everything&nbsp;&nbsp; out. We're not changing anything else about the&nbsp; context and the role. We're just adding this&nbsp;&nbsp; detailed list of tasks. And this is how we want&nbsp; Claude to go about analyzing this. And a really&nbsp;&nbsp; key thing that we found here as we were building&nbsp; this demo and when we were working on the customer&nbsp;&nbsp; example is that the order in which Claude analyzes&nbsp; this information is very important. And this is&nbsp;&nbsp; analogous to way you might think about doing this.&nbsp; If you were a human, you would probably not look&nbsp;&nbsp; at the drawing first and try to understand what&nbsp; was going on, right? It's pretty unclear. It's&nbsp;&nbsp; a bunch of boxes and lines. We don't really know&nbsp; what that drawing is supposed to mean without any&nbsp;&nbsp; additional context. But if we have the form and we&nbsp; can read the form first and understand that we're&nbsp;&nbsp; talking about a car accident and that we're seeing&nbsp; some checkboxes that indicate what vehicles we're&nbsp;&nbsp; doing at certain times, then we know a little&nbsp; bit more about how to understand what might be&nbsp;&nbsp; in the drawing. And so that's the kind of detail&nbsp; that we're going to give Claude here is to say,&nbsp;&nbsp; "Hey, first go look at the form. Look at it very&nbsp; carefully. Make sure you can tell what boxes are&nbsp;&nbsp; checked. Make sure you're not missing anything&nbsp; here. Um, make a list for yourself of what you see&nbsp;&nbsp; in that. And then move on to the sketch. So after&nbsp; you've kind of confidently gotten information&nbsp;&nbsp; out of the form and you can say what's factually&nbsp; true, then you can go on and think about what you&nbsp;&nbsp; can gain from that sketch. keeping in mind your&nbsp; understanding of the accident so far. So, whatever&nbsp;&nbsp; you've learned from the form, you're trying to&nbsp; match that up with the sketch. And that's how&nbsp;&nbsp; you're going to arrive um at your final uh at your&nbsp; final assessment of the form. And we'll run it. And here you can see one behavior that&nbsp; this produced for Claude because I told&nbsp;&nbsp; it to very carefully examine the form. It's&nbsp; showing me its work as it does that. So,&nbsp;&nbsp; it's telling me each individual box. Is the box&nbsp; checked? Is it not checked? And so, this is one&nbsp;&nbsp; thing you'll notice as you do prompt engineering.&nbsp; In our previous prompts, we were kind of letting&nbsp;&nbsp; claw decide how much it wanted to tell us about&nbsp; what it saw on the form here. Because I've told&nbsp;&nbsp; it carefully examine each and every box, it's very&nbsp; carefully examining each and every box. And that&nbsp;&nbsp; might not be what we want in the end. So, that's&nbsp; something we might change. Um, but it's also going&nbsp;&nbsp; to give me these other things that I asked for&nbsp; in XML tags. So, a nice analysis of the form, the&nbsp;&nbsp; accident summary so far. It's going to give me a&nbsp; sketch analysis, and it's going to continue to say&nbsp;&nbsp; that vehicle B appears to be clearly at fault. In&nbsp; this in this example, it's pretty simple example&nbsp;&nbsp; with more complicated drawings, more uh less&nbsp; clarity in the forms. This kind of step-by-step&nbsp;&nbsp; thinking for Claude is really impactful in&nbsp; its ability to make a correct assessment here.&nbsp;&nbsp; Uh, so I think we'll go back to the slides and&nbsp; Christian's going to talk about a last kind of&nbsp;&nbsp; piece that we might add to this um to really make&nbsp; it useful for a real world task. Indeed. Thank&nbsp;&nbsp; you so much. So, as Hannah mentioned, uh, we sort&nbsp; of set the stage in this prompt to make sure that&nbsp;&nbsp; really acting on our behalf in a right manner.&nbsp; Um, and a key step that we also add towards the&nbsp;&nbsp; end of this prompt that I'm going to show you in a&nbsp; second is a simple sort of guidelines or reminder&nbsp;&nbsp; part as well. just strengthening and reinforcing&nbsp; exactly what we want to get out of it. And one&nbsp;&nbsp; important piece is actually output formatting.&nbsp; You can imagine if you're a data engineer working&nbsp;&nbsp; on this LM application, all the sort of fancy&nbsp; preamble is great, but at the end of the day,&nbsp;&nbsp; you want your piece of information to to&nbsp; be stored in, let's say, your SQL database,&nbsp;&nbsp; wherever you want to store that data. And the rest&nbsp; of it that is necessary for cloud to sort of give&nbsp;&nbsp; its verdict isn't really that necessary for your&nbsp; application. You want the nitty-gritty information&nbsp;&nbsp; for your application. So if we transition back to&nbsp; the console, you'll see here that we just added&nbsp;&nbsp; a simple importance guidelines part. And again,&nbsp; this is just reinforcing the sort of mechanical&nbsp;&nbsp; behavior that we want out of cloud here. Want&nbsp; to make sure that the summary is clear, concise,&nbsp;&nbsp; and accurate. Want to make sure that nothing&nbsp; is sort of impeding in in in Claw's assessment&nbsp;&nbsp; apart from the data it's analyzing. And then&nbsp; finally, when it comes to output formatting,&nbsp;&nbsp; in my case here, I'm just going to ask Claude&nbsp; to wrap its final verdict. All other stuff I'm&nbsp;&nbsp; actually going to ignore for my application and&nbsp; just look at what it's actually assessing. And&nbsp;&nbsp; that is I can I can use this if I want to build&nbsp; some sort of analytics tool afterwards as well.&nbsp;&nbsp; Or if I just want a clearcut um uh determination,&nbsp; this is a way I can do so. So if I just run this&nbsp;&nbsp; here, you'll see it's going through the same sort&nbsp; of process that we've seen before. In this case,&nbsp;&nbsp; it's much more succinct because we've asked&nbsp; to be to summarize its findings in a much&nbsp;&nbsp; more straightforward manner. And then finally&nbsp; towards the end you'll see that it'll wrap my&nbsp;&nbsp; output in these final verdict XML tags. So you&nbsp; can see that during this demo we've gone from&nbsp;&nbsp; a skiing accident to sort of unconfident insecure&nbsp; outputs from perhaps a car accident in the second&nbsp;&nbsp; version to now a much more strictly formatted&nbsp; confident output that we can actually build an&nbsp;&nbsp; application around and actually help you know a&nbsp; real world um car insurance company for example.&nbsp;&nbsp; U finally if we transition back to the um slides&nbsp; another key way of shaping CL's output is actually&nbsp;&nbsp; putting words in CL's mouth or as we call it&nbsp; pre-filled responses. You could imagine that&nbsp;&nbsp; parsing XML tags is nice and all but maybe you&nbsp; want a structured JSON output to make sure that&nbsp;&nbsp; uh it's JSON serializable and you can use this&nbsp; in a subse subsequent call for example. Um this&nbsp;&nbsp; is quite simple to do. You could just add that um&nbsp; claude needs to begin its output with a certain&nbsp;&nbsp; format. This could be for example a uh open&nbsp; square bracket squarely bracket for example&nbsp;&nbsp; or even in this case that we see in front of us&nbsp; this would be an XML tag for itinerary. In our&nbsp;&nbsp; case it could also be that final verdict XML tag.&nbsp; Um, and this is just a great way of again shaping&nbsp;&nbsp; how Claude is supposed to respond. Um, without all&nbsp; the preamble if you don't want that, even though&nbsp;&nbsp; that is also key in shaping his output to make&nbsp; sure that Claude is reasoning through the steps&nbsp;&nbsp; that we wanted. So in our case here, we would just&nbsp; wrap it in the final verdict and then parse it&nbsp;&nbsp; afterwards. But you can use prefill as well. Now&nbsp; finally one step that I would like to highlight&nbsp;&nbsp; here as well is that both cloud 3.7 and especially&nbsp; cloud 4 of course is a sort of hybrid reasoning&nbsp;&nbsp; model meaning that there's extended thinking at&nbsp; your disposal. Um and this is something we want&nbsp;&nbsp; to highlight because you can use extended thinking&nbsp; as a crutch for your prompt engineering. Basically&nbsp;&nbsp; you can enable this to make sure that Claude&nbsp; actually has time to think. It adds his thinking&nbsp;&nbsp; tags and the scratch pad. Um and the beauty of&nbsp; that is you can actually analyze that transcript&nbsp;&nbsp; to understand how claude is going about that data.&nbsp; So as we mentioned we have these check boxes where&nbsp;&nbsp; it goes through step by step of the scenario&nbsp; that transpired for the accident. And in many&nbsp;&nbsp; ways there you can actually try to help claude in&nbsp; building this into the system prompt itself. It's&nbsp;&nbsp; not only more token efficient but it's a good way&nbsp; of understanding how these intelligent models that&nbsp;&nbsp; don't have our intuition actually go about the&nbsp; data that we provide them. And because of that,&nbsp;&nbsp; it's quite key in actually trying to break down&nbsp; how your system prompt can get a lot better. Um,&nbsp;&nbsp; and with that said, I think uh I'd like to thank&nbsp; all you for coming today. We'll be around as well.&nbsp;&nbsp; So if you have any questions on prompting, please&nbsp; uh please go ahead. I know there's a prompting.&nbsp;&nbsp; You want to learn more about prompting in an hour.&nbsp; We have prompting for agents and right now we&nbsp;&nbsp; have an amazing demo of Claude plays Pokemon. So&nbsp; don't go anywhere for that. And as Christian said,&nbsp;&nbsp; we'll be around all day. So, I know we&nbsp; didn't have time for Q&amp;A in this session,&nbsp;&nbsp; but uh please come find us if you want to chat.&nbsp; And thank you guys for coming. Thank you so much.

---

## 8. Prompting for Agents

**Preview:**
> Kind: captions Language: en All right, thank you. Thank you everyone for&nbsp; joining us. Uh, so we're picking up with prompting&nbsp;&nbsp; for agents. Um, hopefully you were here for&nbsp; prompting 101 or maybe you're just joining us. U,&nbsp;&nbsp; but I'll give a little intro. My name is Hannah.&nbsp; I'm part of the applied AI team in Anthropic. Hi,&nbsp;&nbsp; I'm Jeremy. I'm on our applied AI team as well&nbsp; and I'm a product engineer. Uh, so we're going&nbsp;&nbsp; to talk about pro...

**Full Transcript:**

Kind: captions Language: en All right, thank you. Thank you everyone for&nbsp; joining us. Uh, so we're picking up with prompting&nbsp;&nbsp; for agents. Um, hopefully you were here for&nbsp; prompting 101 or maybe you're just joining us. U,&nbsp;&nbsp; but I'll give a little intro. My name is Hannah.&nbsp; I'm part of the applied AI team in Anthropic. Hi,&nbsp;&nbsp; I'm Jeremy. I'm on our applied AI team as well&nbsp; and I'm a product engineer. Uh, so we're going&nbsp;&nbsp; to talk about prompting for agents. So, we're&nbsp; going to switch gears a little bit, move on from&nbsp;&nbsp; the basics of prompting, um, and talk about how&nbsp; we do this for agents like playing Pokemon. Uh,&nbsp;&nbsp; so hopefully you were here, uh, for prompting&nbsp; 101 or maybe you have some familiarity with&nbsp;&nbsp; basic prompting. So, we're not going to go over&nbsp; um the really kind of basic console prompting or&nbsp;&nbsp; interacting with Claude and the desktop today.&nbsp; But just a refresher, uh, we think about prompt&nbsp;&nbsp; engineering as kind of programming in natural&nbsp; language. you're thinking about what your agent&nbsp;&nbsp; or your model is going to be doing, what kind&nbsp; of tasks it's accomplishing. You're trying to&nbsp;&nbsp; clearly communicate to the agent, give examples&nbsp; where necessary, um, and give guidelines. Uh,&nbsp;&nbsp; we do, you know, follow kind of a very specific&nbsp; structure for console prompting. I want you to&nbsp;&nbsp; remove this from your mind because it could look&nbsp; very different for an agent. So, for an agent,&nbsp;&nbsp; you may not be laying out this type of very&nbsp; structured prompt. Uh, it's actually going to&nbsp;&nbsp; look a lot different. We're going to allow&nbsp; a lot of different things to come in. So,&nbsp;&nbsp; I'm going to turn it over I'm going to talk about&nbsp; what agents are and then I'll turn it over to&nbsp;&nbsp; Jeremy to talk about how we do this for agents.&nbsp; So, hopefully you have a sense in your mind of&nbsp;&nbsp; what an agent is. At Anthropic, we like to say&nbsp; that agents are models using tools in a loop. So,&nbsp;&nbsp; we give the agent a task and we allow it to work&nbsp; continuously and use tools as it thinks fit. Um,&nbsp;&nbsp; update its decisions based on the information&nbsp; that it's getting back from its tool calls and&nbsp;&nbsp; continue working independently until it completes&nbsp; the task. So that's we kind of keep it as simple&nbsp;&nbsp; as that. Um the environment which is where the&nbsp; agent is working, the tools that the agent has&nbsp;&nbsp; and the system prompt is just where we tell the&nbsp; agent what it should be doing or what it should be&nbsp;&nbsp; accomplishing. And we typically find the simpler&nbsp; you can keep this the better. Allow the agent to&nbsp;&nbsp; do its work. Allow the model to be the model and&nbsp; kind of work through this task. So when do you use&nbsp;&nbsp; agents? You do not always need to use an agent.&nbsp; In fact, there's many scenarios in which you won't&nbsp;&nbsp; actually want to use an agent. There are other&nbsp; approaches that would be more appropriate. Um,&nbsp;&nbsp; agents are really best for complex and&nbsp; valuable tasks. It's not something you&nbsp;&nbsp; should deploy in every possible scenario. You&nbsp; will not get the results that you want. Um,&nbsp;&nbsp; and you'll spend a lot more resources than&nbsp; you maybe need to. So, we'll talk a little&nbsp;&nbsp; bit about checklist or or kind of ways of thinking&nbsp; about when you should be using an agent and maybe&nbsp;&nbsp; you don't want to be using an agent. So, is the&nbsp; task complex? Is this a task that you, a human,&nbsp;&nbsp; can think through a step-by-step process to&nbsp; complete? If so, you probably don't need an&nbsp;&nbsp; agent. You want to use an agent where it's not&nbsp; clear to you how you'll go about accomplishing the&nbsp;&nbsp; task. You might know where you want to go, but you&nbsp; don't know exactly how you're going to get there,&nbsp;&nbsp; what tools, and what information you might need&nbsp; to arrive at the end state. Is a task valuable?&nbsp;&nbsp; Are you going to get a lot of value out of the&nbsp; agent accomplishing this task? Or is this a kind&nbsp;&nbsp; of a low value uh task or workflow? In that case,&nbsp; a workflow might also be better. You don't really&nbsp;&nbsp; want to be using the resources of an agent unless&nbsp; this is something you get that's highly leveraged.&nbsp;&nbsp; It's maybe revenue generating. It's something&nbsp; that's really valuable to your user. Again,&nbsp;&nbsp; it's something that's complex. Uh the last next&nbsp; piece is are the parts of the task doable? So,&nbsp;&nbsp; when you think about the task that has to occur,&nbsp; would you be able to give the agents the tools&nbsp;&nbsp; that it needs in order to accomplish this task?&nbsp; If you can't define the tools or if you can't&nbsp;&nbsp; give the agent access to the information or&nbsp; the tool that it would need, you may want to&nbsp;&nbsp; scope the task down. Um, if you can define and&nbsp; give to the agent the tools that it would want,&nbsp;&nbsp; that's a better use case for an agent. The last&nbsp; thing you might want to think about is the cost of&nbsp;&nbsp; errors or how easy it is to discover errors. So,&nbsp; if it's really uh difficult to correct an error or&nbsp;&nbsp; detect an error, that is maybe not a place where&nbsp; you want the agent to be working independently.&nbsp;&nbsp; you might want to have a human in the loop in&nbsp; that case. If it the error is something that&nbsp;&nbsp; you can recover from or if it's not too costly to&nbsp; have an error occurring, then you might continue&nbsp;&nbsp; to allow the agent to work independently. So to&nbsp; make this a little bit more real, uh we'll talk&nbsp;&nbsp; about a few examples. I'm not going to go through&nbsp; each single one of these, but let's pick out a few&nbsp;&nbsp; that will be pretty clear or intuitive for most of&nbsp; us. So coding, obviously, um all of you are very&nbsp;&nbsp; familiar with using agents and coding. Uh coding&nbsp; is a great use case. We can think about something&nbsp;&nbsp; uh like a design document. And although you know&nbsp; where you want to get to, which is raising a PR,&nbsp;&nbsp; you don't know exactly how you're going to get&nbsp; there. It's not clear to you what you'll build&nbsp;&nbsp; first, how you'll iterate on that, what changes&nbsp; you might make along the way depending on what&nbsp;&nbsp; you find. Um this is high value. You're all very&nbsp; skilled. If an agent, okay, if an agent is able,&nbsp;&nbsp; this is like more like what the midway is like&nbsp; at night. I feel I feel more at home now. Um,&nbsp;&nbsp; uh, Claude Claude is great at coding. Um, and this&nbsp; is a high value use case, right? If your agent is&nbsp;&nbsp; actually able to go from a design document to&nbsp; a PR, that's a lot of time that you, a highly&nbsp;&nbsp; skilled engineer, are saved and you're able to&nbsp; then spend your time on something else that's&nbsp;&nbsp; higher leverage. So, great use case for agents.&nbsp; A couple other examples I'll mention here. Um,&nbsp;&nbsp; maybe we'll talk about the the cost of error.&nbsp; So, search, if we make an error in the search,&nbsp;&nbsp; there's ways that we can correct that, right? So&nbsp; we can use citations, we can use other methods of&nbsp;&nbsp; double-checking the results. So if the agent makes&nbsp; a mistake in the search process, this is something&nbsp;&nbsp; we can recover from and it's probably not too&nbsp; costly. Computer use, um, this is also a place&nbsp;&nbsp; where we can recover from errors. We might just go&nbsp; back, we might try clicking again. It's not, uh,&nbsp;&nbsp; too difficult to allow Claude just to click a few&nbsp; times until it's able to use the tool properly.&nbsp;&nbsp; Um, data analysis, I think, is another interesting&nbsp; example, kind of analogous to coding. We might&nbsp;&nbsp; know uh the end result that we want to get to.&nbsp; We know a set of insights that we want to gather&nbsp;&nbsp; out of data or a visualization that we want to&nbsp; produce from data. We don't know exactly what the&nbsp;&nbsp; data might look like. Uh so the data could have&nbsp; different formats. It could have errors in it.&nbsp;&nbsp; It could have other uh it could have granularity&nbsp; issues that we're not sure how to disagregate. We&nbsp;&nbsp; don't know the exact process that we're going to&nbsp; take in analyzing that data, but we know where we&nbsp;&nbsp; want to get in the end. Um so this is another&nbsp; example of a great use case for agents. Uh,&nbsp;&nbsp; so hopefully these make sense to you and I'm going&nbsp; to turn it over to Jeremy now. He has some really&nbsp;&nbsp; rich experience building agents and he's going to&nbsp; share some best practices for actually prompting&nbsp;&nbsp; them well and how to structure a great prompt&nbsp; for an agent. Thanks Hannah. Hi all. Um, yeah,&nbsp;&nbsp; so prompting for agents. Um, I think some things&nbsp; that we think about here, I I'll go over a few of&nbsp;&nbsp; them. We've learned these experiences mostly from&nbsp; building agents ourselves. So some agents that you&nbsp;&nbsp; can try from enthropic are cla code which works in&nbsp; your terminal and sort of agentically browses your&nbsp;&nbsp; files and uses the bash tool to really accomplish&nbsp; tasks um in coding. Similarly we have our new&nbsp;&nbsp; advanced research feature in cloud.ai and this&nbsp; allows you to do hours of research. For example,&nbsp;&nbsp; you can find hundreds of startups building agents&nbsp; or you can find hundreds of potential prospects&nbsp;&nbsp; for your company. And this allows the model to&nbsp; do research across your tools, your Google Drive,&nbsp;&nbsp; web search and stuff like that. And so in the&nbsp; process of building these products, one things&nbsp;&nbsp; that we learned is that you need to think like&nbsp; your agents. This is maybe the most important&nbsp;&nbsp; principle. Um the idea is that essentially you&nbsp; need to understand and develop a mental model&nbsp;&nbsp; of what your agent is doing and what it's like to&nbsp; be in that environment. So the environment for the&nbsp;&nbsp; agent is a set of tools and the responses it gets&nbsp; back from those tools. In the context of cloud&nbsp;&nbsp; code, the way you might do this is by actually&nbsp; simulating the process and just imagining if you&nbsp;&nbsp; were in cloud code's shoes given the exact tool&nbsp; descriptions it has and the tool schemas it has,&nbsp;&nbsp; would you be confused or would you be able to&nbsp; do do the task that it's doing? If a human can't&nbsp;&nbsp; understand what your agent should be doing, then&nbsp; an AI will not be able to either. And so this is&nbsp;&nbsp; really important for thinking about tool design,&nbsp; thinking about prompting is to simulate and go&nbsp;&nbsp; through their environment. Another is that you&nbsp; need to give your agents reasonable heristics.&nbsp;&nbsp; And so, you know, Hannah mentioned that prompt&nbsp; engineering is conceptual engineering. What does&nbsp;&nbsp; that really mean? It's one of the reasons why&nbsp; prompt engineering is not going away and why I&nbsp;&nbsp; personally expect prompting to get more important,&nbsp; not less important as models get smarter. This is&nbsp;&nbsp; because prompting is not just about text. It's not&nbsp; just about the words that you give the model. It's&nbsp;&nbsp; about deciding what concepts the model should have&nbsp; and what behaviors it should follow to perform&nbsp;&nbsp; well in a specific environment. So for example,&nbsp; cloud code has the concept of irreversibility.&nbsp;&nbsp; It should not take irreversible actions that&nbsp; might harm the user or harm their environment.&nbsp;&nbsp; So it will avoid these kinds of harmful actions&nbsp; or anything that might cause irreversible damage&nbsp;&nbsp; to your environment or to your code or anything&nbsp; like that. So that concept of irreversibility is&nbsp;&nbsp; something that you need to instill in the model&nbsp; and be very clear about and think about the edge&nbsp;&nbsp; cases. How might the model in misinterpret&nbsp; this concept? How might it not know what it&nbsp;&nbsp; means? For example, if you want the model to be&nbsp; very eager and you want it to be very agentic,&nbsp;&nbsp; well, it might go over the top a little bit. It&nbsp; might misinterpret what you're saying and do more&nbsp;&nbsp; than what you expect. And so, you have to be very&nbsp; crisp and clear about the concepts you're giving&nbsp;&nbsp; the models. Um, some examples of these reasonable&nbsp; heristics that we've learned. One is that while&nbsp;&nbsp; we were building research, we noticed that the&nbsp; model would often do a ton of web searches when&nbsp;&nbsp; it was unnecessary. For example, it would find the&nbsp; actual answer it needed. like maybe you would find&nbsp;&nbsp; a list of scaleups in the United States and then&nbsp; it would keep going even though it already had the&nbsp;&nbsp; answer and that's because we hadn't told the model&nbsp; explicitly when you find the answer you can stop&nbsp;&nbsp; you no longer need to keep searching uh similarly&nbsp; we had to give the model sort of budgets to think&nbsp;&nbsp; about for example we told it that for simple&nbsp; queries it should use under five tool calls&nbsp;&nbsp; but for more complex queries it might use up to 10&nbsp; or 15 so these kinds of heruristics that you might&nbsp;&nbsp; assume the model already understands you really&nbsp; have to articulate clearly. A good way to think&nbsp;&nbsp; about this is that if you're managing maybe a new&nbsp; intern who's fresh out of college and has not had&nbsp;&nbsp; a job before, how would you articulate to them&nbsp; how to get around all the problems they might get&nbsp;&nbsp; run into in their first job? And how would you&nbsp; be very crisp and clear with them about how to&nbsp;&nbsp; accomplish that? That's often how you should&nbsp; think about giving heristics to your agents,&nbsp;&nbsp; which are just general principles that it&nbsp; should follow. They may not be strict rules,&nbsp;&nbsp; but they're, you know, sort of practices.&nbsp; Another point is that tool selection is key.&nbsp;&nbsp; So as models get more powerful able to handle more&nbsp; and more tools. Sonnet 4 and Opus 4 can handle&nbsp;&nbsp; you know up to a hundred tools even more than&nbsp; that if you have great prompting. But in order&nbsp;&nbsp; to use these tools you have to be clear about&nbsp; which tools it should use for different tasks.&nbsp;&nbsp; So for example for research we can give the model&nbsp; access to Google Drive. We can give it access to&nbsp;&nbsp; MCP tools like Sentry or Data Dog or GitHub. It&nbsp; can search across all these tools, but the model&nbsp;&nbsp; doesn't know already which tools are important&nbsp; for which tasks. Especially in your specific&nbsp;&nbsp; company context. For example, if your company uses&nbsp; Slack a lot, maybe it should default to searching&nbsp;&nbsp; Slack for company related information. All these&nbsp; questions about how the model should use tools,&nbsp;&nbsp; you have to give it explicit principles about&nbsp; when to use which tools and in which contexts. Um,&nbsp;&nbsp; and this is really important and it's often&nbsp; something I see where people don't prompt the&nbsp;&nbsp; agent at all about which tool to use and they&nbsp; just give the model some tools with some very&nbsp;&nbsp; short descriptions and then they wonder like&nbsp; why isn't the model using the right tool? Well,&nbsp;&nbsp; it's likely because the model doesn't know what&nbsp; it should be doing in that context. Another point&nbsp;&nbsp; here is that you can guide the thinking process.&nbsp; So people often sort of turn extended thinking on&nbsp;&nbsp; and then let their agents run and assume it will&nbsp; get out of the box better performance. Actually&nbsp;&nbsp; that assumption is true. Most of the time you will&nbsp; get out of the box better performance, but you can&nbsp;&nbsp; squeeze even more performance out of it if you&nbsp; just prompt the agent to use its thinking well.&nbsp;&nbsp; So for example, for search, what we do is tell&nbsp; the model to plan out its search process. So in&nbsp;&nbsp; advance, it should decide how complicated is this&nbsp; query? How many tool calls should I use here? What&nbsp;&nbsp; sources should I look for? How will I know when&nbsp; I'm successful? We tell it to plan out all these&nbsp;&nbsp; exact things in its first thinking block. And then&nbsp; a new capability that the cloud 4 models have is&nbsp;&nbsp; the ability to use interled thinking between tool&nbsp; calls. So after getting results from the web, we&nbsp;&nbsp; often find that models assume that all web search&nbsp; results are true, right? They don't have any,&nbsp;&nbsp; you know, we we haven't told them explicitly that&nbsp; this isn't the case. And so they might take these&nbsp;&nbsp; web results and run with them immediately. So,&nbsp; one thing we prompted our models to do is to use&nbsp;&nbsp; this interleaf thinking to really reflect on the&nbsp; quality of the search results and decide if they&nbsp;&nbsp; need to verify them, if they need to get more&nbsp; information, or if they should add a disclaimer&nbsp;&nbsp; about how the results might not be accurate. Um,&nbsp; another point with when prompting agents is that&nbsp;&nbsp; agents are more unpredictable than workflows&nbsp; or just, you know, classification type prompts.&nbsp;&nbsp; Most changes will have unintended side effects.&nbsp; This is because agents will operate in a loop&nbsp;&nbsp; autonomously. And so for example, if you tell the&nbsp; agent, you know, keep searching until you find the&nbsp;&nbsp; correct answer, you know, find the highest quality&nbsp; possible source and always keep searching until&nbsp;&nbsp; you find that source. What you might run into is&nbsp; the unintended side effect of the agent just not&nbsp;&nbsp; finding any sources. Maybe this perfect source&nbsp; doesn't exist for the for the query. And so it&nbsp;&nbsp; will just keep searching until it hits its context&nbsp; window. And that's actually what we ran into as&nbsp;&nbsp; well. And so you have to tell the agent if you&nbsp; don't find the perfect source, that's okay. You&nbsp;&nbsp; can stop after a few tool calls. Um, so just be&nbsp; aware that your prompts may have unintended side&nbsp;&nbsp; effects and you may have to roll those back.&nbsp; Another point is to help the agent manage its&nbsp;&nbsp; context window. The Cloud 4 models have a 200k&nbsp; token context window. Um, this is long enough for&nbsp;&nbsp; a lot of longrunning tasks, but when you're using&nbsp; an agent to do work autonomously, you may hit this&nbsp;&nbsp; context window and there are several strategies&nbsp; you can use to sort of extend the effective&nbsp;&nbsp; context window. One of them that we use for cloud&nbsp; code is called compaction. And this is just a tool&nbsp;&nbsp; that the model has um that will automatically be&nbsp; called once it hits around 190,000 tokens. So near&nbsp;&nbsp; the context window. And this will summarize&nbsp; or compress everything in the context window&nbsp;&nbsp; to a really dense but accurate summary that is&nbsp; then passed to a new instance of claude with the&nbsp;&nbsp; summary. And it continues the process. And we find&nbsp; that this essentially allows you to run infinitely&nbsp;&nbsp; with cloud code. You almost never run out of&nbsp; context. um occasionally it will miss details&nbsp;&nbsp; from the previous session but the vast majority of&nbsp; the time this will keep all the important details&nbsp;&nbsp; and the model will sort of remember what happened&nbsp; in the last session. Similarly you can sort of&nbsp;&nbsp; write to an external file. So the model can have&nbsp; access to an extra file and these cloud for models&nbsp;&nbsp; are especially good at writing memory to a file&nbsp; and they can use this file to essentially extend&nbsp;&nbsp; their context window. Another point is that you&nbsp; can use sub aents. Um, we won't talk about this&nbsp;&nbsp; a lot here, but essentially if you have agents&nbsp; that are always hitting their context windows, you&nbsp;&nbsp; may delegate some of what the agent is doing to&nbsp; another agent. Um, which can sort of, for example,&nbsp;&nbsp; you can have one agent be the lead agent and then&nbsp; sub agents do the actual searching process. Then&nbsp;&nbsp; the sub agents can compress the results to the&nbsp; lead agent in a really dense form that doesn't&nbsp;&nbsp; use as many tokens and the lead agent can give the&nbsp; final report to the user. So we actually use this&nbsp;&nbsp; process in our research system and this allows you&nbsp; to sort of compress what's going on in the search&nbsp;&nbsp; and then only use the context window for the lead&nbsp; agent for actually writing the report. So this&nbsp;&nbsp; kind of multi- aent system can be effective&nbsp; for limiting the context window. Finally,&nbsp;&nbsp; you can let Claude be Claude. And essentially&nbsp; what this means is that Claude is great at being&nbsp;&nbsp; an agent already. You don't have to do a ton of&nbsp; work at the very beginning. So, I would recommend&nbsp;&nbsp; just trying out your system with sort of a bare&nbsp; bones prompt and barebones tools and seeing where&nbsp;&nbsp; it goes wrong and then working from there. Don't&nbsp; sort of assume that Claude can't do it ahead of&nbsp;&nbsp; time because cloud often will surprise you with&nbsp; how good it is. Um, I talked already about tool&nbsp;&nbsp; design, but essentially the key point here is you&nbsp; want to make sure that your tools are good. Um,&nbsp;&nbsp; what is a good tool? It will have a simple&nbsp; accurate tool name that reflects what it does.&nbsp;&nbsp; You'll have tested it and make sure that it works&nbsp; well. um it'll have a well-formed description&nbsp;&nbsp; so that a human reading this tool like imagine&nbsp; you give a function to another engineer on your&nbsp;&nbsp; team would they understand this function and be&nbsp; able to use it. You should ask the same question&nbsp;&nbsp; about the agent computer interfaces or the tools&nbsp; that you are giving your agent. Make sure that&nbsp;&nbsp; they're usable and clear. Um we also often find&nbsp; that people will give an agent a bunch of tools&nbsp;&nbsp; that have very similar names or descriptions.&nbsp; So for example, you give it six search tools&nbsp;&nbsp; and each of the search tools searches a slightly&nbsp; different database. This will confuse the model.&nbsp;&nbsp; So try to keep your tools fairly distinct&nbsp; um and combine similar tools into just one.&nbsp;&nbsp; So, one quick example here is just that you can&nbsp; have an agent, for example, use these different&nbsp;&nbsp; tools to first search the inventory in a database,&nbsp; run a query. Based on the information it finds, it&nbsp;&nbsp; can reflect on the inventory, think about it for&nbsp; a little bit, then decide to generate an invoice,&nbsp;&nbsp; generate this invoice, think about what it should&nbsp; do next, and then decide to send an email. And so,&nbsp;&nbsp; this loop involves the agent getting information&nbsp; from the database, which is its external&nbsp;&nbsp; environment, using its tools, and then updating&nbsp; based on that information. until it accomplishes&nbsp;&nbsp; the task. And that's sort of how agents work&nbsp; in general. So, let's walk through a demo real&nbsp;&nbsp; quick. I'll switch to my computer. Um, so you can&nbsp; see here that this is our console. The console is&nbsp;&nbsp; a great tool for sort of simulating your prompts&nbsp; and seeing what they would look like in a UI. Um,&nbsp;&nbsp; and I use this while we were iterating on research&nbsp; to sort of understand what's really going on and&nbsp;&nbsp; what the agents doing. This is a great way to&nbsp; think like your agents and sort of put yourself&nbsp;&nbsp; in their shoes. So, you can see we have a big&nbsp; prompt here. Um, it's not sort of super long.&nbsp;&nbsp; It's around a thousand tokens. It involves the&nbsp; researcher going through a research process. We&nbsp;&nbsp; tell it exactly what should what it what it should&nbsp; plan ahead of time. We tell it how many tool&nbsp;&nbsp; calls it should typically use. We give it some&nbsp; guidelines about what facts it should think about,&nbsp;&nbsp; what makes a high quality source, stuff like&nbsp; that. And then we tell it to use parallel tool&nbsp;&nbsp; calls. So, you know, run multiple web searches in&nbsp; parallel at the same time rather than running them&nbsp;&nbsp; all sequentially. Then we give it this question.&nbsp; How many bananas can fit in a Rivian R1S? This&nbsp;&nbsp; is not a question that the model will be able&nbsp; to answer because the Rivian R1S came out very&nbsp;&nbsp; recently. It's a car. It doesn't know in advance&nbsp; all the specifications and everything. So, it'll&nbsp;&nbsp; have to search the web. Let's run it and see what&nbsp; happens. You'll see that at the very beginning,&nbsp;&nbsp; it will think and break down this request. And&nbsp; so, it realizes, okay, web search is going to&nbsp;&nbsp; be helpful here. I should get cargo capacity.&nbsp; I should search. Um, woo. Um, and you see here&nbsp;&nbsp; it ran two web searches in parallel at the same&nbsp; time. That allowed it to get these results back&nbsp;&nbsp; very quickly. And then it's reflecting on the&nbsp; results. So it's realizing, okay, I found the&nbsp;&nbsp; banana dimensions. I know that a USDA identifies&nbsp; bananas as 7 to 8 in long. I need to run another&nbsp;&nbsp; web search. Let me convert these to more standard&nbsp; measurements. You can see it's using tool calls&nbsp;&nbsp; interled with thinking, which is something&nbsp; new that the quad 4 models can do. Finally,&nbsp;&nbsp; it's running some calculations. It's about how&nbsp; many bananas could be packed into the cargo space&nbsp;&nbsp; of the truck. And it's running a few more web&nbsp; searches. You can see here that this is a fairly pending approximately 48,000 bananas. I've seen the model&nbsp; estimate anything between 30,000 50,000. I think&nbsp;&nbsp; the right answer is around 30,000. So this is this&nbsp; is roughly correct. Um going back to the slides,&nbsp;&nbsp; I think that you know this this sort of approach&nbsp; of testing out your prompt, seeing what tools&nbsp;&nbsp; the model calls, reading its thinking blocks,&nbsp; and actually seeing how the model's thinking&nbsp;&nbsp; will often make it really obvious. um what the&nbsp; issues are and what's going wrong. So you'll&nbsp;&nbsp; test it out and you'll just see like okay&nbsp; maybe the model's using too many tools here,&nbsp;&nbsp; maybe it's using the wrong sources or maybe&nbsp; it's just following the wrong guidelines. Um&nbsp;&nbsp; so this is a really helpful way to sort of think&nbsp; like your agents and make them more concrete. Um switching back to the slides. Okay, so eval evaluations are really important&nbsp; for any system. Um, they're really important&nbsp;&nbsp; for systematically measuring whether you're&nbsp; making progress in your prompt. Very quickly,&nbsp;&nbsp; you'll notice that it's difficult to really make&nbsp; progress on a prompt if you don't have an eval&nbsp;&nbsp; that tells you meaningfully whether your prompt is&nbsp; getting better and whether your system is getting&nbsp;&nbsp; better. But eval are much more difficult for&nbsp; agents. Um, agents are longunning. They do a bunch&nbsp;&nbsp; of things. They may not they may not always have&nbsp; a predictable process. classification is easier to&nbsp;&nbsp; eval because you can just check did it classify&nbsp; this output correctly but agents are harder. So&nbsp;&nbsp; a few tips to make this a bit easier. One is that&nbsp; the larger the effect size the smaller the sample&nbsp;&nbsp; size you mean you need um and so this is sort of&nbsp; just a principle from science in general where&nbsp;&nbsp; if an effect size is very large for example if a&nbsp; medication will cure people immediately you don't&nbsp;&nbsp; really need a large sample size of a ton of people&nbsp; to know that the model is that that this treatment&nbsp;&nbsp; is having an effect. Similarly, when you change a&nbsp; prompt, if it's really obvious that the system is&nbsp;&nbsp; getting better, you don't need a large eval. I&nbsp; often see teams think that they need to set up&nbsp;&nbsp; a huge eval of like hundreds of test cases and&nbsp; make it completely automated when they're just&nbsp;&nbsp; starting out building an agent. This is a failure&nbsp; mode and it's an antiattern. You should start out&nbsp;&nbsp; with a very small eval and just run it and see&nbsp; what happens. You can even start out manually. Um,&nbsp;&nbsp; but the important thing is to just get started.&nbsp; I often see teams delaying evals because they&nbsp;&nbsp; think that they're so intimidating or that they&nbsp; need such a sort of intense eval to really get&nbsp;&nbsp; some signal, but you can get great signal from&nbsp; a small number of test cases. You just want to&nbsp;&nbsp; keep those test cases s consistent and then keep&nbsp; testing them so you know whether the model and&nbsp;&nbsp; the prompt is getting better. You also want to&nbsp; use realistic tasks. So don't just sort of come&nbsp;&nbsp; up with arbitrary prompts or descriptions&nbsp; or tasks that don't really have any real&nbsp;&nbsp; correlation to what your system will be doing.&nbsp; For example, if you're working on coding tasks,&nbsp;&nbsp; you don't won't want to give the model just&nbsp; competitive programming problems because this is&nbsp;&nbsp; not what real world coding is like. You'll want to&nbsp; give it realistic tasks that really reflect what&nbsp;&nbsp; your agent will be doing. Similarly, in finance,&nbsp; you'll want to sort of take tasks that real people&nbsp;&nbsp; are trying to solve and just use them to evaluate&nbsp; whether the model can do those. This allows you&nbsp;&nbsp; to really measure whether the model is getting&nbsp; better at the tasks that you care about. Another&nbsp;&nbsp; point is that LLM is judge is really powerful,&nbsp; especially when you give it a rubric. So agents&nbsp;&nbsp; will have lots of different kinds of outputs.&nbsp; For example, if you're using them for search,&nbsp;&nbsp; they might have tons of different kinds of search&nbsp; reports with different kinds of structure. But LMS&nbsp;&nbsp; are great at handling lots of different kinds of&nbsp; structure and text with different characteristics.&nbsp;&nbsp; And so one thing that we've done, for example,&nbsp; is given the model just a clear rubric and then&nbsp;&nbsp; ask it to evaluate the output of the agent.&nbsp; For example, for search tasks, we might give&nbsp;&nbsp; it a rubric that says, check that the model,&nbsp; you know, um, looked at the right sources,&nbsp;&nbsp; check that it got the correct answer. In this&nbsp; case, we might say, um, check that the model&nbsp;&nbsp; guessed that the amount of bananas that can fit&nbsp; in a Rivian R1s is between like 10,000 and 50,000.&nbsp;&nbsp; Anything outside that range is not realistic. So,&nbsp; you know, you can use things like that to sort of&nbsp;&nbsp; benchmark whether the model is getting the right&nbsp; answers, whether it's following the right process.&nbsp;&nbsp; At the end of the day though, nothing is a perfect&nbsp; replacement for human evals. You need to test the&nbsp;&nbsp; system manually. You need to see what it's doing.&nbsp; You need to sort of look at the transcripts, look&nbsp;&nbsp; at what the model is doing, and sort of understand&nbsp; your system if you want to make progress on it.&nbsp;&nbsp; Here are some examples of eval. So one example&nbsp; that I sort of showed uh talked about is answer&nbsp;&nbsp; accuracy. And this is where you just use an LLM&nbsp; as judge to judge whether the answer is accurate.&nbsp;&nbsp; So for example in this case you might say the&nbsp; agent needs to use a tool to query the number&nbsp;&nbsp; of employees and then report the answer and then&nbsp; you know the number of employees at your company.&nbsp;&nbsp; So you can just check that with an LM as judge.&nbsp; The reason you use an LMS as judge here is because&nbsp;&nbsp; it's more robust to variations. For example, if&nbsp; you're just checking for the integer 47 in this&nbsp;&nbsp; case in the output that is not very robust and&nbsp; if the model says 47 as text you'll grade it&nbsp;&nbsp; incorrectly. So you want to use an LMS as judge&nbsp; there to be robust to those minor variations.&nbsp;&nbsp; Another way you can eval agents is tool use&nbsp; accuracy. Agents involve using tools in a&nbsp;&nbsp; loop. And so if you know in advance what tools&nbsp; the model should use or how it should use them,&nbsp;&nbsp; you can just evaluate if it used the correct tools&nbsp; in the process. For example, in this case, I might&nbsp;&nbsp; evaluate the agent should use web search at least&nbsp; five times to answer this question. And so I could&nbsp;&nbsp; just check in the transcript programmatically did&nbsp; the tool call for web search appear five times or&nbsp;&nbsp; not. Similarly, you might check in this case&nbsp; for in response to the question book a flight,&nbsp;&nbsp; the agent should use the search flights tool&nbsp; and you can just check that programmatically&nbsp;&nbsp; and this allows you to make sure that the right&nbsp; tools are being used at the right times. Finally,&nbsp;&nbsp; a really good eval for agents is tobench. You&nbsp; can sort of look this up. Towen is a sort of open&nbsp;&nbsp; source benchmark that shows that you can evaluate&nbsp; whether agents reach the correct final state.&nbsp;&nbsp; So a lot of agents are sort of modifying a&nbsp; database or interacting with a user in a way&nbsp;&nbsp; where you can say the model should always get to&nbsp; this state at the end of the process. For example,&nbsp;&nbsp; if your agent is a customer service agent for&nbsp; airlines and the user asks to change their flight&nbsp;&nbsp; at the end of the agentic process in response to&nbsp; that prompt, it should have changed the flight in&nbsp;&nbsp; the database. And so you can just check at the end&nbsp; of the agentic process, was the flight changed?&nbsp;&nbsp; was this row in the database changed to a&nbsp; different date and that can verify that the&nbsp;&nbsp; agent is working correctly. This is really robust&nbsp; and you can use it a lot in a lot of different use&nbsp;&nbsp; cases. For example, you can check that your&nbsp; database is updated correctly. You can check&nbsp;&nbsp; that certain files were modified, things like&nbsp; that as a way to evaluate the final state that&nbsp;&nbsp; the agent reaches. And that's it from us. Um,&nbsp; we're happy to take your questions. [Applause] Can you talk about building prompts for agents?&nbsp; Are you giving it kind of long longer prompts&nbsp;&nbsp; first and then iterating or you starting kind&nbsp; of chunk by chunk? Uh what's that look like?&nbsp;&nbsp; And can you show sort of a little bit more on that&nbsp; thought process? That's a great question. Um can&nbsp;&nbsp; I switch back to my screen actually? I just want&nbsp; to sort of show the demo. Thank you. Um, yeah. So,&nbsp;&nbsp; you can see this is sort of a final prompt&nbsp; that we've arrived at, but this is not where&nbsp;&nbsp; we started. I think the answer to your question&nbsp; is that you start with a short simple prompt. Um, and I might just say search the web&nbsp; aentically. I'll change this to a different&nbsp;&nbsp; question. Um, how good are the Cloud 4 models&nbsp; and then we'll just run that. And so you'll&nbsp;&nbsp; want to start with something very simple and just&nbsp; see how it works. You'll often find that Claude&nbsp;&nbsp; can do the task well out of the box. But if you&nbsp; have more needs and you need it to operate really&nbsp;&nbsp; consistently in production, you'll notice edge&nbsp; cases or small flaws as you test with more use&nbsp;&nbsp; cases. And so you'll sort of add those into the&nbsp; prompt. So I would say building an agent prompt&nbsp;&nbsp; what it looks like concretely is start simple,&nbsp; test it out, see what happens, iterate from there,&nbsp;&nbsp; start collecting test cases where the model fails&nbsp; or succeeds and then over time try to increase the&nbsp;&nbsp; number of test cases that pass. Um, and the way&nbsp; to do this is by sort of adding instructions,&nbsp;&nbsp; adding examples to the prompt. But you really&nbsp; only do that when you find out what the edge&nbsp;&nbsp; cases are. And you can see that it thinks that&nbsp; the models are indeed good. So that's great.&nbsp;&nbsp; when I do like normal prompting and it's not&nbsp; agentic, uh I'll often give like a few shot&nbsp;&nbsp; example of like, hey, here's like input,&nbsp; here's output. This works really well for&nbsp;&nbsp; like classification tasks, stuff like that, right?&nbsp; Uh is there a parallel here in this like agentic&nbsp;&nbsp; world? Are you finding that that's ever helpful&nbsp; or should I not think about it that way? That is&nbsp;&nbsp; a great question. Yeah. So should you include&nbsp; fewshot examples in your prompt and sort of&nbsp;&nbsp; traditional prompting techniques involve like&nbsp; giving the saying the model should use a chain&nbsp;&nbsp; of thought and then giving a few shot examples&nbsp; like a bunch of examples to imitate. We find&nbsp;&nbsp; that these techniques are not as effective for&nbsp; state-of-the-art frontier models and for agents.&nbsp;&nbsp; Um the main reason for this is that if you give&nbsp; the model a bunch of examples of exactly what&nbsp;&nbsp; process it should follow, that just limits the&nbsp; model too much. These models are smarter than&nbsp;&nbsp; you can predict and so you don't want to tell&nbsp; them exactly what they need to do. Similarly,&nbsp;&nbsp; chain of thought has just been trained into the&nbsp; models at this point. The models know to think&nbsp;&nbsp; in advance. They don't need to be told like use&nbsp; chain of thought. But what we can do here is one&nbsp;&nbsp; you can tell the model how to use its thinking.&nbsp; So you know I talked about earlier rather than&nbsp;&nbsp; telling the model you need to use a chain of&nbsp; thought. It already knows that. You can just&nbsp;&nbsp; say use your thinking process to plan out your&nbsp; search or to plan out what you're going to do&nbsp;&nbsp; in terms of coding. Reme or you can tell it to&nbsp; remember specific things in its thinking process&nbsp;&nbsp; and that sort of helps the agent stay on track.&nbsp; As far as examples go, um you'll want to give&nbsp;&nbsp; the model examples but not too prescriptive.&nbsp; I think we are out of time, but you can come&nbsp;&nbsp; up to me personally and I'll talk to you all&nbsp; after. Thanks. Thank you. Thanks for coming.

---

## 9. Spotlight on Canva

**Preview:**
> Kind: captions Language: en Hello. Hi everyone. Hello. Hello. Thank you. Thank you all so&nbsp; much for joining me today. My name is Danny.&nbsp;&nbsp; Danny Woo. I'm head of AI products at Canva&nbsp; and we're a visual communications platform.&nbsp;&nbsp; Today I'm really excited to share a bit about&nbsp; how how we've have built camera claude and how&nbsp;&nbsp; we've been working with um anthropic and using the&nbsp; amazing claude models. Um I'll be first starting&nbsp;&nbsp; by giving an...

**Full Transcript:**

Kind: captions Language: en Hello. Hi everyone. Hello. Hello. Thank you. Thank you all so&nbsp; much for joining me today. My name is Danny.&nbsp;&nbsp; Danny Woo. I'm head of AI products at Canva&nbsp; and we're a visual communications platform.&nbsp;&nbsp; Today I'm really excited to share a bit about&nbsp; how how we've have built camera claude and how&nbsp;&nbsp; we've been working with um anthropic and using the&nbsp; amazing claude models. Um I'll be first starting&nbsp;&nbsp; by giving an intro to Canva as well as taking you&nbsp; behind the story of Canva code like what gave us&nbsp;&nbsp; the idea why we built it as well as the things&nbsp; we learned and finish up by sharing sharing a&nbsp;&nbsp; few lessons that will hopefully be helpful and&nbsp; some time in Q&amp;A. So, I'm incredibly excited and&nbsp;&nbsp; let's jump right in. Firstly, just a little bit&nbsp; of context about Canva in case um you don't know&nbsp;&nbsp; what we are. Um what we do is we really bring&nbsp; the entire design journey and experience in to&nbsp;&nbsp; one simple page and one simple experience. Um a&nbsp; decade ago, design used to be quite inaccessible&nbsp;&nbsp; and it was just quite quite hard. You had to you&nbsp; had to learn how to use fairly complex tools.&nbsp;&nbsp; um they could cost a lot of money and you&nbsp; had to go to so many different platforms and&nbsp;&nbsp; services to complete a design like finding&nbsp; stock photos, getting the right fonts,&nbsp;&nbsp; um starting with a template or publishing. And&nbsp; what we do is we really have integrated all&nbsp;&nbsp; the different steps of this journey into just&nbsp; one page and one platform. So you can create a&nbsp;&nbsp; presentation or you can create a poster, use one&nbsp; of the templates, generate some amazing AI images,&nbsp;&nbsp; um create some text and publish to a website,&nbsp; publish to or print it out, um and get delivered&nbsp;&nbsp; to your door. Um that's that's really what we&nbsp; do. Um and so we have a pretty simple mission&nbsp;&nbsp; and we've had the same mission for the past&nbsp; decade and a bit and it's really to empower the&nbsp;&nbsp; world to design. When we think about this, like we&nbsp; really think about empowering absolutely everyone,&nbsp;&nbsp; whether you are a student, whether you're a&nbsp; teacher, whether you are a nonprofit, or whether&nbsp;&nbsp; you are a marketer and creative um creative in a&nbsp; large enterprise um to design just about anything,&nbsp;&nbsp; publish it anywhere with every ingredient like&nbsp; photos, video, music, and more supporting every&nbsp;&nbsp; language as well as every single device. And the&nbsp; thing about AI is that like we really see it as&nbsp;&nbsp; something that absolutely supercharges the core&nbsp; premise of what Canva was um built to do, which is&nbsp;&nbsp; to allow everyone to to make designs really simply&nbsp; and really easily and make it take less time&nbsp;&nbsp; instead of this being time consuming and tedious.&nbsp; Let you create amazing outputs really really&nbsp;&nbsp; quickly. And the as I'm sure you all know like the&nbsp; power of generative AI is just really helping us&nbsp;&nbsp; helping us accelerate on all of those fronts.&nbsp; So when it comes to our AI strategy, we have&nbsp;&nbsp; a three-pronged approach. When it comes to things&nbsp; like really to things like specialized things like&nbsp;&nbsp; design generation and rich understanding, that's&nbsp; where we're really investing our efforts into&nbsp;&nbsp; developing our own models for our unique needs. Uh&nbsp; we're also really huge fans of a flexible approach&nbsp;&nbsp; where we add magic everywhere to the Canva product&nbsp; through the best commercial commercially available&nbsp;&nbsp; and open source models. And this really helps us&nbsp; deliver AI powered features and workflows to users&nbsp;&nbsp; um much faster much more scalably and also much&nbsp; cheaper. And finally to round things off um we&nbsp;&nbsp; have our app ecosystem where any developer can&nbsp; build apps including AI apps um for all of our 300&nbsp;&nbsp; million plus users. Um and just a bit of a short&nbsp; timeline of our AI journey. We started in 2017&nbsp;&nbsp; using machine learning for search rankings and&nbsp; color recommendations and that was already really&nbsp;&nbsp; cool. Uh we launched background remover which&nbsp; lets you take any image and would use computer&nbsp;&nbsp; vision to remove um the background from it and&nbsp; that continues to be one of our most popular and&nbsp;&nbsp; highest revenue driving features uh with diffusion&nbsp; models and the geni wave. Um, we launched texture&nbsp;&nbsp; image, magic write, magic edit, magic design,&nbsp; and so much more as part of canvas magic studio,&nbsp;&nbsp; which is a collection of integrated and native AI&nbsp; features that help you with just about every step&nbsp;&nbsp; of design. Um, and actually just a month ago&nbsp; in LA, uh, we had, um, we had Canva create uh,&nbsp;&nbsp; and we had some very exciting launches.&nbsp; We launched Canva AI, we launched, um,&nbsp;&nbsp; Canva sheet, and of course, Cana code, which I'm&nbsp; here to talk about. So what is Canva code? Let me&nbsp;&nbsp; play a little video. If you have an idea, a dream&nbsp; of what you want to create, just um in camera AI,&nbsp;&nbsp; just prompt it, describe what you want, and we use&nbsp; Claude to generate amazing interactive prototypes&nbsp;&nbsp; that really understand you and really help just&nbsp; about everyone, especially non-technical folks&nbsp;&nbsp; and non-engineers create their dreams. And then&nbsp; with a one with a one click you can including a&nbsp;&nbsp; camera design you can publish your website get a&nbsp; custom domain everything without touching a single&nbsp;&nbsp; line of code or knowing what CLI is. So what's&nbsp; the story behind it? And you might also think,&nbsp;&nbsp; you know, design and code, they don't seem like&nbsp; they naturally mix, right? So why did we I guess&nbsp;&nbsp; um why did we make Canva code? So I actually want&nbsp; to zoom out a little bit and um when we build&nbsp;&nbsp; products at Canva, we like to describe as going&nbsp; from the chaos to clarity spectrum. So at the very&nbsp;&nbsp; start, like in it might just be a little idea.&nbsp; It might just be like a simple um simple like&nbsp;&nbsp; vision deck that's not really thought through of&nbsp; something that's really really cool. And there's&nbsp;&nbsp; a stage where there's just a lot of openness, but&nbsp; also a lot of a lot of confusion and chaos and&nbsp;&nbsp; just what what do we actually want to build? How&nbsp; should the flows work? Like who is this for? Like&nbsp;&nbsp; how might how might we build it? And um as we as&nbsp; we answer those questions and um get more designs,&nbsp;&nbsp; really refine them, we navigate through those&nbsp; stages and get to extreme clarity on the other&nbsp;&nbsp; end, which is definitely all of our happy happy&nbsp; place. We found that interactive prototypes are&nbsp;&nbsp; just so so so incredibly helpful at the earliest&nbsp; stages of chaos. They're so much more expressive.&nbsp;&nbsp; They allow they allow teams they allow teams&nbsp; product engineering design leads to communicate&nbsp;&nbsp; ideas using the highest fidelity. They allow&nbsp; stakeholders to actually try through try and click&nbsp;&nbsp; through all the different um the actual experience&nbsp; and give feedback um test with users. And this is&nbsp;&nbsp; this really helps us like go from go from one to&nbsp; the very very end. But the thing about interactive&nbsp;&nbsp; prototypes is that they can actually take a bit&nbsp; of engineering time, very precious engineering&nbsp;&nbsp; time while we're working on so many different&nbsp; plans, so many different goals. Um, closing&nbsp;&nbsp; the loop with our users and fixing bugs from&nbsp; our past launchers. Um, so so they can there's&nbsp;&nbsp; a resource cost to making really amazing high&nbsp; fidelity prototypes. Um and oftentimes, you know,&nbsp;&nbsp; if you have um if you have a triad of a product&nbsp; manager, a designer, engineer like jammy on this,&nbsp;&nbsp; there can also be a lot of back and forth to get&nbsp; something really right and really capture it,&nbsp;&nbsp; especially for very wild ideas. We actually&nbsp; discovered um just how amazing it is to be&nbsp;&nbsp; prototyping using claude internally and a lot of&nbsp; our teams have started um actually using claude to&nbsp;&nbsp; make interact with artifacts is artifacts really&nbsp; changed how we do product development and dreaming&nbsp;&nbsp; at Canva. So when we had new ideas and new&nbsp; features that we want to build or new flows like&nbsp;&nbsp; um we're making active prototype and putting&nbsp; up and share it around and that that really&nbsp;&nbsp; accelerated things and enabled us to make way&nbsp; more prototypes. Um so we started prototyping&nbsp;&nbsp; with cord and we started embedding them in camera&nbsp; designs. You can actually see like a super early&nbsp;&nbsp; claude artifact. we were trying we're trying&nbsp; to is to update our search box um and include&nbsp;&nbsp; um include our latest AI features. So we did a&nbsp; very very low fidelity like tailwind um prototype&nbsp;&nbsp; and um if you've used camera recently um you see&nbsp; that um you see that we've launched we call this&nbsp;&nbsp; internally the wonder box but then we also kind&nbsp; of realized this is so cool this is absolutely&nbsp;&nbsp; amazing power the just using for internal&nbsp; prototypes that's like 1% of the way there we&nbsp;&nbsp; realized just how much like bringing the power of&nbsp; coding to everyday people and nontechnical users&nbsp;&nbsp; unlocks. It's a whole whole world of possibility.&nbsp; So whether you're say like a teacher creating&nbsp;&nbsp; an educational game or whether you're like a&nbsp; wedding planner creating a seating channel tool&nbsp;&nbsp; or like a marketer building interactive activation&nbsp; experiences, the possibilities are truly endless.&nbsp;&nbsp; And internally in Canva across all our teams, we&nbsp; really started using this functionality more and&nbsp;&nbsp; more for just about everything. But just like on&nbsp; you know just like kind of canvas like DNA we also&nbsp;&nbsp; realize that this this power is something that is&nbsp; very broadly inaccessible. Um just about one just&nbsp;&nbsp; about half a% of internet users are engineers and&nbsp; know how to code which is a pretty small number.&nbsp;&nbsp; Um, so what if we could bring this amazing power&nbsp; to the whole world, the n the rest of the 99.5 and&nbsp;&nbsp; that's really where like the idea the idea and the&nbsp; vision for camera code was born. We really want&nbsp;&nbsp; to we really wanted to make it incredibly easy&nbsp; for people to create entirely new interactive&nbsp;&nbsp; experiences like whether it's, you know, like a&nbsp; lovely thank you card to the team, whether it's a&nbsp;&nbsp; checklist for closing out the office, whether it's&nbsp; um a meal plan generator or even something like a&nbsp;&nbsp; timeline in a Canva presentation. There's so many&nbsp; possibilities and these actually real examples&nbsp;&nbsp; um that our team has been creating. So what did&nbsp; we do? We did something that was actually pretty&nbsp;&nbsp; pretty new to how we do product um back then&nbsp; which was just maybe two or three months ago&nbsp;&nbsp; and we started making a functional prototype&nbsp; using using claude um claude code wasn't out&nbsp;&nbsp; yet so we just used um yeah we just used um that&nbsp; sonet 3.5 back then and what we did what we did&nbsp;&nbsp; with a functional prototype was um it was a fully&nbsp; working and hooked up experience that's um not&nbsp;&nbsp; part of the main camera code base and just really&nbsp; allowed us to test like test different ideas test&nbsp;&nbsp; different UI and concepts um and do that much&nbsp; faster and actually play around with it and use&nbsp;&nbsp; it every day. So it let it let us test ideas and&nbsp; test concepts including really dreamy and really&nbsp;&nbsp; wacky ones very quickly. It also allowed us to&nbsp; get real user feedback. So we did user testing&nbsp;&nbsp; sessions and we put this in front of users and&nbsp; gave them very little guidance and just saw what&nbsp;&nbsp; the use cases were, saw how they use it, saw where&nbsp; they got confused, saw where the magic moment was&nbsp;&nbsp; and it was also a really efficient ground for us&nbsp; to be experimenting with all the different models&nbsp;&nbsp; that are out there. And um so we started building&nbsp; it in Canva uh for reals um because we after we&nbsp;&nbsp; got really confident about it. We did um we hooked&nbsp; up to the camera code base. We handle things like&nbsp;&nbsp; responsiveness um nine slicing and editing and&nbsp; camera design. But we also continue to iterate&nbsp;&nbsp; um um our prototype and basically that was&nbsp; essentially the next version where we did things&nbsp;&nbsp; like um div based support which is um coming&nbsp; very very soon. And um we that that's also when&nbsp;&nbsp; we tested this experience across all the different&nbsp; canvas surfaces uh can code can end up in like you&nbsp;&nbsp; know whether it's in the editor whether it's on&nbsp; mobile or whether you're on a presentation mode.&nbsp;&nbsp; We actually even printed out a t-shirt um with&nbsp; a can code um just to make sure that it actually&nbsp;&nbsp; worked which was um pretty fun. Like obviously&nbsp; it wasn't interactive. It was like more of just&nbsp;&nbsp; a static screenshot but you can print it on a mug&nbsp; or a t-shirt. Um and the way it works is that we&nbsp;&nbsp; put kind of code creations um in iframe since it&nbsp; is really arbitrary um arbitrary user generated&nbsp;&nbsp; content um and we just use a similar approach&nbsp; to how we support um embeds in hammer designs.&nbsp;&nbsp; um when it comes to choosing the model and why&nbsp; we ended up with um right now with Claude's&nbsp;&nbsp; um Claude's um sonet models um this is this is a&nbsp; definitely a really really fun question to explore&nbsp;&nbsp; and the first thought you all probably have is you&nbsp; know evals evals and evals and that's definitely&nbsp;&nbsp; really important especially for correctness but&nbsp; when we looked at um the model choice for camera&nbsp;&nbsp; code we had a few additional considerations as&nbsp; well we really value how magical it is and you&nbsp;&nbsp; really like the delightfulness as we like to call&nbsp; it. And when we tested across real user requests,&nbsp;&nbsp; the ability for models to handle very short and&nbsp; very underspecified user prompts, things like,&nbsp;&nbsp; hey, can you like make me can you make me a&nbsp; game that does blah or hey, can you make me like&nbsp;&nbsp; um a menu planner where the user isn't really&nbsp; giving you like very very specific specific&nbsp;&nbsp; requirements and the LM just has to infer&nbsp; it. That was really really important to&nbsp;&nbsp; us. Um the model's ability to make amazing web&nbsp; designs was also super important. We are we're&nbsp;&nbsp; a visual communication platform after all.&nbsp; And finally of course um things like latency&nbsp;&nbsp; as well as scalability and capacity were also&nbsp; very very real very very important factors.&nbsp;&nbsp; So back around um yeah back around March we tested&nbsp; um a series of tested a lot of different models um&nbsp;&nbsp; across not just not just whether it generated&nbsp; valid um valid code but also how subjectively&nbsp;&nbsp; how good was our web design um at following the&nbsp; instructions we have in the prompt around things&nbsp;&nbsp; like UI UX the information hierarchy um all that&nbsp; kind of stuff um how good is how good is at things&nbsp;&nbsp; like SVG generations which is something that we're&nbsp; like always really impressed test by with um CL&nbsp;&nbsp; family of models things like animations like um&nbsp; it's able to set things like C index correctly&nbsp;&nbsp; it's able to you know do colors do timings really&nbsp; really well and we were just really delighted with&nbsp;&nbsp; um we were already really delighted with Sonet&nbsp; 3.5 V2 uh 3.7 was a huge huge step up and&nbsp;&nbsp; um with today's announcement of clot 4&nbsp; like that's something that's really a&nbsp;&nbsp; transformational leap in intelligence for coding&nbsp; and we're also super excited about that as Well,&nbsp;&nbsp; but I guess like my takeaway here is like&nbsp; um really think about especially when you're&nbsp;&nbsp; launching to users like think about the complete&nbsp; user experience and what they care about. Don't&nbsp;&nbsp; don't be super academical and just focus um just&nbsp; focus on you know things like how good it does&nbsp;&nbsp; at benchmarks or how good it does at um you know&nbsp; like large code base or really big requests like&nbsp;&nbsp; um does it actually do the things like generating&nbsp; icons and SVGs really well too because those are&nbsp;&nbsp; those are really important to all of our users.&nbsp; And finally um three lessons we've learned. The&nbsp;&nbsp; first one, and this is a pretty interesting one&nbsp; to us, was just how important it is to actually&nbsp;&nbsp; communicate AI's limitations in all the different&nbsp; places where there are AI limitations. So, one of&nbsp;&nbsp; the product principles that we have at Canva is&nbsp; that everything should just work. It should just&nbsp;&nbsp; do what it says um on the tin. And when when&nbsp; coding and building interfaces with LLMs like&nbsp;&nbsp; um generative yeah LMS can be uh can make very&nbsp; beautiful things but they might not actually&nbsp;&nbsp; work. And so when when we did some in real life&nbsp; user testing sessions um we had users like make&nbsp;&nbsp; things like portfolio pages um just kind of uh&nbsp; galleries and things like that. that they included&nbsp;&nbsp; contact forms but since chemical code right now&nbsp; is uh just front end only um those contact forms&nbsp;&nbsp; didn't work and users thought it was broken um&nbsp; they don't really understand like you know it's&nbsp;&nbsp; just front end only like you needs destination&nbsp; needs to hook it up to um you know like an email&nbsp;&nbsp; sender and things like that. So one of the things&nbsp; we did is we made sure to prompt the prompt the&nbsp;&nbsp; model to actually very visibly communicate things&nbsp; that wouldn't work so that they don't so that&nbsp;&nbsp; they at least understand that it's the limitations&nbsp; and that it's intentional and it's not them doing&nbsp;&nbsp; something wrong. Uh the second lesson is uh we&nbsp; are absolutely in love with the power of building&nbsp;&nbsp; functional prototypes. Uh so if you don't know um&nbsp; so we have a mon repo at Canva with um just about&nbsp;&nbsp; half a million commits over the past decade.&nbsp; We support more than 100 languages um um many&nbsp;&nbsp; different platforms. There's also there's also&nbsp; global laws and regulations in all environments&nbsp;&nbsp; we operate. We have enterprise customers with a&nbsp; lot of contracts SLAs and all that kind of stuff.&nbsp;&nbsp; So building things like um in canvas repository&nbsp; um it enables to scale and serve all of our&nbsp;&nbsp; customers but it's not really the fastest place&nbsp; to be experimenting and testing new concepts&nbsp;&nbsp; especially in the fast moving world of AI. So by&nbsp; building and by starting to build and um iterate&nbsp;&nbsp; and develop and test and walking prototypes we&nbsp; could move so much more faster and dream heaps&nbsp;&nbsp; and heaps of ahead. What we found really useful&nbsp; was that our team was really actively using these&nbsp;&nbsp; prototypes dayto-day for real work and real tasks&nbsp; and that really gave us a lot of learnings and&nbsp;&nbsp; understanding. Um the live user testing helps&nbsp; identify issues, bugs and really inform our&nbsp;&nbsp; road map before we even started actually building&nbsp; building it for real. And um it's kind of it was&nbsp;&nbsp; kind of entirely closing the loop. We've built&nbsp; it um very very heavily with um with claw models&nbsp;&nbsp; um and just allowed us to move just how to make&nbsp; make magic. Uh the last lesson I want to share&nbsp;&nbsp; is like the importance of really differentiating&nbsp; and playing to your strengths. So in this AI world&nbsp;&nbsp; like there's so many products um and the barrier&nbsp; to entry like especially with things like cloud&nbsp;&nbsp; code and AI coding can be really really low. So&nbsp; it's really important to be thinking about what&nbsp;&nbsp; are your unique strengths? What are our unique&nbsp; strengths in this market and the product that&nbsp;&nbsp; we're building when it comes to Canva code um is&nbsp; two things but the first one the most important&nbsp;&nbsp; is really we're explicitly not building Canva&nbsp; code for technical users or engineers. Right now,&nbsp;&nbsp; we're starting bottoms up with nontechnical&nbsp; users because we want to basically bring the&nbsp;&nbsp; superpower to just about everyone to the&nbsp; billions of internet users out there and&nbsp;&nbsp; then move bottoms up and get to get to higher&nbsp; and higher levels of sophistication. So for us,&nbsp;&nbsp; things like how easy it is to start, um how guided&nbsp; it can be and how well how well it turns people's&nbsp;&nbsp; dreams or screenshots of mocks in reality is&nbsp; really important as we move up and up into more&nbsp;&nbsp; advanced functionality. And I think this a really&nbsp; important thing to think about. You might not not&nbsp;&nbsp; necessarily start bottoms up, but definitely have&nbsp; a focus for what you target and leveraging your&nbsp;&nbsp; unique strengths and what you what you what you&nbsp; already have and what your users already use is&nbsp;&nbsp; really important. So here's a preview for example&nbsp; like we are working on integrating the camera code&nbsp;&nbsp; creations with canvas editor. You can do things&nbsp; like change the colors using our color picker,&nbsp;&nbsp; apply your brand fonts or drag and drop your&nbsp; photos um from your asset library and get what&nbsp;&nbsp; you see is what you get editing experience. And&nbsp; of course, we're we're spiking this um on our&nbsp;&nbsp; interactive um prototype. finding and figuring out&nbsp; just what's the best way to leverage your unique&nbsp;&nbsp; strengths is definitely really really important&nbsp; um for not just for building something that offers&nbsp;&nbsp; value to your users that other people can't. And&nbsp; we're just really we're just so excited about this&nbsp;&nbsp; era that we're now in and all the possibilities&nbsp; that we can unlock. So yeah, that is um that's&nbsp;&nbsp; everything I have. Um hope you um we do have time&nbsp; for some Q&amp;A if you want to if you want to chat&nbsp;&nbsp; and I think there's microphones up there and there&nbsp; the hope you thanks for having me and I hope you&nbsp;&nbsp; found the session useful. [Applause] Thanks so&nbsp; much. Could you share some tips on how you get&nbsp;&nbsp; the models to produce such beautiful front ends?&nbsp; Oh yeah, absolutely. So we had um we had a lot of&nbsp;&nbsp; our designers actually just um really experiment a&nbsp; lot like um really just trip engineering but some&nbsp;&nbsp; examples are really simple like one is we instruct&nbsp; claude to you know never have like a white gray or&nbsp;&nbsp; black background unless it's requested to always&nbsp; pick you know say a nice color another one is&nbsp;&nbsp; we've incorporated things like just basically&nbsp; our product design principles like you know&nbsp;&nbsp; like like we have just a set of guidelines on how&nbsp; information hierarchy should be where controls and&nbsp;&nbsp; where buttons should be um and we just basically&nbsp; summarize and put that in there as um as defaults&nbsp;&nbsp; it tries to follow. So yeah, everything's more or&nbsp; less like just in the prompt. Thank you. Hello. Um&nbsp;&nbsp; I'm interested in what is your perspective on like&nbsp; the future of AI and design more broadly and how&nbsp;&nbsp; should people who design be thinking about that&nbsp; is a great AI. That's a really great question.&nbsp;&nbsp; Like what we what we're really excited about is&nbsp; how how AI like can unlock and supercharge human&nbsp;&nbsp; creativity. Like um this technology is allowing&nbsp; people who normally like you know you would&nbsp;&nbsp; normally take like say weeks or days um to get to&nbsp; some output like you can now use AI to to firstly&nbsp;&nbsp; like clean up all your tedious to not do all your&nbsp; tedious tasks but it also allows you to dream and&nbsp;&nbsp; prototype. I think a good example is when we were&nbsp; planning our event um kind of create event like um&nbsp;&nbsp; our creative and motion teams were using diffusion&nbsp; models to really quickly make story boards for&nbsp;&nbsp; different talks and different activations and&nbsp; experiences. Um and that allowed us to to just&nbsp;&nbsp; like get an impression of those ideas, get some&nbsp; inspiration and um make the actual make the actual&nbsp;&nbsp; creative process like basically give superpowers&nbsp; to the creative process. I think like what we're&nbsp;&nbsp; really excited about is just how much like um how&nbsp; much air can actually help like um exponentially&nbsp;&nbsp; increase human creativity. Cool. Well, thank&nbsp; you. Thank you all so much for having me. Bye. [Music]

---

## 10. Spotlight on Databricks

**Preview:**
> Kind: captions Language: en Hey, thank you for joining. Thank you for sticking&nbsp; around in here. I suppose it' probably be the most&nbsp;&nbsp; apppropo thing to say. Uh, thank you for joining&nbsp; me today. They wanted to talk a little bit about&nbsp;&nbsp; how all of this technology actually gets into the&nbsp; path of value inside large organizations and large&nbsp;&nbsp; businesses because as it would turn out the the&nbsp; ability for us to go prototype cool stuff versus&nbsp;&nbsp; us...

**Full Transcript:**

Kind: captions Language: en Hey, thank you for joining. Thank you for sticking&nbsp; around in here. I suppose it' probably be the most&nbsp;&nbsp; apppropo thing to say. Uh, thank you for joining&nbsp; me today. They wanted to talk a little bit about&nbsp;&nbsp; how all of this technology actually gets into the&nbsp; path of value inside large organizations and large&nbsp;&nbsp; businesses because as it would turn out the the&nbsp; ability for us to go prototype cool stuff versus&nbsp;&nbsp; us go and deliver these things into the critical&nbsp; path can can vary widely. I'm Craig. I lead&nbsp;&nbsp; product management for data bricks in case you&nbsp; hadn't figured that yet. Uh been with data bicks&nbsp;&nbsp; for about three years. before that was at Google&nbsp; where I was the leader of product for uh the&nbsp;&nbsp; founding of Vert.ex AI and before that I was the&nbsp; the founding general manager of of AWS SageMaker.&nbsp;&nbsp; So I've been I as my wife says uh continuing to&nbsp; strike out as I try and get better and better at&nbsp;&nbsp; helping enterprises uh build AI. But uh as we dive&nbsp; into this I wanted to quickly just set a little&nbsp;&nbsp; bit of context on who data bricks, why data bricks&nbsp; and why is data bicks here talking to you and&nbsp;&nbsp; what have you. Right? We are a uh a leading data&nbsp; platform, multicloud or crosscloud data platform,&nbsp;&nbsp; uh tens of thousands of customers, you know, uh&nbsp; billions of dollars in revenue and and moreover&nbsp;&nbsp; the creator of a number of open-source very&nbsp; popular open-source capabilities, Spark,&nbsp;&nbsp; uh MLflow, Delta, etc. Um, you know, we live in&nbsp; a world Brad just a minute ago, he talked about&nbsp;&nbsp; the importance of the model and then the data you&nbsp; bring to the model. And the enterprises we work&nbsp;&nbsp; with have a kind of nightmarish data scenario&nbsp; because you know you talk to these large bank&nbsp;&nbsp; multinational banks or something like that and&nbsp; they've they've done dozens if not uh scores of of&nbsp;&nbsp; of acquisitions over the years and they have data&nbsp; on every cloud in every possible vendor in every&nbsp;&nbsp; possible service and they're trying at this moment&nbsp; to figure out how to take advantage of this kind&nbsp;&nbsp; of transformational techn technological moment,&nbsp; but they're doing it with kind of a mess in the&nbsp;&nbsp; back end, if you will, right? And it turns out the&nbsp; problem is actually much worse than this because&nbsp;&nbsp; it's not like they just have one data warehouse or&nbsp; something like that. They often have many of them,&nbsp;&nbsp; right? And and often the the experts in one or&nbsp; two of these systems are only experts in one&nbsp;&nbsp; or two of these systems and they don't know the&nbsp; other system. So if if you're stuck and your data&nbsp;&nbsp; warehouse or your streaming person isn't a Gen&nbsp; AI person, you may find yourself kind of locked&nbsp;&nbsp; out of of being able to bring your data into into&nbsp; these systems as easily as you want to. Now I'm&nbsp;&nbsp; not going to go head on into data bricks. Data&nbsp; bricks ultimately we help you manage your data&nbsp;&nbsp; and then on top of that management of your data we&nbsp; have a whole series of capabilities and going to&nbsp;&nbsp; really focus on our AI capabilities with Mosaic&nbsp; AI today. Now we think of this as a difference&nbsp;&nbsp; between what we call general intelligence&nbsp; and data intelligence. Both of these things&nbsp;&nbsp; are extraordinarily useful and extraordinarily&nbsp; important. But as Brad talked about, particularly&nbsp;&nbsp; for for businesses or large enterprises, as&nbsp; they want to move into using this technology&nbsp;&nbsp; to automate more of their systems or drive greater&nbsp; insights within their organization, almost always&nbsp;&nbsp; it comes back to connecting it. We saw here&nbsp; Brad connecting it to the web or connecting&nbsp;&nbsp; it to MCP servers, but inevitably it comes back to&nbsp; trying to connect it to their data estate, right?&nbsp;&nbsp; So for a really good example of this uh Faxet, I&nbsp; don't know if you guys have heard of Faxet. Faxet&nbsp;&nbsp; is a financial services company that sells uh data&nbsp; about other companies. They sell financial data&nbsp;&nbsp; about companies to banks and hedge funds and what&nbsp; have you. Uh Faxet has their own query language,&nbsp;&nbsp; which is now a yellow flag to me when considering&nbsp; employers. If your employer has their own query&nbsp;&nbsp; language, you got to think about whether or not&nbsp; this is the right place to be. Having said that,&nbsp;&nbsp; I did work at Google who I think probably&nbsp; has a dozen of their own query languages. But&nbsp;&nbsp; uh so Faxet had this this problem and opportunity,&nbsp; which is that any customer they had who wanted to&nbsp;&nbsp; access their data, they had to learn FQL, Faxet&nbsp; query language, creative name in there. Uh,&nbsp;&nbsp; and so when this whole Genai craze started, these&nbsp; guys lost their minds with excitement because&nbsp;&nbsp; they thought, what if we could translate English&nbsp; into factet query language? And so they went to&nbsp;&nbsp; their favorite cloud of choice. They hit one the&nbsp; one-click rag button. I think they did a little&nbsp;&nbsp; more than the one-click rag button, but they&nbsp; basically showed up with this massive prompt of&nbsp;&nbsp; a bunch of examples and a bunch of documentation&nbsp; and then a massive vector DB of a bunch of prompts&nbsp;&nbsp; and a bunch of documentation or a bunch of uh&nbsp; examples and a bunch of documentation. And this&nbsp;&nbsp; is what they ended up with, right? They ended up&nbsp; with 59% accuracy in about 15 seconds of latency.&nbsp;&nbsp; And I share with you that latency metric not just&nbsp; because it's an important customer experience&nbsp;&nbsp; metric and all of these kinds of things. But in&nbsp; this world of Genai, it's probably the closest&nbsp;&nbsp; thing we have to a cost metric, right? You're more&nbsp; or less paying for compute time. And so that 15&nbsp;&nbsp; seconds is basically 15 seconds of cost, right?&nbsp; And 59% accuracy. With this, they they showed up.&nbsp;&nbsp; They they contacted us and said, "Hey, good news.&nbsp; We've got a Jedi solution. Bad news. it's just&nbsp;&nbsp; slightly better than a coin flip kind of thing,&nbsp; right? And so we worked with them on this problem&nbsp;&nbsp; and tried to understand what what the opportunity&nbsp; was, what the challenge was. And really what we&nbsp;&nbsp; did was we just decomposed the prompt into each&nbsp; of the individual tasks that that prompt was&nbsp;&nbsp; being asked to use. Right? So effectively what we&nbsp; did was we took that prompt and created kind of&nbsp;&nbsp; something of an agent of a multi-node a multi-step&nbsp; chain or process to be able to solve this problem&nbsp;&nbsp; more wholly. And really the reason we did that&nbsp; was because it allowed us the opportunity to start&nbsp;&nbsp; tuning performance at each step of this problem.&nbsp; Right? And you can see we got them to 85% accuracy&nbsp;&nbsp; in six seconds of latency. at 85% accuracy. They&nbsp; they did two things. They turned to us and they&nbsp;&nbsp; said, "Cool. We're comfortable showing this to&nbsp; our existing customers and they said, "We get&nbsp;&nbsp; how you're helping us. We don't want to pay you&nbsp; to help us anymore. We'll take it from here." Uh,&nbsp;&nbsp; last I talked to them, they had it into the '9s,&nbsp; and last I talked to them, uh, transitioning to&nbsp;&nbsp; Claude was one of their next roadmap items.&nbsp; Um, the reason I say all of this is because&nbsp;&nbsp; there's a paper out there uh from the Berkeley&nbsp; artificial intelligence research uh lab which&nbsp;&nbsp; uh if you look into it, yes, there's a little&nbsp; bit of uh cross-pollination between us and and uh&nbsp;&nbsp; Berkeley, but basically the folks at Berkeley did&nbsp; a right after Genai kind of really hit its stride,&nbsp;&nbsp; they went out and they looked at all the popular&nbsp; AI systems that are out in production today. And&nbsp;&nbsp; what they found was that none of these systems&nbsp; were as easy as kind of a a a a single input and a&nbsp;&nbsp; single output kind of basic system. These systems&nbsp; were all kind of very complex multi-node multi- uh&nbsp;&nbsp; kind of multi-art systems that were being chained&nbsp; together to create really fantastic outcomes.&nbsp;&nbsp; So our goal at data bricks is really to simplify&nbsp; the creation of these kinds of capabilities for&nbsp;&nbsp; our customers but very specifically we want to&nbsp; do it on the areas where there is financial and&nbsp;&nbsp; reputational risk. If what you're wanting to do&nbsp; is build a chatbot for you and your buddies to&nbsp;&nbsp; kind of search over your your uh you know your&nbsp; documents or your you know your emails or what&nbsp;&nbsp; have you your recent PRDs in my case. uh great,&nbsp; go for it. One click, one click rag away at that&nbsp;&nbsp; thing kind of or or prompt away at that thing. But&nbsp; if what you want to do is build something that you&nbsp;&nbsp; trust putting into a situation of financial or&nbsp; reputational risk, then it takes some additional&nbsp;&nbsp; capabilities. And not only that, but what one&nbsp; of the things we see, and I'm sure you've seen&nbsp;&nbsp; this as well, is that many of the folks out there&nbsp; who are developing these systems, they're trying&nbsp;&nbsp; to develop deterministic systems using the most&nbsp; probabilistic portion of their entire software&nbsp;&nbsp; stack, right? And so one of the pieces of this&nbsp; is how do we help them drive those levels of&nbsp;&nbsp; consistently drive those levels of repeatable&nbsp; determinism? And we think it comes down to two&nbsp;&nbsp; things. All else being equal, we think it comes&nbsp; down to governance. Making sure you can control&nbsp;&nbsp; at the tightest levels, at the lowest grain,&nbsp; what this thing has access to and can do. And&nbsp;&nbsp; then evaluation. I was super excited. I met with&nbsp; with a company this morning, a global logistics&nbsp;&nbsp; provider this morning, and it was one of the first&nbsp; times I had met with a customer who said, "Hey,&nbsp;&nbsp; we built this system, and it's like 85% accurate."&nbsp; And it was such a joy because usually people say,&nbsp;&nbsp; "Hey, we built the system. We have it in&nbsp; production. We're super proud of it." And I say,&nbsp;&nbsp; "How accurate is it?" And they go, "Oh, it's&nbsp; pretty good, right?" And so being able to really&nbsp;&nbsp; start to quantify and hill climb that we believe&nbsp; is critical. So governance, what are we talking&nbsp;&nbsp; about? We're talking about really governing the&nbsp; access, treating these agents or these prototype&nbsp;&nbsp; agents we're building as principles within our&nbsp; data stack and governing every single aspect of&nbsp;&nbsp; that. Now on data bricks, we don't just govern&nbsp; your data. We also govern access to the models,&nbsp;&nbsp; right? And we govern tools, right? And we&nbsp; govern queries. So we govern access to the data,&nbsp;&nbsp; we govern access to the models, we govern access&nbsp; all of the pieces. There is one piece we don't&nbsp;&nbsp; yet govern yet is MCP servers. But uh stick with&nbsp; us. We have a conference in a few weeks. You might&nbsp;&nbsp; come check it out and uh hopefully we'll have news&nbsp; for you there. Um so how do we get all of this to&nbsp;&nbsp; reason over your data? And uh you know we do that&nbsp; by injecting it with either the vector store or or&nbsp;&nbsp; the feature store. Uh, and then we, as I said, we&nbsp; govern all of the aspects, whether it's the data,&nbsp;&nbsp; the models, the tools. And I want to stop for&nbsp; a second and talk about tools and tool calling&nbsp;&nbsp; because we saw some of it just a second ago in&nbsp; Brad's demos. And tool calling when it comes to&nbsp;&nbsp; trying to build a deterministic system. Usually&nbsp; what we actually see is we see someone building&nbsp;&nbsp; uh using an LLM to as a classifier to choose one&nbsp; of six or eight paths, right? One of six or eight&nbsp;&nbsp; tools. And those tools may be agents, those&nbsp; tools may be SQL queries, those tools are any&nbsp;&nbsp; sort of parameterizable function kind of thing,&nbsp; right? So we see them creating access to these&nbsp;&nbsp; tools. And then what do we see? We often see the&nbsp; next layer another set of to of agents calling&nbsp;&nbsp; choosing between a set of tools and and so they&nbsp; end up with this massive decision tree which is&nbsp;&nbsp; great from a kind of deterministic perspective on&nbsp; really d reducing the entropy in these systems.&nbsp;&nbsp; The challenge for us was that before we had&nbsp; this relationship with anthropic we were&nbsp;&nbsp; talking to people about this stuff but the tool&nbsp; calling just wasn't where it was needed to be.&nbsp;&nbsp; You would have these moments where it would be&nbsp; unbelievably obvious what tool should be called&nbsp;&nbsp; and the models would consistently not necessar&nbsp; well would consistently not get it right. Uh with&nbsp;&nbsp; with claude that has changed completely. Right. We&nbsp; now see the ability for these systems to do tool&nbsp;&nbsp; calling really becomes the way in which software&nbsp; development engineers and and app app engineers&nbsp;&nbsp; can start building these quasi deterministic&nbsp; systems using a highly probabilistic backend.&nbsp;&nbsp; Cloud really in many ways completes this puzzle&nbsp; for us by giving us that frontier LLM available&nbsp;&nbsp; directly inside data bricks that has all of the&nbsp; capabilities needed to really superpower the use&nbsp;&nbsp; cases that our customers are putting together. So&nbsp; why cloud and data bricks together? First of all,&nbsp;&nbsp; cloud is natively available on data bicks in on&nbsp; any cloud, right? So on Azure, on AWS, on GCP,&nbsp;&nbsp; you can call cloud within your databicks instance.&nbsp; You can build uh state-of-the-art agents on data&nbsp;&nbsp; bricks powered by cloud and then fundamentally you&nbsp; can connect cloud. You know the vast majority of&nbsp;&nbsp; folks who are using data bricks are are much&nbsp; lower level data engineers and what have you&nbsp;&nbsp; building out kind of massive schemas and building&nbsp; out massive governance policies systems and what&nbsp;&nbsp; have you. And you can use claude as a principle&nbsp; within that system, right? And as you can see,&nbsp;&nbsp; uh, including MCP servers coming soon.&nbsp; Uh, so why use it with us, right? Well,&nbsp;&nbsp; it really comes down to really pairing the&nbsp; strongest model with the strongest platform,&nbsp;&nbsp; using it in a fully controlled, right? You know,&nbsp; when you talk to these companies, I was sitting&nbsp;&nbsp; at a a collection of of banks recently. There were&nbsp; 10 or 12 banks at the table. We were all talking&nbsp;&nbsp; about what they were working on. I think more than&nbsp; half the banks in the room were were prototyping&nbsp;&nbsp; on Claude as we spoke. Uh one of the banks did&nbsp; raise their hand and said, "We're not allowed to&nbsp;&nbsp; use any of this generative stuff in this industry&nbsp; of which he was laughed at by the others in the&nbsp;&nbsp; room who were working on these things." uh and and&nbsp; the real difference was what that guy was saying&nbsp;&nbsp; was hey we don't have the controls in place to&nbsp; use this within our own organization whereas the&nbsp;&nbsp; banks the hospitals the other highly governed&nbsp; highly regulated areas that have gone through&nbsp;&nbsp; this now have full access to this technology and&nbsp; no longer need to wait for the technology to kind&nbsp;&nbsp; of come to them right you can uh commercially&nbsp; there's some advantages scale and and operational&nbsp;&nbsp; capabilities really add to the reasons for why&nbsp; to use it all together. Now we together we enable&nbsp;&nbsp; these really high value use cases and one of the&nbsp; great things about sitting at the intersection&nbsp;&nbsp; of Gen AI and enterprise is getting to see these&nbsp; highv value use cases that kind of come through&nbsp;&nbsp; and really you know give us the the confidence&nbsp; to see that this technology is not going to be&nbsp;&nbsp; uh another uh kind of three-year flash in the pan&nbsp; but is is really going to end up changing ing the&nbsp;&nbsp; way we all work and and we can now see that coming&nbsp; to fruition in some of these organizations we're&nbsp;&nbsp; working with. Now I had said at the beginning&nbsp; that this comes down to governance and evaluation,&nbsp;&nbsp; right? And for us, one is not complete without&nbsp; the other. You can lock these things down. You&nbsp;&nbsp; can control what they have access to. You can&nbsp; control how they're going to operate within&nbsp;&nbsp; your data estate. But if you're not measuring&nbsp; the quality of the system, then you're really&nbsp;&nbsp; not going to know whether or not this this system&nbsp; you've built is high enough quality to be able to&nbsp;&nbsp; start putting in to those higher risk use cases&nbsp; without necessarily a human approval in the loop&nbsp;&nbsp; at every step. Right? And so that's where eval&nbsp; comes in. This is our eval platform. By the way,&nbsp;&nbsp; you bring in a golden data set. We have a series&nbsp; of LLM judges that help determine whether or not&nbsp;&nbsp; your performance is what it needs to be. And&nbsp; uh and you can use this. By the way, this whole&nbsp;&nbsp; system has a secondary UI for your subject matter&nbsp; expert. Time and again, we see the app developers&nbsp;&nbsp; building these systems are not necessarily the&nbsp; subject matter experts on these topics. And so&nbsp;&nbsp; having a simplified UI for that subject matter&nbsp; expert to be able to kind of quickly and easily&nbsp;&nbsp; give uh context or or correct a correct a prompt&nbsp; or or create a better answer is critical. This is&nbsp;&nbsp; how we start down this path of gaining confidence&nbsp; that these systems can perform in robust higher&nbsp;&nbsp; risk situations is by really kind of you know I I&nbsp; had a a guy uh the other day who who said you know&nbsp;&nbsp; oh you're just unit testing the agent and I I kind&nbsp; of said well I like to think it's more clever than&nbsp;&nbsp; that but yeah you know more or less right you know&nbsp; really kind of searching across the the question&nbsp;&nbsp; space or that that is expected to be uh uh kind&nbsp; of gone after with this system and then diving&nbsp;&nbsp; in at the most granular levels to ensure that&nbsp; this system is performing. Now this eval system&nbsp;&nbsp; I should say uh a lot of it is open source in ML&nbsp; flow the uh LLM judges are not but a lot of the&nbsp;&nbsp; capabilities here can be run whether or not you're&nbsp; using data bricks or not you can use o open source&nbsp;&nbsp; ML flow to do these evals or you can if you're&nbsp; using data bricks you can hook it up and gain all&nbsp;&nbsp; the value of some of our custom judges and what&nbsp; have you right um so that is kind of the stack&nbsp;&nbsp; and that's how we're helping organizations bring&nbsp; Gen AI and particularly bringing Claude into this&nbsp;&nbsp; space. Now before we wrap up though, I wanted to&nbsp; share uh you know there are these these analysts&nbsp;&nbsp; out there Gartner and Forester and all these they&nbsp; go around and they they write report cards on how&nbsp;&nbsp; good is every how good are all the the vendors,&nbsp; right? Which vendors are the leaders and what&nbsp;&nbsp; have you. We do pretty well in these. Uh, but I'm&nbsp; really excited to say that we're now using Arya to&nbsp;&nbsp; do these. So, to give you a sense, the last time&nbsp; we filled out the Gartner one of these things,&nbsp;&nbsp; we ended up writing a 450 page document, right?&nbsp; They had 180 questions for us and it we ended up&nbsp;&nbsp; passing back to them a 450 page document. So using&nbsp; claude we actually have taken a whole our our&nbsp;&nbsp; uh doc our blogs our docs uh a whole bunch&nbsp; of the information about our system as well&nbsp;&nbsp; as past answers we've written for these types&nbsp; of things and we've actually gotten it so that&nbsp;&nbsp; when Gartner or Forester or what have you send us&nbsp; these questionnaires we just run them through the&nbsp;&nbsp; bot and you know I'll say the answers. We still&nbsp; read the answers over and we correct some of them&nbsp;&nbsp; some of the time, but the ability for us to do&nbsp; this has made it so that now instead of it being&nbsp;&nbsp; kind of hundreds of hours of product managers and&nbsp; engineers and marketing folks all kind of pounding&nbsp;&nbsp; on the keyboard to try and put something together.&nbsp; Now, we're just editing what I what I wouldn't&nbsp;&nbsp; even call a rough draft. But we're editing what's&nbsp; pretty darn close to a final draft coming out of&nbsp;&nbsp; Claude. And and the reason why I have this up here&nbsp; is because we built this uh this went through many&nbsp;&nbsp; iterations. We started with open source models.&nbsp; Then we went to uh non-anthropic models uh of&nbsp;&nbsp; a different vendor and then we started using&nbsp; claude. And it wasn't until we started using&nbsp;&nbsp; Claude that the results were good enough that we&nbsp; it was it was when we started using Claude that&nbsp;&nbsp; we for the first time had results that we could&nbsp; ship without touching them. And that was a huge&nbsp;&nbsp; win for us. Uh and so it's a a really exciting&nbsp; This is one of these that I'm super excited&nbsp;&nbsp; about because it makes my life way better. Uh we&nbsp; uh we just published a blog on this. Uh really&nbsp;&nbsp; exciting stuff. if you if you have to spend your&nbsp; days filling out these darn questionnaires. Um,&nbsp;&nbsp; Block is also a customer of ours and Block has&nbsp; built this open source system called Goose. And&nbsp;&nbsp; if you haven't given Goose a try, you should&nbsp; take a look. Uh, as I said, it's open source.&nbsp;&nbsp; It's a it's really a dev environment, an agentic&nbsp; dev environment to accelerate their developers&nbsp;&nbsp; to be able to build, you know, it has basically&nbsp; claude built into it and it has connections to&nbsp;&nbsp; all of their systems and all of their data so that&nbsp; they can much much more quickly and easily build&nbsp;&nbsp; uh you know kind of within and really accelerate&nbsp; the developer experience far far beyond kind&nbsp;&nbsp; of what we're all used to with code complete or&nbsp; something like that into a much more purpose-built&nbsp;&nbsp; system to be able to go attack uh improvement&nbsp; of their workflows and things like this. You&nbsp;&nbsp; can see 40 to 50% weekly user adoption increase 8&nbsp; to 10 hours saved per week by using this. And it's&nbsp;&nbsp; uh it's been really exciting to see block be this&nbsp; successful uh with data bricks on or with cloud&nbsp;&nbsp; on data bricks as well as to see uh goose start to&nbsp; pick up in the market and more and more people uh&nbsp;&nbsp; playing around and starting to try out goose. So&nbsp; those are just a couple of the areas where we've&nbsp;&nbsp; had success getting uh getting these these models&nbsp; and these systems in production and creating value&nbsp;&nbsp; for customers. So, I I'll just end it with, you&nbsp; know, uh I'm sure everyone here is is deep enough&nbsp;&nbsp; in this that I don't need to tell you to start&nbsp; identifying your AI use cases, but once you've&nbsp;&nbsp; identified those AI use cases and you've started&nbsp; to understand what success may look like, you&nbsp;&nbsp; know, contact us, reach out to data bricks, reach&nbsp; out to anthropic, happy to work with you either,&nbsp;&nbsp; uh, you know, kind of in your professional&nbsp; capacity with the organizations you work for&nbsp;&nbsp; and and really help them gain the confidence I&nbsp; the the meeting I was in earlier today. Uh as&nbsp;&nbsp; I was walking out of the meeting, the head&nbsp; of AI came running over to me and he said,&nbsp;&nbsp; "Hey, I really appreciate the session&nbsp; today." And I said, "No, no worries. Like,&nbsp;&nbsp; happy to happy to present. Happy to chat with&nbsp; you about what we're doing." He goes, "No,&nbsp;&nbsp; no, no, no. It wasn't learning about your stuff&nbsp; that I appreciated. It was you telling our chief&nbsp;&nbsp; data officer how hard my job is that I really&nbsp; appreciated." Right? And so, let us know how we&nbsp;&nbsp; can help you uh in this in this journey. With&nbsp; that, I wanted to open it up to any questions. Uh, no, no, no. So, the question was,&nbsp; is the is the safe score among our LLM&nbsp;&nbsp; judges uh kind of using a red teaming or a&nbsp; a kind of adversarial technique or something&nbsp;&nbsp; like that? No. It's much more of a kind of&nbsp; think of it more as like a guardrail type&nbsp;&nbsp; measure around you know uh was this response&nbsp; a green response or a a comfortable response&nbsp;&nbsp; kind of thing. Any other questions?&nbsp; Yeah. Would you think like Minecraft? Yeah. I mean uh you know some of the folks over&nbsp; you know it's tough. There's um we have a bunch&nbsp;&nbsp; of competitors for for point solutions within the&nbsp; genai space right you know eval you could say you&nbsp;&nbsp; know might be Galileo it might be you know uh&nbsp; patronis it might be others kind of thing right&nbsp;&nbsp; uh you know and so there's some point specific&nbsp; folks um I think the way we think about this is&nbsp;&nbsp; much more that the value comes in the connection&nbsp; between the AI system and the data system like&nbsp;&nbsp; having worked at both AWS and GCP I can say like&nbsp; uh the reason I'm at data bricks is because there&nbsp;&nbsp; was a conversation I had while at while at Vertex&nbsp; where we were sitting there saying hey with MLOps&nbsp;&nbsp; we had taken an order of magnitude off the&nbsp; development time where does the next order&nbsp;&nbsp; of magnitude off development time came from&nbsp; and it really I believe comes from being able&nbsp;&nbsp; to really integrate the AI and the data layers&nbsp; together much much more intimately and deeply&nbsp;&nbsp; than we've seen from most of the hyperscalers.&nbsp; Any other last questions? Yeah. In one of your&nbsp;&nbsp; earlier slides, the customer. Yeah, it&nbsp; looks like they have many agents. Yeah. Yeah. I I mean that's certainly, you know, we've&nbsp; um we often work we often encourage companies to&nbsp;&nbsp; take a more kind of composable agentic approach.&nbsp; And we often encourage them to do that simply&nbsp;&nbsp; because when you're trying to build these systems&nbsp; to behave deterministically uh in a higher risk&nbsp;&nbsp; environment, uh then you need to be able to&nbsp; tune them at a much more granular level. And so,&nbsp;&nbsp; you know, our goal is really to drive as much&nbsp; entropy out of these systems as possible in&nbsp;&nbsp; trying to get this determinism. And so, you know,&nbsp; yes, uh I think three, you know, 37 I I haven't&nbsp;&nbsp; gotten to play with four nearly enough yet, but&nbsp; I think 37 uh probably could do a lot of that,&nbsp;&nbsp; but I I I guess my only concern would be, uh if&nbsp; we did find errors, would we have the knobs to&nbsp;&nbsp; be able to go and get them beyond just swapping&nbsp; up the prompts, right? And and that's I think&nbsp;&nbsp; where you know even as these models have gotten&nbsp; much larger. I'll tell you one of the things 37&nbsp;&nbsp; that I've really appreciated about 37 is that&nbsp; uh it does a great job of taking prompts to&nbsp;&nbsp; other models and decomposing them into each of&nbsp; the steps. Like I can take it and say hey if I&nbsp;&nbsp; needed to rewrite this where every sing where it&nbsp; was as many small granular steps as possible then&nbsp;&nbsp; uh then 37 or 37 has done a great job of&nbsp; that. So, listen, I appreciate all the uh&nbsp;&nbsp; the time and attention today. I'll be back if&nbsp; you have other questions uh back by the door,&nbsp;&nbsp; back outside. And uh thanks again for&nbsp; coming today. Thank you. [Applause] [Music] [Music]

---

## 11. Spotlight on Manus

**Preview:**
> Kind: captions Language: en My name is Tao and my nick is hik you can just&nbsp; find me on any social network with medic high&nbsp;&nbsp; cloud and right now I'm the co-founder of manasai&nbsp; and I'm acting our chief product officer but you&nbsp;&nbsp; know I'm not just a product guy I coding for&nbsp; like 28 years from my very early age like when&nbsp;&nbsp; I was nine I I can't remember but you know for&nbsp; AI actually I'm very newbie I just into this&nbsp;&nbsp; industry for only two ye...

**Full Transcript:**

Kind: captions Language: en My name is Tao and my nick is hik you can just&nbsp; find me on any social network with medic high&nbsp;&nbsp; cloud and right now I'm the co-founder of manasai&nbsp; and I'm acting our chief product officer but you&nbsp;&nbsp; know I'm not just a product guy I coding for&nbsp; like 28 years from my very early age like when&nbsp;&nbsp; I was nine I I can't remember but you know for&nbsp; AI actually I'm very newbie I just into this&nbsp;&nbsp; industry for only two years and uh uh what I want&nbsp; to achieve in the AI industry is like I want to&nbsp;&nbsp; build a product that can you know influence 24&nbsp; hours for a single user that is two years ago&nbsp;&nbsp; that is my dream but right now I think with manus&nbsp; AI actually I can achieve that dream at end of&nbsp;&nbsp; this year right Now the maximum usage for our user&nbsp; there's one single user uh he can just consume the&nbsp;&nbsp; GPD GPU power two hours per day just for himself.&nbsp; So I think we can achieve this goal at end of this&nbsp;&nbsp; year and today uh what I want to talk about is&nbsp; about two things uh one is about manners and&nbsp;&nbsp; another thing is about cloud models. So the first&nbsp; talk is about what is manas. Yeah, man. Actually,&nbsp;&nbsp; you know, it's some not like an English word. Uh&nbsp; because really it isn't. Yeah, man. This name just&nbsp;&nbsp; comes from MIT's motto which is men's at manas.&nbsp; It's a old Latin words which means man and hand.&nbsp;&nbsp; Why we choose this name manas? Mess is the hand&nbsp; is because we think for like all the past two&nbsp;&nbsp; years all these frontier models they're already&nbsp; like very smart just like uh humans brain. Yeah.&nbsp;&nbsp; They're super smart. they are capable of like&nbsp; doing different kind of of tasks but you know&nbsp;&nbsp; even with this very very smart brain we can&nbsp; make a real impact into the real world because&nbsp;&nbsp; we don't build hands for it you know just like&nbsp; when I was nine at that time that is like 1996&nbsp;&nbsp; uh back in China and at that time our families&nbsp; are not you know we are not rich enough to have a&nbsp;&nbsp; computer in each of our family so I only have like&nbsp; two times a week uh to go to the computer room in&nbsp;&nbsp; our in my primary school and at that time even&nbsp; I was the best in the class of of of of coding&nbsp;&nbsp; but without going to the computer to like debug uh&nbsp; to to write the code on the on the real machine I&nbsp;&nbsp; can't get the code right just in one time with a&nbsp; pen and a paper but you know that's exactly what&nbsp;&nbsp; we did for the past two years we have a very smart&nbsp; man but we only give it a pen and a paper and we&nbsp;&nbsp; ask them to solve very complex programs for us. So&nbsp; we think that's a program. So in manas we we don't&nbsp;&nbsp; train models. We don't even like post training&nbsp; models. What we do is like we are building hands&nbsp;&nbsp; for these models. Yeah, that is you know the&nbsp; the concept be behind our lane manas. Yeah,&nbsp;&nbsp; I think like most of you maybe uh saw our product&nbsp; man on social or you already a man user and you&nbsp;&nbsp; must like saw some use cases on our website&nbsp; any like social network but today I also want&nbsp;&nbsp; to like just share like two uh new use cases one&nbsp; from our internal usage another is from our user.&nbsp;&nbsp; Yeah. So this one is actually from our internal&nbsp; usage. Yeah because we are expanding globally.&nbsp;&nbsp; We just opened our Singapore office three weeks&nbsp; ago and opened our Tokyo office two weeks ago and&nbsp;&nbsp; we will open our San Francisco office tomorrow.&nbsp; Yeah. So when we are choosing our Tokyo office,&nbsp;&nbsp; uh we are asking man to help us. It's like&nbsp; we will say okay we have like a 40 people&nbsp;&nbsp; uh we'll be relocating to Tokyo. So just find the&nbsp; office we can which can fit 40 people and also we&nbsp;&nbsp; have to solve their accommodation problem. So this&nbsp; is a this is a prompt we give to manus and after&nbsp;&nbsp; we give this prompt to manus manus just have&nbsp; its own plan and then executes the plan. It's&nbsp;&nbsp; like search browse all these websites around the&nbsp; internet doing a lot of browsing browsing browsing&nbsp;&nbsp; research research research and after 24 minutes&nbsp; man just uh uh deliver this website for us. Yeah,&nbsp;&nbsp; this website it says a Tokyo office accommodation&nbsp; recommendations. It first it comes with a very&nbsp;&nbsp; interactive map which comes all the 10 options&nbsp; matt found for us. The blue marker is for the&nbsp;&nbsp; offic's location and the green marker is the&nbsp; accommodation near that office. All these 10&nbsp;&nbsp; options just on this interactive map. Yeah. And&nbsp; if you keep scrolling down you can see the first&nbsp;&nbsp; pair is a Shibuya one. It choose the Shibuya&nbsp; scramble square. That's exactly the office we&nbsp;&nbsp; went to when when we are looking office there.&nbsp; But you know this office is kind of like very&nbsp;&nbsp; fancy but too expensive for startup like us. So&nbsp; we just choose our office dear this dear near de&nbsp;&nbsp; near de near de near de near de near de near de&nbsp; near de near de near de near this this one very&nbsp;&nbsp; very dear very very close like maybe 200 meters&nbsp; away and it has the price and why we should choose&nbsp;&nbsp; this office and if you we choose this office uh&nbsp; what the commenation uh options we can get also&nbsp;&nbsp; has the distance to the office which is very great&nbsp; comes with like 10 pairs of the and at the bottom&nbsp;&nbsp; there's like an overview table for all these&nbsp; options yeah so all these have been done just&nbsp;&nbsp; under 20 minutes. So you can imagine maybe like&nbsp; your your inter or your assistant can achieve&nbsp;&nbsp; such detailed quantity of uh of information uh&nbsp; in such a short time. Another thing I want to&nbsp;&nbsp; demo here is like yeah you can just maybe send&nbsp; an image to manus uh empty room and ask manus to&nbsp;&nbsp; analyze this rule style and go to EKS website&nbsp; to find some furnitureures for your room and&nbsp;&nbsp; then you can see the final result. Yeah. So look&nbsp; first manus will just analyze uh this empty rules&nbsp;&nbsp; image and come up with idea okay this uh what&nbsp; what what style of this of this rule and the&nbsp;&nbsp; layout and what kind of furnitureures I should&nbsp; look look up in the EKS website and then M just go&nbsp;&nbsp; to EKS website and start browsing browsing for all&nbsp; these furnitureures and save these furnitureures&nbsp;&nbsp; images and then at last man will just return an&nbsp; image just with the real EKA furniture and also&nbsp;&nbsp; with a document with all the furniture he choose&nbsp; to like uh comband in this image with the link so&nbsp;&nbsp; actually you can just click click the link to buy&nbsp; it yeah right now we can't buy things for you but&nbsp;&nbsp; who knows after three months yeah maybe we can do&nbsp; payment yeah so that is kind of the things mass&nbsp;&nbsp; can do yeah it's like a a general agent yeah they&nbsp; can solve a very long tale of different type of&nbsp;&nbsp; tasks for you. Yeah. So we can jump back to the uh&nbsp; slides. Yeah. So that is like kind of like what is&nbsp;&nbsp; manas and today I think the most I want to share&nbsp; is like how we built manas. Yeah. Cursor just&nbsp;&nbsp; inspired us a lot. We got a lot of inspirations&nbsp; from cursor. I think you may sound weird because&nbsp;&nbsp; cursor is kind of like a code editor you know.&nbsp; Yeah. So all three founders of manas like pig,&nbsp;&nbsp; red and I we're all coders for a lot of years.&nbsp; So when we are using cursor I think cursor is&nbsp;&nbsp; actually very great product. It can help us to&nbsp; write different language even we don't know how&nbsp;&nbsp; to code in this language and it's very like&nbsp; like efficiency but the most interesting part&nbsp;&nbsp; is not we coders using uh using cursor the&nbsp; most interesting part is when we watch our&nbsp;&nbsp; friends our colleagues which are long coders&nbsp; watching they using cursor to solve their daily&nbsp;&nbsp; tasks like doing data visualization batch file&nbsp; processing convert a video file into an audio file&nbsp;&nbsp; It's very fascinating because you know this is&nbsp; the the interface of cursor right when these long&nbsp;&nbsp; coders using cursor they don't care about the&nbsp; left side because they they can't evaluate the&nbsp;&nbsp; code at all they can do is just keep the accept&nbsp; accept accept accept accept you know yeah so it's&nbsp;&nbsp; very interesting when watching the those friends&nbsp; you know using cursor so we we just come up with&nbsp;&nbsp; idea which is that those l programmers just using&nbsp; cursor to deal with their daily tasks. Yeah. Not&nbsp;&nbsp; like us, you know, when we are using cursor,&nbsp; we really want to write some code, you know,&nbsp;&nbsp; that can run multiple times for the future. But&nbsp; for this friend, they just want cursor to solve&nbsp;&nbsp; their task. They don't care about the code. Next&nbsp; time when they have the same task, they will not&nbsp;&nbsp; run that code again. They will just ask a cursor&nbsp; to do that. Maybe generate a new code for them.&nbsp;&nbsp; So code is is like maybe not the ultimate goal.&nbsp; It's just you know just an intermediate step for&nbsp;&nbsp; solving problems. So we come up with idea is that&nbsp; maybe we should build the opposite. Yeah we should&nbsp;&nbsp; build the right panel of cursor and another thing&nbsp; we want to build is that we want the right panel&nbsp;&nbsp; to running in the cloud. Why is that? Because&nbsp; you know when we are using cursor on our computer&nbsp;&nbsp; there's it has to ask your permission to to&nbsp; continue because it is running on on your computer&nbsp;&nbsp; any action it perform on your computer is like&nbsp; will may be dangerous right after it maybe it will&nbsp;&nbsp; install some dependencies install some software&nbsp; it may break your computer but we think if it's&nbsp;&nbsp; running in the cloud it's much safer and also when&nbsp; it's running in the cloud you don't have to pay&nbsp;&nbsp; your attention to it you just you know assign&nbsp; a task to manus you can just close your laptop&nbsp;&nbsp; or just put your phone back to your pocket you&nbsp; can do other thing and after the task is done we&nbsp;&nbsp; will give you a notification and you will get the&nbsp; result yeah so that's the original idea of how we&nbsp;&nbsp; come up with idea of the manus that is happening&nbsp; last October so from last October to this March&nbsp;&nbsp; five months of work. This is what you saw today.&nbsp; Uh which is the manusi in fifth of March. Yeah. So&nbsp;&nbsp; that's the original idea. Yeah. Thank you guy. And&nbsp; also I want to share some details when we building&nbsp;&nbsp; manus. What kind of source we have in our man.&nbsp; The first is that the first key component of manus&nbsp;&nbsp; is that we give manus a computer which is super&nbsp; important when you compare to other chatbot usage&nbsp;&nbsp; because you know in manas each manus t will be&nbsp; assigned a fully functional virtual machine manus&nbsp;&nbsp; can use the file system terminal vs code and a&nbsp; real chromium browser you know it's not a headless&nbsp;&nbsp; browser yeah all these apps in a virtual machine&nbsp; which you know it it creates a lot of different&nbsp;&nbsp; opportunity for manus to solve different kinds&nbsp; of problem like you can just drag you can just&nbsp;&nbsp; send a zip compress file contains maybe hundreds&nbsp; of of PDFs and ask manus to un unzip it and then&nbsp;&nbsp; uh extract all these unstructured data from these&nbsp; hundreds of PDFs into a structured spreadsheet&nbsp;&nbsp; yeah that's the thing you can do so we think&nbsp; give it a computer is actually what make matters&nbsp;&nbsp; really different comparing to other agents or&nbsp; other chat bots and the second thing is that we&nbsp;&nbsp; think you know nowadays it's not it's not every&nbsp; information or data is on the public internet&nbsp;&nbsp; there are so many things under pay behind pay or&nbsp; in this private databases but users because man&nbsp;&nbsp; you know we are targeting to the consumer market&nbsp; we're not targeting to the enterprise market so&nbsp;&nbsp; for for an average user who is not very familiar&nbsp; of like how to call an API you know how to write&nbsp;&nbsp; code to access databases we think it's better for&nbsp; us to prepay all these like private databases APIs&nbsp;&nbsp; for the users then users want won't care how they&nbsp; can get maybe some real-time financial data yeah&nbsp;&nbsp; things like that so it's like we pre we prepare&nbsp; this uh private databases for our users and the&nbsp;&nbsp; third thing is that in manus actually you can&nbsp; just teach manus how to solve programs like&nbsp;&nbsp; uh one month before we release manus check GBT&nbsp; just released their deep research so there's a&nbsp;&nbsp; experience actually I don't like it's like you ask&nbsp; a question to deep research it will returns five&nbsp;&nbsp; or six questions for you know I don't I don't&nbsp; think that's a very good experience because I&nbsp;&nbsp; want you to solve task for me not asking me more&nbsp; questions but you know some of our colleagues&nbsp;&nbsp; someone in our team they like this experience so&nbsp; we have an internal discussion about whether we&nbsp;&nbsp; we should do something maybe like a workflow&nbsp; or maybe like a hard code things to deliver&nbsp;&nbsp; such experience but instead we didn't do that we&nbsp; implemented a personal logic system which is like&nbsp;&nbsp; you can just teach manus next time when you are&nbsp; go out to do some research before you do before&nbsp;&nbsp; you start just confirm all the details with me and&nbsp; then executes it then manus will once you accept&nbsp;&nbsp; that knowledge into your personal knowledge&nbsp; system man will remembers it and it will just&nbsp;&nbsp; uh acts like you know every time when it want to&nbsp; do some research it will confirm with you first&nbsp;&nbsp; and then exit it I think all these three just&nbsp; makes management really powerful but I think the&nbsp;&nbsp; most important part because if you want to build&nbsp; an agent all these three reasoning I think maybe&nbsp;&nbsp; you you definitely will give it a try but we think&nbsp; why the magic happens why man is such a a great&nbsp;&nbsp; experience the most important part is not about&nbsp; these three components we think the most important&nbsp;&nbsp; part is that at the fundamental concept of manners&nbsp; we have this less structure more intelligence&nbsp;&nbsp; you can defend this sentence at the bottom of&nbsp; our official website. We just you know we just&nbsp;&nbsp; believe in this. We we we take so much faith&nbsp; in in it. And how do we define net structure&nbsp;&nbsp; more intelligence is that when we release manus&nbsp; actually we put 42 use cases on our website and&nbsp;&nbsp; someone say okay man is is is not a saying you&nbsp; just uh uh predefine some workflows maybe you&nbsp;&nbsp; have like 42 predefined workflows but actually&nbsp; you know inside manus in the core of manus we have&nbsp;&nbsp; zero predefined workflows it's just a very simple&nbsp; but very robust structure very simple structure&nbsp;&nbsp; But we just left all the intelligence part to the&nbsp; foundational model. At that time it is cloud sol&nbsp;&nbsp; 3.7 and now we have cloud 4 right. So what we are&nbsp; doing is like because we are building the hands&nbsp;&nbsp; right it's like we just compose all these context&nbsp; to build them into like a more good structure and&nbsp;&nbsp; then we provide more context to the model and we&nbsp; have less control. When I say license control,&nbsp;&nbsp; I mean multi-roll agent system, right? You have&nbsp; to specify this a coding agent, this is a search&nbsp;&nbsp; agent, this is blah blah blah agent. We think all&nbsp; these kind of phones are a control. It just limits&nbsp;&nbsp; the real potential of LMS. So for us, it's like we&nbsp; just provide more context to the to the model and&nbsp;&nbsp; the net the model uh to like improise by itself.&nbsp; And uh all this magic you get from manas just come&nbsp;&nbsp; from this very simple but very strong ideology is&nbsp; less structure more intelligence. Yeah. So that&nbsp;&nbsp; is the secret behind manas and why we choose cloud&nbsp; in the first place is for three reasons. The first&nbsp;&nbsp; one is about the long horizon planning. You know&nbsp; I think before cloud sol models because because&nbsp;&nbsp; chatbot is just so successful. So all these models&nbsp; out out there uh maybe before before this March&nbsp;&nbsp; are post train and align their alignments is for&nbsp; chatboard scenario and in chatboard scenario the&nbsp;&nbsp; model intends to answer your question in one&nbsp; turn is that you ask the question and you get&nbsp;&nbsp; the answer but you know in agentic scenarios&nbsp; like in manners an average task will take maybe&nbsp;&nbsp; 30 or 50 steps and then get the final answer.&nbsp; So when we are building manus actually we try&nbsp;&nbsp; every model we can get our hands on but it finds&nbsp; out only sonet can know okay I imaging a very&nbsp;&nbsp; giant agentic loop so I should perform the action&nbsp; watch the observation and decide what's the action&nbsp;&nbsp; observation abs observation it's just a very giant&nbsp; agentic loop so just so know okay I'm in this loop&nbsp;&nbsp; so I have to gather more information before I&nbsp; delivers a final result. But you know for these&nbsp;&nbsp; five months we try all other models all of them&nbsp; failed after maybe only one two three iterations&nbsp;&nbsp; those models will think that okay I think it's&nbsp; enough I will answer your questions right now so I&nbsp;&nbsp; think that's a problem and right now what we found&nbsp; the best model to run a very long horizon planning&nbsp;&nbsp; sc is cloud solenance models that is the first&nbsp; thing why we choose cloud and for second part is&nbsp;&nbsp; about the tool use because you agent product just&nbsp; heavily relied on the tool use on the function&nbsp;&nbsp; calling. Yeah, like for us we just abstract&nbsp; all these tools in that virtual machine. We&nbsp;&nbsp; have like 27 tools in the virtual machine. So the&nbsp; agent framework has to decide what's the action&nbsp;&nbsp; should be performed into that virtual machine.&nbsp; So it's it's very important to call the right&nbsp;&nbsp; tool and write the tools parameters uh right. So&nbsp; at that time when we are building manus we don't&nbsp;&nbsp; have the sync tool in cloud models. So we kind&nbsp; of like implemented some mechanism by our own we&nbsp;&nbsp; call it coot injection which is like before every&nbsp; function call we just will use another specific we&nbsp;&nbsp; call the planner agent to do the reasoning at that&nbsp; time we don't have reasoning too. Yeah because&nbsp;&nbsp; that's that's the three 3.5. Yeah. So we have to&nbsp; do the reasoning by oursel and then we inject that&nbsp;&nbsp; reasoning that coot into the main agent and then&nbsp; we perform that function call and what we found&nbsp;&nbsp; out it's just boost the performance of function&nbsp; calling and also in an article anoroptical release&nbsp;&nbsp; at end of March with the sync tool they also&nbsp; found out this and in cloud 4 it kind of has&nbsp;&nbsp; some like a native support for the uh syncing in&nbsp; the tool use I think you you guys all say in this&nbsp;&nbsp; morning's session. Yeah. So that's the second one.&nbsp; And the for third one is like we think is model&nbsp;&nbsp; maybe the best they have the best alignment with&nbsp; agentic usage because you know in agentic usage&nbsp;&nbsp; we have to deal with the browser the computer I&nbsp; think as put so much resources on the alignment&nbsp;&nbsp; with the computer use the browser you see so which&nbsp; makes cloud model maybe the best model uh to build&nbsp;&nbsp; agents that's why we choose astropical models for&nbsp; manness and also you know we just spend a lot of&nbsp;&nbsp; tokens on M on cloud and the whistle. This is the&nbsp; t-shirt I wear in the GTC event. We just wear this&nbsp;&nbsp; this this shirt in a GTC event. Yeah, we spend&nbsp; like $1 million on cloud model in the first 14&nbsp;&nbsp; days. I think maybe that's why they they invited&nbsp; us here to to give the speed. Yeah, it cost us&nbsp;&nbsp; a lot to be on the stage, you know. Yeah. So,&nbsp; that's the whole story about the manner so far.&nbsp;&nbsp; Yeah. So if you have questions, you can just line&nbsp; up at the two mics. We gonna start from right to&nbsp;&nbsp; left. Yeah. Each by each. Yeah. Any questions?&nbsp; Yeah. [Applause] Cool. Oh yeah, there's one. Hello. Thanks for the presentation. Very&nbsp; interesting. So um I don't know how much&nbsp;&nbsp; you can share but uh I was just curious about&nbsp; uh in your uh agentic workflow uh especially the&nbsp;&nbsp; browser part how much is that uh vision and how&nbsp; much is that actually you know parsing the code of&nbsp;&nbsp; the web page right so how much of it is like the&nbsp; model looking at the browser like a person would&nbsp;&nbsp; uh and how much of that is that is more like&nbsp; text type of interaction if that makes sense.&nbsp;&nbsp; Uh sorry I I may I may to I may get the question.&nbsp; It's like a so when you when the the manus uses&nbsp;&nbsp; the web browser right? Oh yeah yeah yeah it goes&nbsp; on the on a website right and how much of the&nbsp;&nbsp; understanding of the web page oh is based on the&nbsp; vision yeah I got your question how much is based&nbsp;&nbsp; on the yeah yeah yeah that's a very good question&nbsp; I actually I can share it's it's not a secret&nbsp;&nbsp; because you know yeah uh um when we are building&nbsp; mass from last October there is an open open&nbsp;&nbsp; source project called browser use I think most of&nbsp; you guys maybe know that project so we just take&nbsp;&nbsp; a look at that project we think their way of how&nbsp; to talk to the browsers is actually very useful&nbsp;&nbsp; So we just use that part because you know two&nbsp; months is later browser use has its own agent&nbsp;&nbsp; framework. We don't use that part. We don't&nbsp; use that agent framework bar. We only use the&nbsp;&nbsp; protocol they talk to the browsers. So the&nbsp; thing we are sending to the uh the context&nbsp;&nbsp; we're sending to the foundational model when&nbsp; we when is browsing internet are three things.&nbsp;&nbsp; The first is the text in this viewport. We will&nbsp; send it to to uh cloud. And the second thing is a&nbsp;&nbsp; screenshot. Yeah. In in this viewport. And the&nbsp; third thing is a screenshot but with bounding&nbsp;&nbsp; boxes. You know then the model can decide which&nbsp; area he should click. Yeah. So right now we are&nbsp;&nbsp; just share this reason to the model. Yeah. I&nbsp; don't know if I answer your question. Yeah.&nbsp;&nbsp; Yeah. Yeah. Yeah. Yeah. Okay. Yeah. Cool. Oh yeah.&nbsp; Another one. Hi uh I have a slightly controversial&nbsp;&nbsp; question but I I want to kind of put put this out&nbsp; to you. Yeah. uh with the advancement of lot of&nbsp;&nbsp; these foundational model and the deep research&nbsp; capability, how do you see a rapper company kind&nbsp;&nbsp; of keeping their edge like what how do you see&nbsp; it sort of what are the research area what are&nbsp;&nbsp; the areas that you're concerned about what are you&nbsp; focused so that you can actually grow in parallel&nbsp;&nbsp; with these foundational model as their capability&nbsp; increases definitely I I kind of get your question&nbsp;&nbsp; because you know we answer this question for the&nbsp; past two months to a lot of investors you know&nbsp;&nbsp; yeah they are all asking okay what's your mode&nbsp; being as a rapper you know yeah things like that&nbsp;&nbsp; um we think you know there will be no mode just&nbsp; for some very specific technology or framework&nbsp;&nbsp; because technology will be out of data very very&nbsp; very fast even for models right so we think the&nbsp;&nbsp; only thing we can win this competition is about&nbsp; the pace of innovation like what you mentioned&nbsp;&nbsp; there's like a a use case like deep research&nbsp; deep deep research is one of the main use case&nbsp;&nbsp; in manus we have like maybe 20% of users are doing&nbsp; deep research every day on manas but you know&nbsp;&nbsp; actually we put zero effort on deep research use&nbsp; cases like I just mentioned we just build a very&nbsp;&nbsp; simple structure a very simple agent framework&nbsp; the deep research capability is just emergence&nbsp;&nbsp; from this framework which means you know maybe&nbsp; open AI they need maybe half a year to do the&nbsp;&nbsp; end to end training just for this specific use&nbsp; cases. But this capability just you know just&nbsp;&nbsp; emerges from from the whole structure. So in six&nbsp; months is maybe we will have like a hundreds of&nbsp;&nbsp; use cases like deep research and and also what we&nbsp; can do is like we can leverage the best model in&nbsp;&nbsp; the world. Yeah. You know I I I think these are&nbsp; two things. It's about the flexibility of your&nbsp;&nbsp; whole agent framework and the second thing is like&nbsp; we are really flexible of choosing the best model&nbsp;&nbsp; in the world. Yeah. All right. Thank you. Okay.&nbsp; Yeah. Hi. Hello. Hi. Hello. Question. Uh so uh&nbsp;&nbsp; right now maybe the last question. Sorry. Uh turn&nbsp; around. Yeah. Okay. Um so right now you build the&nbsp;&nbsp; uh browser in a virtual environment. Are you&nbsp; planning to put uh the browser in a docker in&nbsp;&nbsp; the local computer and later they can use the&nbsp; local cookie and access all the account? Yeah,&nbsp;&nbsp; actually we we don't have plan for local&nbsp; environment because as I just mentioned&nbsp;&nbsp; we think it's very important to like give the&nbsp; users attention back to users. We don't want to&nbsp;&nbsp; like user to pay attention still to his phone&nbsp; or to his computer. We want everything to run&nbsp;&nbsp; in the cloud. And we we we don't just have a a&nbsp; Linux virtual machine. We still we we also have&nbsp;&nbsp; plan to have a virtual Windows virtual Android&nbsp; for manus to to run in the future. So it's all&nbsp;&nbsp; about running in the cloud. Yeah. Okay. I think&nbsp; my time is up. Thanks for guys for today. Yeah. Uh

---

## 12. Spotlight on Shopify

**Preview:**
> Kind: captions Language: en Hi everyone. I'm Obie Fernandez. Uh, and I'm&nbsp; going to be talking to you today about some&nbsp;&nbsp; of the awesome things we're doing at Shopify. Uh,&nbsp; leveraging the power of claude and claude code as&nbsp;&nbsp; part of uh, our massive in a way. I'll talk about&nbsp; that in a second. Uh, first of all, I just want to&nbsp;&nbsp; introduce myself. My name is Obie Fernandez. I'm&nbsp; a principal engineer in our augmented engineering&nbsp;&nbsp; group. I wo...

**Full Transcript:**

Kind: captions Language: en Hi everyone. I'm Obie Fernandez. Uh, and I'm&nbsp; going to be talking to you today about some&nbsp;&nbsp; of the awesome things we're doing at Shopify. Uh,&nbsp; leveraging the power of claude and claude code as&nbsp;&nbsp; part of uh, our massive in a way. I'll talk about&nbsp; that in a second. Uh, first of all, I just want to&nbsp;&nbsp; introduce myself. My name is Obie Fernandez. I'm&nbsp; a principal engineer in our augmented engineering&nbsp;&nbsp; group. I work on everything that has to do with&nbsp; augmented engineering or in other words like using&nbsp;&nbsp; AI to improve developer experience at Shopify. I'm&nbsp; also the author of a pretty interesting book that&nbsp;&nbsp; you might like called patterns of application&nbsp; development using AI. So I hope that you look&nbsp;&nbsp; that up uh if you get a chance. Uh today we're&nbsp; going to be talking about the challenges of scale&nbsp;&nbsp; at Shopify Engineering. We are a very uh fairly&nbsp; large organization, probably one of the largest&nbsp;&nbsp; Ruby on Rails uh shops in the world. Uh our main&nbsp; application we've been working on for almost 20&nbsp;&nbsp; years. Uh and has millions upon millions of lines&nbsp; of code. It's probably about 5,000 uh repos uh in&nbsp;&nbsp; our organization last time I checked. and we're&nbsp; generating about uh half a million PRs a year&nbsp;&nbsp; at last count which is a significant amount&nbsp; of um stuff to take into account when you're&nbsp;&nbsp; uh you know doing anything with AI uh in terms of&nbsp; your context and whatnot. Um, our core challenge&nbsp;&nbsp; is how do we maintain productivity or rather&nbsp; that's what my my group focuses on and uh I'm here&nbsp;&nbsp; to kind of tell you some of the solutions we've&nbsp; developed and how those interact with uh cloud. the the key really to understanding kind of&nbsp; the point of what what I'm presenting is to&nbsp;&nbsp; understand that there there's two very&nbsp; very different ways of using AI uh one&nbsp;&nbsp; which we've been immersed in throughout&nbsp; this conference today are agentic tools&nbsp;&nbsp; uh when we're talking about how to leverage AI&nbsp; uh as assistants and tooling there's this agentic&nbsp;&nbsp; approach which are ideal for scenarios&nbsp; that require adaptive decisionmaking&nbsp;&nbsp; uh iteration and autonomy, right? They shine when&nbsp; the tasks that you're you're trying to accomplish&nbsp;&nbsp; with AI help uh are exploratory or ambiguous&nbsp; in some way and you're relying on the LM's&nbsp;&nbsp; reasoning and judgment uh because your path to&nbsp; the solution is not known in advance. It might&nbsp;&nbsp; not be known in advance because it's a complex&nbsp; domain that you're dealing with that has factors&nbsp;&nbsp; that are changing all the time. It might be very&nbsp; very complicated and just kind of beyond what you&nbsp;&nbsp; know or it might be like something as simple as a&nbsp; feature development where you're going to do some&nbsp;&nbsp; exploratory work to figure out how to implement&nbsp; that feature. Uh those kinds of use cases as&nbsp;&nbsp; we've seen again and again today in the various&nbsp; sessions are perfect for tools like cloud code.&nbsp;&nbsp; um anything that involves ongoing adaptation uh&nbsp; debugging and iteration perfectly fantastic to&nbsp;&nbsp; give that to an agentic tool and see what it&nbsp; can do. Um in contrast um structured workflow&nbsp;&nbsp; orchestration including what we can do with this&nbsp; open source tool that I'm going to present to you&nbsp;&nbsp; in this uh presentation which we call roast&nbsp; um are better for tasks that have predictable&nbsp;&nbsp; well- definfined steps uh cases where you seek&nbsp; uh consistency repeatability and clear oversight.&nbsp;&nbsp; you want to leverage AI and these kind and&nbsp; this kind of work for intelligent completion&nbsp;&nbsp; of components of that bigger workflow. So far,&nbsp; I don't think I'm saying anything that is super&nbsp;&nbsp; exotic or or wild. It's really the difference&nbsp; between non-deterministic and deterministic&nbsp;&nbsp; kind of behavior. Um, and it turns out that&nbsp; like peanut butter and chocolate, you know,&nbsp;&nbsp; these make a great combination. Sometimes you want&nbsp; one, sometimes you want the other. And examples&nbsp;&nbsp; of what these structured workflows are great for&nbsp; are things like migrating legacy code bases. So&nbsp;&nbsp; for instance, going from Python 2 to Python 3,&nbsp; uh from going whatever your current, you know,&nbsp;&nbsp; JavaScript implementation is based on to whatever&nbsp; the new flavor of the month is. uh or as is the&nbsp;&nbsp; case uh with a lot of things that my team deals&nbsp; with refactoring large systems where it isn't&nbsp;&nbsp; really an exploratory task like we know what we&nbsp; want to do. Maybe we're addressing performance,&nbsp;&nbsp; maybe we're addressing technical debt that we&nbsp; understand kind of what the basis is. So we&nbsp;&nbsp; know that we want to go through a certain amount&nbsp; of steps. uh specifically the kinds of things&nbsp;&nbsp; that that we do at Shopify using roast uh which is&nbsp; really what we extracted this open source library&nbsp;&nbsp; out of started with automated testing generation&nbsp; and test optimization. So we looked at our over&nbsp;&nbsp; half a million tests associated with our main&nbsp; monolith and said we would really like to address&nbsp;&nbsp; some of the coverage gaps in this codebase. So how&nbsp; do we go about doing that? Of course, one approach&nbsp;&nbsp; would be to simply open up that project in cloud&nbsp; code and say, "Hey, I want to address coverage&nbsp;&nbsp; gaps in this place." However, in practice,&nbsp; it really it's it's really helpful to break&nbsp;&nbsp; down that problem in the way that you would do it&nbsp; manually and say, "Okay, well, what would I do if&nbsp;&nbsp; I was going to work on test coverage?" Well, first&nbsp; of all, I need to know what the test coverage is.&nbsp;&nbsp; If we know that every time we're going to do&nbsp; a series of steps that calls for a structured&nbsp;&nbsp; workflow. For instance, running the coverage tool,&nbsp; running the test, you know, to generate the report&nbsp;&nbsp; of what needs to be covered. So, taking a step&nbsp; back for a second, we've been using Cloud Code&nbsp;&nbsp; for a while. We were one of the early shops that&nbsp; actually adopted it. As soon as it launched, there&nbsp;&nbsp; was interest in using it. And as soon as people&nbsp; started using it, we started seeing a lot of&nbsp;&nbsp; excitement in our Slack, right? So, I copied some&nbsp; of the earliest comments I could find there from&nbsp;&nbsp; March, you know, from from some of our folks. And&nbsp; I pulled this graph from from our AI proxy that&nbsp;&nbsp; Cloud Code runs through. And I think it's actually&nbsp; a pretty a fairly impressive amount of usage. We&nbsp;&nbsp; have at peak now about 500 daily active users and&nbsp; that number is growing rapidly. And as of lately,&nbsp;&nbsp; we hit 250,000 requests per second at at peak,&nbsp; which is is an impressive amount, I believe. Um,&nbsp;&nbsp; and in fact, ROST itself, which is this open&nbsp; source framework that I was telling you about,&nbsp;&nbsp; is called ROS because it helps you set your&nbsp; money on fire. Yeah, think about it. So, anyway,&nbsp;&nbsp; what does it look like? I uh I let this video kind&nbsp; of stay here at uh a workflow definition. So this&nbsp;&nbsp; is a workflow orchestration tool. There's nothing&nbsp; super super exotic about it. Uh probably the most&nbsp;&nbsp; one of the most interesting things about it is&nbsp; that it's implemented in Ruby, which is a bit&nbsp;&nbsp; of an oddity in this world where everyone uses py&nbsp; python and typescript unfortunately. Uh however,&nbsp;&nbsp; you don't need to implement anything in Ruby&nbsp; to use uh roast for your own things. This can&nbsp;&nbsp; actually help you interle uh prompt oriented kind&nbsp; of tasks with bash scripts or you know whatever&nbsp;&nbsp; whatever you want to invoke. Um so anyway why did&nbsp; why did we go through the trouble of writing roast&nbsp;&nbsp; and open sourcing it on my team? Well the thing is&nbsp; uh our illustrious CEO uh Toby Licki has instilled&nbsp;&nbsp; throughout the years a culture of tinkering in the&nbsp; company. So even without AI uh we have a culture&nbsp;&nbsp; where people are constantly working on homegrown&nbsp; projects little skunk works little research uh you&nbsp;&nbsp; know things within their departments and this&nbsp; is not only limited to engineering it's across&nbsp;&nbsp; the board you know I've seen people in sales and&nbsp; support and things like that working on their own&nbsp;&nbsp; tooling AI exploded that so you know as soon as uh&nbsp; you know vibe coding became a thing curs you know&nbsp;&nbsp; uh different kinds of tools came about and&nbsp; were available like cloud code and and all the&nbsp;&nbsp; different kinds of you know chat completion. All&nbsp; of a sudden everyone was coding across the company&nbsp;&nbsp; and specifically when it came to anything that&nbsp; looks like a structured workflow or essentially&nbsp;&nbsp; a script that puts together a number of prompts&nbsp; or chains them together. I think that there's&nbsp;&nbsp; probably safe to say that there's hundreds of&nbsp; different ways that this has been implemented&nbsp;&nbsp; across your company. And if I see some of you&nbsp; nodding like you if you work at big companies you&nbsp;&nbsp; probably have seen this like constant reinventing&nbsp; of the wheel. You know some people are using&nbsp;&nbsp; um you know one frameworks you know some people&nbsp; are using lang chain some people are just writing&nbsp;&nbsp; their own scripts etc etc. That's cool and&nbsp; all, but it's better, you know, if you start&nbsp;&nbsp; identifying the common needs across the&nbsp; organization and you you put something&nbsp;&nbsp; together to to really help them out. So that's&nbsp; where ROS came from. And I want to tell you about&nbsp;&nbsp; the relationship with cloud code and roast because&nbsp; it's a birectional thing which is really really&nbsp;&nbsp; cool. So like I said earlier, you could try to get&nbsp; cloud code to execute a workflow uh a predefined&nbsp;&nbsp; workflow. You could set up commands. You could set&nbsp; up a bunch of CloudMD files. All that's well and&nbsp;&nbsp; good, and I'm not telling you, I'm not here to&nbsp; tell you, hey, don't try to do that. It's just&nbsp;&nbsp; that no matter how good the state-of-the-art&nbsp; models get at following instructions,&nbsp;&nbsp; they're still inherently non-deterministic. And&nbsp; you have something else which is the accumulation&nbsp;&nbsp; of entropy. Uh and what I mean by that is that at&nbsp; every step of a given workflow that you just give&nbsp;&nbsp; an agent to to work on independently, errors and&nbsp; uh you know misdirection and uh lack of you know&nbsp;&nbsp; problems with judgment mistakes add up right&nbsp; I'm sure if you've done any amount of prompt&nbsp;&nbsp; chaining you've seen this like basically something&nbsp; goes slightly wrong that makes the next step work&nbsp;&nbsp; a little bit worse or the model has to do more&nbsp; work to recover. uh it's not ideal. What we're&nbsp;&nbsp; finding is that interle non-deterministic kinds of&nbsp; structured workflow with uh sorry nondeterministic&nbsp;&nbsp; agentic workflows with uh deterministic kind of&nbsp; structured workflows and scripts is actually the&nbsp;&nbsp; perfect combination. So what I mean by that is&nbsp; that you take a workflow, a big workflow like&nbsp;&nbsp; let's say optimizing a test suite and you break&nbsp; it down into component parts and you minimize&nbsp;&nbsp; the amount of instructions that you have to&nbsp; give the agent to work on at any given step. That looks like giving claude code roast&nbsp; workflows um on the on the one side. So&nbsp;&nbsp; on the left side of the slide here, what I'm&nbsp; describing is like basically you tell claude,&nbsp;&nbsp; hey, I want to work on optimizing uh my tests,&nbsp; but I have a workflow tool that handles the&nbsp;&nbsp; grading. So go ahead and call roast test grade&nbsp; with this file or this directory and then take&nbsp;&nbsp; its recommendations and work on them. So that's&nbsp; one way of using roast as a tool for cloud code.&nbsp;&nbsp; On the other side, ROSE includes a coding agent&nbsp; tool that you can add to your workflows in as&nbsp;&nbsp; part of its configuration which wraps claude&nbsp; code. So you could kick off a workflow in an&nbsp;&nbsp; automated fashion that let's say grades a&nbsp; test uh and as part of the steps in that&nbsp;&nbsp; workflow you can kick off cloud code in SDK&nbsp; mode and provide something that you want the&nbsp;&nbsp; agent to work on but on a narrow narrower&nbsp; scope than to giving it the whole thing. I've already talked about test grading, but&nbsp; to give you another example, um the the main&nbsp;&nbsp; application that we use at Shopify, like I said&nbsp; before, is a big Ruby on Rails monolith. Ruby&nbsp;&nbsp; is a dynamic language that doesn't natively have&nbsp; typing. So, we use an add-on typing system called&nbsp;&nbsp; Sorbet. Sorbet is not something that is super&nbsp; super wellknown by the models. certainly has a&nbsp;&nbsp; little bit of knowledge of it, but the the kinds&nbsp; of tools that you invoke when you're doing type&nbsp;&nbsp; checking and preparation of type files and things&nbsp; like that is not something that is let's call it&nbsp;&nbsp; quote unquote intuitive to the models. very&nbsp; very helpful to break anything up that has to&nbsp;&nbsp; do with like type checking or imp improving the&nbsp; application of types in our codebase into these&nbsp;&nbsp; roast workflows where we actually interle calls&nbsp; to the sorbet tools that are predefined like we're&nbsp;&nbsp; always going to run the type checking in this way&nbsp; with a command line and then we interle that with&nbsp;&nbsp; giving the results of the type checking to claude&nbsp; and saying hey uh please address the deficiencies&nbsp;&nbsp; that we found in this manual step. This is not&nbsp; a super compelling video. I didn't have a ton of&nbsp;&nbsp; time to prepare this talk, but like basically um&nbsp; if the video starts here, what you'll see is like&nbsp;&nbsp; the result of running one of these workflows um&nbsp; to generate tests. So, it gets stuck here running&nbsp;&nbsp; coding agent. I'm talking to the uh cloud code&nbsp; team about maybe giving us some ability to output&nbsp;&nbsp; what the coding agent is doing. But we see that&nbsp; it's generating tests. I'm actually flipping over&nbsp;&nbsp; and running the test to verify that they run, or&nbsp; rather to show you that they run, but this is kind&nbsp;&nbsp; of what it looks like at scale. It's a bit messy.&nbsp; I should add if you want to try roast, um, it is&nbsp;&nbsp; a very early version. It does work. It has a cool&nbsp; set of tools. It has cool features like being able&nbsp;&nbsp; to save your your session every time you run a&nbsp; workflow. If any of you have tried to do workflow&nbsp;&nbsp; kinds of things, uh, one of the key benefits of&nbsp; using a tool like roast is that for instance, if&nbsp;&nbsp; you have a five-step workflow, you don't have to&nbsp; run the first four steps over and over again just&nbsp;&nbsp; to debug the fifth step. You can just go ahead and&nbsp; replay from the fourth step afterwards and then,&nbsp;&nbsp; you know, work on it. Big big timesaver. We&nbsp; also do things like incorporate function uh tool&nbsp;&nbsp; function caching. Uh a lot of times when you're&nbsp; developing these workflows, you're kind of working&nbsp;&nbsp; on the same data set. If you're only working in&nbsp; an agentic tool, you kind of have to give it the&nbsp;&nbsp; whole thing and let it run from the beginning&nbsp; and do all the tool things that it's going to&nbsp;&nbsp; do all the function calling. Uh if you're using a&nbsp; tool like roast, you can do that and have all your&nbsp;&nbsp; function to uh you know calls cached at the roast&nbsp; level so that they just execute super super fast. I mentioned before um but just to bring it home&nbsp; we are using cloud SDK as a tool for roast.&nbsp;&nbsp; So specifically the kinds of things that we're&nbsp; using that for um is that the configured roast&nbsp;&nbsp; workflow oneshots uh a code migration for instance&nbsp; because it's kind of like we know exactly what we&nbsp;&nbsp; want to do. We don't want to beat around the bush&nbsp; or have to discuss what it is we're going to do&nbsp;&nbsp; with Claude. So we're just going to go ahead and&nbsp; do that just using regular chat completion style&nbsp;&nbsp; uh prompting. And then once we have a starting&nbsp; place, we hand that over to Claude using the SDK&nbsp;&nbsp; command line and say, "Hey, uh, run the test&nbsp; for this." And then if it's broken, fix it,&nbsp;&nbsp; iterate on on on doing it. Again, these&nbsp; are these are these are things that are&nbsp;&nbsp; not necessarily that useful to the indiv,&nbsp; you know, to the individual developer,&nbsp;&nbsp; like as they're going about their day,&nbsp; probably they're just going to use cloud code.&nbsp;&nbsp; But if you're doing this at scale or as part of&nbsp; repeatable processes or as part of reacting to&nbsp;&nbsp; PRs or anything like that uh you know as part&nbsp; of data pipelines becomes super super useful. I want to leave some time for questions. So I'm&nbsp; just going to move on a little bit uh faster. I&nbsp;&nbsp; wanted to mention that uh from experience um&nbsp; one of the things that's a little bit tricky&nbsp;&nbsp; when you're using uh cloud code SDK is kind of&nbsp; figuring out what tools you need. Um however an&nbsp;&nbsp; option that doesn't I think get enough love or&nbsp; get mentioned especially when you're prototyping&nbsp;&nbsp; is you can just say dangerously skip permissions&nbsp; which just kind of lets it do whatever it wants.&nbsp;&nbsp; uh and you know when you're prototyping&nbsp; and figuring out how you know how you're&nbsp;&nbsp; going to use your coding agent that's often&nbsp; very useful and as I was kind of giving some&nbsp;&nbsp; initial versions of the this talk to my&nbsp; colleagues they said hey would probably&nbsp;&nbsp; be useful to put an example prompt of what it&nbsp; looks like when you include a coding agent in&nbsp;&nbsp; uh in your workflow. So uh I put an example&nbsp; prompt in there. Uh use your code agent tool&nbsp;&nbsp; function to raise the branch coverage level&nbsp; of the following test above 90%. After each&nbsp;&nbsp; modification run rake test with coverage p you&nbsp; know path to the test etc. You get the picture. So finally hopefully you've liked this&nbsp; introduction. I know I know that maybe to&nbsp;&nbsp; some of you this might seem a little bit boring&nbsp; but to us kind of making that discovery that&nbsp;&nbsp; interle these deterministic and non-deterministic&nbsp; kinds of processes together uh and leveraging&nbsp;&nbsp; the power of cloud code was actually a magical&nbsp; com uh combination is taking off like wildfire&nbsp;&nbsp; within Shopify um you know this is something&nbsp; we just launched we've had it internally&nbsp;&nbsp; uh within our development kind of environment for&nbsp; test grading and optimization probably for five or&nbsp;&nbsp; 6 weeks now and we launched it as open source&nbsp; I think two or 3 weeks ago and it's starting&nbsp;&nbsp; to take off like wildfire at this point now that&nbsp; people realize hey there's a standardized solution&nbsp;&nbsp; uh also because of time pressure I wasn't able to&nbsp; show you all the features of roast it actually has&nbsp;&nbsp; a lot of cool things uh like just being able to&nbsp; declare inline prompts within your workflows uh&nbsp;&nbsp; being able to declare inline uh bash commands Uh,&nbsp; and it has a lot of conventionoriented things. So,&nbsp;&nbsp; if anyone raise your hand if you've ever used&nbsp; Ruby on Rails or you like Ruby on Rails. Yay.&nbsp;&nbsp; All right, we got some people in the house. So,&nbsp; this I'm I'm a Rails guy. I wrote the book The&nbsp;&nbsp; Rails way back in the day and I really like Ruby&nbsp; on Rails and it's takes a convention oriented&nbsp;&nbsp; approach. That's kind of what you get with Rast.&nbsp; So it has things like the ability to define your&nbsp;&nbsp; prompts and then put a output template alongside&nbsp; it where you you know you're able to transform&nbsp;&nbsp; the output using ERB. Very very Rails-like. So if&nbsp; you like Ruby on Rails I think you'll like Rast.&nbsp;&nbsp; Uh it looks like we have about four minutes for&nbsp; questions. So if anyone wants to step up to the&nbsp;&nbsp; mic, give you a chance. [Applause] Have you tried&nbsp; agent generating Python code to engage agent? Well, first of all, no because I&nbsp; don't write Python in principle.&nbsp;&nbsp; But the have I tried agent generated code to&nbsp; invoke an agent? Correct. Yes. So basically&nbsp;&nbsp; using Python uh either in your interpreter or&nbsp; in code execution uh to orchestrate sub agents&nbsp;&nbsp; um and through that do the same same things&nbsp; as you do like migrations and test coverage&nbsp;&nbsp; and whatnot. No. And if I understand the&nbsp; the thrust of your question correctly,&nbsp;&nbsp; I'm not I'm not sure that we w we would in the&nbsp; context of of doing roast. So the direction that&nbsp;&nbsp; we're going with roast is the introduction&nbsp; of things that you would normally associate&nbsp;&nbsp; with workflows. So the ability to put like&nbsp; control flow, conditionals, branching, looping,&nbsp;&nbsp; uh things like that which are kind of quality&nbsp; of life if you're a workflow developer. What&nbsp;&nbsp; makes it unique though is that this this is very&nbsp; much written for the AI age and for LLMs. So for&nbsp;&nbsp; instance, your conditionals uh allow you to put&nbsp; in a prompt uh or a bash script or a step you know&nbsp;&nbsp; like a fullfeatured let's call it a fullfeatured&nbsp; prompt versus an inline prompt and the results of&nbsp;&nbsp; invoking that prompt can be coerced into like for&nbsp; instance a true or false or if it's in the context&nbsp;&nbsp; of something that expects a collection to iterate&nbsp; over the result of the prompt is coerced into a&nbsp;&nbsp; list and then iterates over it. So, um, I know&nbsp; that you asked about code generation. That's a&nbsp;&nbsp; cool thing. I might actually have to think about&nbsp; that and see if, uh, it fits into the picture,&nbsp;&nbsp; but cool. Yeah. Thanks. All right. Thank&nbsp; you very much. [Music] [Applause] [Music]

---

## 13. Startups building new products with Claude

**Preview:**
> Kind: captions Language: en Welcome, welcome everyone. Um, I'm Joe. I'm&nbsp; from the startups team here at Anthropic and&nbsp;&nbsp; we're so thrilled to have all of you&nbsp; here today. We have a breakout of six&nbsp;&nbsp; amazing founders uh that have really found&nbsp; product market fit building on cloud across&nbsp;&nbsp; a bunch of different industries across agentic&nbsp; coding across design across music generation.&nbsp;&nbsp; And a lot of these startups have really seen that&nbsp; ...

**Full Transcript:**

Kind: captions Language: en Welcome, welcome everyone. Um, I'm Joe. I'm&nbsp; from the startups team here at Anthropic and&nbsp;&nbsp; we're so thrilled to have all of you&nbsp; here today. We have a breakout of six&nbsp;&nbsp; amazing founders uh that have really found&nbsp; product market fit building on cloud across&nbsp;&nbsp; a bunch of different industries across agentic&nbsp; coding across design across music generation.&nbsp;&nbsp; And a lot of these startups have really seen that&nbsp; zero to multi-million user journey within the past&nbsp;&nbsp; 12 months. And I know in this crowd there's a&nbsp; lot of, you know, really great founders too.&nbsp;&nbsp; And so we want all of you to stay until the end&nbsp; of the session. Uh we have a little surprise for&nbsp;&nbsp; all of you to be able to spin up quicker uh&nbsp; on Claude 4 um in our new family of models.&nbsp;&nbsp; So without further ado, I want to bring up Kevin&nbsp; uh who is the CEO and uh co-founder of Tempo Labs. Awesome. Thanks, Joe. Everyone, nice to meet&nbsp; you. As uh she mentioned, my name is Kevin,&nbsp;&nbsp; one of the co-founders of Tempo, and&nbsp; it's an honor to be here. Um to kick off,&nbsp;&nbsp; I have a video uh that'll give you a little&nbsp; bit of a taste of what Tempo's all about. I don't take that fact. Won't you let me go? No,&nbsp;&nbsp; you never come back. I'mma be the best&nbsp; that you never never hide. [Music]&nbsp;&nbsp; I like this. [Music]&nbsp;&nbsp; But there is one more thing. [Applause] [Music] I'm never awesome. So uh what is tempo? At tempo we're&nbsp; building cursor for PMs and designers. Uh,&nbsp;&nbsp; Tempo is a new type of IDE that feels more like a&nbsp; design tool like Figma than it does VS Code. And&nbsp;&nbsp; the whole idea behind it is to give designers&nbsp; and PMs the ability to collaborate with Claude&nbsp;&nbsp; uh to create, you know, the first draft of pull&nbsp; requests and in many cases uh the actual endto-end&nbsp;&nbsp; pull request. Uh kind of pulling the engineer out&nbsp; of the loop in some cases, but if anything, making&nbsp;&nbsp; the process of working on code collaborative as it&nbsp; stands today, it's really non-engineers don't, you&nbsp;&nbsp; know, collaborate too much on code. So, I'll show&nbsp; you a bit of a live demo of what that looks like.&nbsp;&nbsp; Uh, this is Tempo. Uh, Tempo, as you can tell, uh,&nbsp; you know, has three tabs. Product, uh, a PRD tab,&nbsp;&nbsp; a design tab, and a code tab. So, imagine you're&nbsp; a PM, right? You generate a Airbnb app with,&nbsp;&nbsp; uh, Claude, and you're like, "Okay, great. This&nbsp; looks, you know, it's a it's a it's a quick and&nbsp;&nbsp; dirty prototype demo. Uh, and then you pass it off&nbsp; to your designer, and your designer looks at uh,&nbsp;&nbsp; the real Airbnb, and it's kind of like, hm, this&nbsp; is kind of off. I need to get it pixel perfect."&nbsp;&nbsp; So, uh, you come in here and you realize that,&nbsp; uh, you know, you spent some time working on this&nbsp;&nbsp; new animated, uh, tabs header. So, you can see,&nbsp; you know, just like the it's nice and pretty,&nbsp;&nbsp; just like the core, uh, Airbnb. Uh, so what I can&nbsp; do now is I can, uh, drag and drop this right into&nbsp;&nbsp; my header over here. Uh, give that a second.&nbsp; There you go. We've got it in the header. Uh,&nbsp;&nbsp; and then what I can do is just rearrange&nbsp; this to get it in the middle. Um, and again,&nbsp;&nbsp; every change that I'm making is actually, you&nbsp; know, you can see editing the original source code&nbsp;&nbsp; in the correct position. Um, so, okay, so we're&nbsp; kind of like starting to make some progress. Now,&nbsp;&nbsp; clearly these little new badges are not great. You&nbsp; know, whenever whoever designed or created that&nbsp;&nbsp; didn't do a great job. So, why don't we fix that&nbsp; up really quick? Um, so I'm going to open up my&nbsp;&nbsp; uh DOM tree over here, which shows me everything&nbsp; that's in the DOM. I'll find the new badge, and&nbsp;&nbsp; I'll I'll just click uh delete just like I would.&nbsp; Boom. Okay, we're getting somewhere close. Looks,&nbsp;&nbsp; you know, half decent. Now, let's compare to&nbsp; Airbnb. Okay, we've got some work to do. Clearly,&nbsp;&nbsp; Claude thought we need this little text header.&nbsp; I'm just going to delete that like I normally&nbsp;&nbsp; would in a design tool, traditional design tool&nbsp; like Figma. I'm going to delete this over here.&nbsp;&nbsp; And then I am going to let's compare again.&nbsp; I'm going to move my uh the elements in my&nbsp;&nbsp; property grid here to the center. I'm going to&nbsp; add a little bit of gap between them. And then&nbsp;&nbsp; maybe here I'm going to remove this uh Tailwind&nbsp; padding. And then I'm going to compare to core&nbsp;&nbsp; Airbnb and then the site that I built in tempo.&nbsp; And now you get an idea of how you know PMs,&nbsp;&nbsp; designers can collaborate together with Claude.&nbsp; And then again I can enter a commit message,&nbsp;&nbsp; push this to uh to GitHub. Um, and actually&nbsp; the really cool thing is that this actually&nbsp;&nbsp; this code isn't running locally on my machine.&nbsp; It's actually running on a Docker container in&nbsp;&nbsp; the cloud. So this whole thing is collaborative.&nbsp; I can send a link to someone just like Figma and&nbsp;&nbsp; they can come and collaboratively code uh with&nbsp; me and with Claude. There you go. Tempo is again&nbsp;&nbsp; it's it's an IDE for designers and developers&nbsp; uh to collaborate together on code. Uh we&nbsp;&nbsp; already have uh a number of customers and have&nbsp; been growing extremely extremely quickly. Uh,&nbsp;&nbsp; and what we're seeing for customers that adopt&nbsp; Tempo is designers truly are turning into design&nbsp;&nbsp; engineers. And we're seeing that, you know,&nbsp; approximately 10 to 15% of front-end pull&nbsp;&nbsp; requests are actually being able to be opened&nbsp; by designers directly. They're often able to do&nbsp;&nbsp; uh pixel pushing themselves without&nbsp; involving an engineer. They can just&nbsp;&nbsp; work with tempo and with claude directly.&nbsp; Uh, and in in about 60% of pull requests,&nbsp;&nbsp; uh, we see that a sizable chunk of the front-end&nbsp; code has been generated by designers, by PMs, and&nbsp;&nbsp; by claude. and it actually accelerates it's useful&nbsp; code that accelerates um a true front-end engineer&nbsp;&nbsp; uh in their process of going to production. And&nbsp; so that's tempo. Uh we are in the we're imminently&nbsp;&nbsp; about to release support for cloud 4. So I invite&nbsp; you all to uh give it a shot uh and empower your&nbsp;&nbsp; PMs and your designers to start to collaborate&nbsp; with your engineers on code. Thank you. Howdy everybody. Um my name is Andrew Ph. Um&nbsp; I will start uh with a little bit of a past&nbsp;&nbsp; history. I built and sold software business for&nbsp; more than$2 billion dollars. So I ran a team of&nbsp;&nbsp; more than thousand people and despite doing that I&nbsp; could tell that only about two to 5% of our ideas&nbsp;&nbsp; ever came to life. Uh because the majority of&nbsp; the time we spent on fairly routine work and the&nbsp;&nbsp; larger is the company the more of that routine is&nbsp; in your uh kind of the bigger that routine makes&nbsp;&nbsp; part of your of your day job. So I fundamentally&nbsp; believe that uh we can automate at least 90% of&nbsp;&nbsp; that routine work giving us opportunity to move&nbsp; 10 times faster and bring more of our ideas to&nbsp;&nbsp; life and um I personally uh believe that a lot&nbsp; of their of this transformation will happen this&nbsp;&nbsp; year and we're uh actually observing it as we&nbsp; speak including today I think that uh this week&nbsp;&nbsp; actually demark uh demarks their transition into&nbsp; the next generation. So how only 12 months ago the&nbsp;&nbsp; whole industry was on the first generation of AI&nbsp; coding assistant. So a code completion it kind of&nbsp;&nbsp; uh was a very nice convenience uh but not a game&nbsp; changer to to um anybody and then October last&nbsp;&nbsp; year um thanks to Entropic we got um cloud&nbsp; 3.5 and that was a gamecher and that allowed&nbsp;&nbsp; um us and Zen coder and cursor and windsurf and a&nbsp; lot of other players to build true coding agents&nbsp;&nbsp; inside of your ID. So that opened up a completely&nbsp; new uh use cases, completely new scenarios. We saw&nbsp;&nbsp; usage skyrocket uh 10x, 100x and we saw people&nbsp; uh being able to do amazing things that they&nbsp;&nbsp; haven't been able to do before. And part of that&nbsp; revolution was um the model support for tools&nbsp;&nbsp; and environment because behind a lot of those&nbsp; generational changes there are some technical&nbsp;&nbsp; capabilities that kind of unlock all of that,&nbsp; right? uh there was also move in the models from&nbsp;&nbsp; coding quote unquote like Olympia style stuff&nbsp; to through software engineering and that was&nbsp;&nbsp; uh very very very important and then uh obviously&nbsp; context matters um software repositories are quite&nbsp;&nbsp; large so it's good to to have a larger support&nbsp; and I think that uh in this year we'll actually&nbsp;&nbsp; see transition to the next generation uh for me&nbsp; one of the biggest enablers there is verification&nbsp;&nbsp; uh so I'm super excited about something that we&nbsp; haven't yet talked uh at this conference today&nbsp;&nbsp; is computer use. We saw rapid progress in those&nbsp; models and they uh allow us to build uh proper&nbsp;&nbsp; verification and verification is key to scaling&nbsp; AI uh and kind of delivering more fully autonomous&nbsp;&nbsp; cycle. And then the other thing is uh in real&nbsp; life in actual production work uh where you got&nbsp;&nbsp; multiple people engaged and multiple teams engaged&nbsp; uh typically a lot of the time is wasted in what&nbsp;&nbsp; I call in between uh kind of passing the ball. And&nbsp; so when we think about the problem, we think about&nbsp;&nbsp; software development life cycle as a whole uh not&nbsp; just about the coding and that's a big part of our&nbsp;&nbsp; um DNA. And so super happy to announce a couple&nbsp; of weeks ago uh came with Zen agents. So taking&nbsp;&nbsp; that concept of um coding agent and extending&nbsp; into custom agents that you can share across&nbsp;&nbsp; your whole organization with full support for&nbsp; MCP with specialized coding tools and whatnot&nbsp;&nbsp; but you can basically deploy them across your&nbsp; whole uh SDLC from their uh work on your PRD&nbsp;&nbsp; to to the actual coding to verification to uh&nbsp; code reviews and whatnot. So uh that's super&nbsp;&nbsp; exciting and I'll just uh basically show you&nbsp; a quick video and we'll be good to go. Heat. [Music] Heat. [Applause] We're looking forward for Entropic to bring us&nbsp; MCP registry. Uh but while that's not available,&nbsp;&nbsp; we offer you ours. So we got 100 um I believe&nbsp; about 100 MCP servers out there and obviously&nbsp;&nbsp; you can configure your own. And also as we launch&nbsp; Zen agents, we're excited to launch the community&nbsp;&nbsp; aspects of it. So there's an uh MIT licensed&nbsp; GitHub repo where you can commit your agents&nbsp;&nbsp; and if we approve your PR uh everybody will will&nbsp; see them. So we're excited and think that that can&nbsp;&nbsp; help kickstart the creativity process in a lot of&nbsp; organizations and kind of take it beyond the basic&nbsp;&nbsp; coding use uh truly across the whole software&nbsp; development life cycle. So thank you all. Hi,&nbsp;&nbsp; my name is Jordan. I'm the head of AI&nbsp; engineering at Gamma. And at Gamma,&nbsp;&nbsp; our mission is to help bring your ideas&nbsp; to life. Whether that's presentations,&nbsp;&nbsp; documents, websites, or even social media&nbsp; carousels. We do all this through AI to help&nbsp;&nbsp; build these things for you. And we love Quad. Uh&nbsp; throughout Gamma's kind of history of using AI,&nbsp;&nbsp; there's been two moments where a model upgrade has&nbsp; made significant impact to our key metrics around&nbsp;&nbsp; user satisfaction for deck generation. The first&nbsp; was when sonnet 3.5 came out and the second was&nbsp;&nbsp; when sonnet 3.7 came out. Um we actually saw an 8%&nbsp; increase in this metric which is something that we&nbsp;&nbsp; spent hundreds of hours trying to prompt engineer&nbsp; around and could not get such improvements. Um and&nbsp;&nbsp; one of the big reasons for this was around web&nbsp; search. So the built-in tool in sonnet that we&nbsp;&nbsp; can natively use web search for and not have to&nbsp; use a third party service. So I'm going to show&nbsp;&nbsp; you the difference what web search made in gamma&nbsp; by showing two decks generated with gamma. One&nbsp;&nbsp; with web search and one without. So this is gamma.&nbsp; Gamma used AI to generate this. It used sonnet 3.7&nbsp;&nbsp; but it did not use web search. And as you can&nbsp; see got a lot of things wrong. This conference&nbsp;&nbsp; isn't January 22nd 2025. It's not three days. I&nbsp; don't think these are the right speakers. So let's&nbsp;&nbsp; see what we can do with web search. So in gamma&nbsp; you can create a entire presentation, web page,&nbsp;&nbsp; document, anything from a single sentence. So&nbsp; I'll type in code with claude conference 2025. And it's going to first create an outline for&nbsp; me. It's going to search the web to figure out&nbsp;&nbsp; what should be in this outline. and it will&nbsp; inevitably find details about the CL code with&nbsp;&nbsp; Claude conference and put them there. So, as you&nbsp; can see, we're already off to a better start where&nbsp;&nbsp; we have the correct dates, we have real technical&nbsp; sessions, we have the right location. So, this&nbsp;&nbsp; looks much better for us. And I've even gone ahead&nbsp; and created an anthropic theme that we can use. So now that we have the outline generated,&nbsp;&nbsp; Gamma will take that outline and it&nbsp; will take the other information it has&nbsp;&nbsp; and create an entire presentation for us with&nbsp; hopefully the correct details. So let's check. So, I think already we're off to a&nbsp; better start. It has the right date,&nbsp;&nbsp; has the right location, tells us the schedule, and overall, this provides us a&nbsp; much better starting point if we&nbsp;&nbsp; were to say make a presentation or&nbsp; a website about this conference.&nbsp;&nbsp; It's not going to be perfect, but it'll get&nbsp; you to a point where you have the correct&nbsp;&nbsp; information and you're at a good starting&nbsp; point to actually finish your presentation. So, this one change from sonnet 3.5 to 3.7 with&nbsp; with web search was a huge lift for us and we're&nbsp;&nbsp; very excited to continue to use claude and to&nbsp; continue to see these improvements. Lastly, we are&nbsp;&nbsp; hiring. So if you are an engineer or product or&nbsp; designer and are interested in working at Gamma,&nbsp;&nbsp; you can go to careers.gamma.app. Thank you. Hi,&nbsp; my name is Omar Goyel. I'm the CEO and co-founder&nbsp;&nbsp; of BTO. We're building the hopefully the world's&nbsp; best AI code review platform. So as we all know,&nbsp;&nbsp; we're using all these cursor windserve cla uh&nbsp; um claude code to write massive massive amounts&nbsp;&nbsp; of code. The amount of code that developers are&nbsp; writing we think is going to go up 10x over the&nbsp;&nbsp; next year or two. Um but what that means is&nbsp; that you know vibe coding which we all talk&nbsp;&nbsp; about does not equal vibe engineering. How do you&nbsp; get code that's scalable, reliable, performant,&nbsp;&nbsp; fits into your architectural design patterns and&nbsp; that's still left to the code review process. But&nbsp;&nbsp; code review is not going to scale teams have&nbsp; trouble keeping up today. It's not going to&nbsp;&nbsp; scale for 10x the amount of code. BTO's built an&nbsp; AI code review that plugs into GitHub, GitLab,&nbsp;&nbsp; Bitbucket, works with over 50 languages. Our focus&nbsp; is really, especially with Sonnet, having a really&nbsp;&nbsp; high quality model that provides a human-like&nbsp; code review focuses on the critical things. Our&nbsp;&nbsp; biggest thing is really about more signal and less&nbsp; noise. How do we provide you actionable important&nbsp;&nbsp; suggestions? Let me show it to you. So, we've got&nbsp; a a PR here. This is GitLab. It works kind of the&nbsp;&nbsp; same in in all the platforms. So once you create&nbsp; a PR uh BTO's fired bid gets called and we first&nbsp;&nbsp; start by summarizing your PR. This doesn't require&nbsp; any documentation or any comments. We just look at&nbsp;&nbsp; the diffs. We look at your code and we summarize&nbsp; it. Then we give you a quick kind of overview of&nbsp;&nbsp; what we found in the PR. So here we found three&nbsp; actionable suggestions. some things about missing&nbsp;&nbsp; resources uh a cache implementation that needs&nbsp; to be impacted and then a class cast exception&nbsp;&nbsp; error which we'll talk about. Um next we provide&nbsp; a change list. This change list gives you it's&nbsp;&nbsp; kind of a double click on the summary summarizes&nbsp; the key changes in your PR and then looks at what&nbsp;&nbsp; are the diffs that comprise that. So you as a&nbsp; reviewer you get a quick double click on what&nbsp;&nbsp; actually matters in this PR. Now let's go through&nbsp; a couple of these suggestions. So this first one&nbsp;&nbsp; is really about uh this resource management&nbsp; uh resource manager class is talking about a&nbsp;&nbsp; permanent resource leak uh if you don't fix this.&nbsp; So we give you a detailed code suggestion which&nbsp;&nbsp; you can then oneclick apply as a commit or you&nbsp; can batch those commits. Uh the next suggestion&nbsp;&nbsp; uh that we'll look at really briefly is um this&nbsp; non-thread safe static cache implementation. Uh&nbsp;&nbsp; and so we again we've given you a code suggestion.&nbsp; We're suggesting a concurrent hashmap instead of a&nbsp;&nbsp; hashmap to avoid race conditions. Now, let's talk&nbsp; a little bit about this third suggestion which I&nbsp;&nbsp; think is really important and highlights how we&nbsp; use sonnet. So, we really use sonnet to provide&nbsp;&nbsp; that human-like code review and those reasoning&nbsp; capabilities. We've done a lot of work to really&nbsp;&nbsp; understand your codebase and that's where sonnet&nbsp; kind of comes in. So, if you look here, what we've&nbsp;&nbsp; got is this network data fetcher um class. It&nbsp; has been cast to a link list. Uh this is Java. If&nbsp;&nbsp; you're not familiar with Java, I'll just kind of&nbsp; walk you through it. But really what's happening&nbsp;&nbsp; here is that this data uh you can see there's&nbsp; a data processor constructor that then cast&nbsp;&nbsp; something to an array list which is going to cause&nbsp; a class cast exception in production. Now how did&nbsp;&nbsp; how did we find this? Well, what Bid has done is&nbsp; that it looked through the code and said, "Hey,&nbsp;&nbsp; for this network data fetcher, you were trying&nbsp; to instantiate this data processor object. this&nbsp;&nbsp; data processor object. Then well, what is that? We&nbsp; went and looked, crawled your code, understood it&nbsp;&nbsp; through abstract syntax trees, a symbol index, and&nbsp; saw its reasoning and said, "Hey, this is cast as&nbsp;&nbsp; an array list." And so now this array lists, you&nbsp; know, is going to collide with this link list and&nbsp;&nbsp; going to create an exception in production. This&nbsp; is probably an error that most humans wouldn't&nbsp;&nbsp; even find, let alone, you know, most systems. Now,&nbsp; if we want to shift this left, we can even bring&nbsp;&nbsp; this to your IDE. So, here's the same changes in&nbsp; your IDE. uh you can easily kind of rightclick and&nbsp;&nbsp; say hey I want to review my local changes stage&nbsp; commits etc run review those local changes the&nbsp;&nbsp; same agent runs in your IDE again being driven&nbsp; by sonnet um and we provide the the output and&nbsp;&nbsp; then you can oneclick apply those suggestions uh&nbsp; so that's uh that's a little bit about how it how&nbsp;&nbsp; it works uh maybe I'll just mention one last&nbsp; thing so we hundreds of customers and you know&nbsp;&nbsp; the question is well what impact is this really&nbsp; having so PRs are closing in onetenth the time&nbsp;&nbsp; so that PR that used to close take 50 hours to go&nbsp; from open to merge now goes open to merge in five&nbsp;&nbsp; hours um these are three companies hundreds of&nbsp; engineers um and BTO's providing 80ish% of the&nbsp;&nbsp; feedback that a PR receives so we're taking care&nbsp; of the vast majority of the work and it's really&nbsp;&nbsp; being driven by the fact that AI provides that&nbsp; feedback in three minutes or four minutes instead&nbsp;&nbsp; instead of one or two days. Thank you. Let's talk&nbsp; about something different. So, my name is Hike and&nbsp;&nbsp; I want to talk about Refusion. So, Refusion is&nbsp; a generative music startup here in San Francisco&nbsp;&nbsp; and we work on training frontier music models and&nbsp; then create an incredible product experience for&nbsp;&nbsp; crafting and exploring the art of music. And our&nbsp; song lyrics writing pipeline is powered by Claude&nbsp;&nbsp; So our core thing that we train is a diffusion&nbsp; transformer. Um we train this from scratch and&nbsp;&nbsp; we think it's the most creative music model in the&nbsp; world. We train it for quality, for diversity, for&nbsp;&nbsp; speed, for controllability. And as a fun anecdote,&nbsp; this square of pixels here represents 30 seconds&nbsp;&nbsp; of music in our latent space. Um, it's kind of&nbsp; crazy to think about how compressed that is,&nbsp;&nbsp; but uh, writing fantastic song lyrics is a crucial&nbsp; part of great songs. Um, and current LLMs are um,&nbsp;&nbsp; good at a very many things, but writing good song&nbsp; lyrics, they're they're still pretty cringy. Um,&nbsp;&nbsp; but, uh, Claude is the best for sure. And we built&nbsp; an agent called Ghost Writer that's essentially&nbsp;&nbsp; meant to help you write and refine song lyrics as&nbsp; an artist. and basically focusing on, you know,&nbsp;&nbsp; diversity, humor, taste, what flowing with&nbsp; the music itself. And this is a crucial tool&nbsp;&nbsp; um in our product. Uh it's been used tens of&nbsp; millions of times now to to write song lyrics. So,&nbsp;&nbsp; let's go demo some stuff. U if we pop over to my&nbsp; computer. So, this is our homepage for fusion.com.&nbsp;&nbsp; Check it out. It's really fun. There's a&nbsp; lot of amazing music to explore on here.&nbsp;&nbsp; Um, I will just punch in something here. Uh,&nbsp; let's say experimental indie trip hop about&nbsp;&nbsp; the feeling of getting better after being&nbsp; really sick. There's a song concept. So,&nbsp;&nbsp; we'll pop in a couple of those. Um, in the&nbsp; meantime, let me just play a song. And there's&nbsp;&nbsp; a there's a ton of really deep editing workflows&nbsp; here for getting into stuff, but also just getting&nbsp;&nbsp; started. You can just type something in and&nbsp; start listening to music and go from there. Um,&nbsp;&nbsp; so let me just play a song from the the homepage&nbsp; and then we'll go back and listen to what we made. [Music] Don't downplay my grind. I've been&nbsp; clocking kills since the ink dried on that&nbsp;&nbsp; line. You ain't these souls now. Limited supply.&nbsp; Too many lost left cold in the breeze. Then they&nbsp;&nbsp; mama selling gumbo just to cover the fees. Please&nbsp; flip the corner cash the equity that's the creed&nbsp;&nbsp; but most is crashing out living life on the speed&nbsp; in the drop breeze thick hair like a key manifest on the scene but I'm cut different.&nbsp; [Music] All right so let's listen&nbsp;&nbsp; to some of the songs here. Got the reawakening. Let's see what the lyrics look like here.&nbsp; Written by a ghost with my muscles forotreng [Music] to move. [Music] Let's see dawn fade. [Applause] [Music] So whenever Ghost Rider&nbsp; generates we have uh kind of an iterative process&nbsp;&nbsp; of thinking about the concept of a song ideating&nbsp; about actually the context of the genre you're&nbsp;&nbsp; writing for because say the lyrics of a drum and&nbsp; bass song are super different the lyrics of a you&nbsp;&nbsp; know a folk song that might be storytelling um and&nbsp; that process of just trying to get something that&nbsp;&nbsp; actually fits with the music is is actually really&nbsp; hard and subtle and requires a lot of iteration&nbsp;&nbsp; and and refinement. So an example of some of the&nbsp; kind of deep editing stuff we can do here. So we&nbsp;&nbsp; can do remixing to create variations to extend&nbsp; from a certain section to replace a section&nbsp;&nbsp; actually to swap the stems like the vocals and&nbsp; the sound while keeping the other one the same&nbsp;&nbsp; and even capturing this idea of a vibe which is&nbsp; like a short audio snippet that you can then use&nbsp;&nbsp; to prompt instead of text. So you can mix and&nbsp; match these things and just get really deep in&nbsp;&nbsp; your refinement. And then with Ghost Rider,&nbsp; let's say if we were to use a prompt here,&nbsp;&nbsp; um, or we were to use the lyrics here, actually,&nbsp; throw them up. Then we could type in here like&nbsp;&nbsp; add a spoken word intro in French and just pop&nbsp; that in and start start iterating. So yeah,&nbsp;&nbsp; that's the that's the app and the product&nbsp; and it's it's a really fun experience and&nbsp;&nbsp; um Claude's Claude's definitely the&nbsp; best for this kind of stuff. So yeah,&nbsp;&nbsp; come talk to me. Come talk to Henry if&nbsp; you're interested in music and uh thanks. [Applause] Um hey everyone, I'm Drew. I'm the&nbsp; co-founder and CEO of Create. Uh we're an AI&nbsp;&nbsp; textto app builder that lets anybody build working&nbsp; software products um on the internet and uh the&nbsp;&nbsp; exciting thing about create is it's an AI agent&nbsp; that can take in just natural language prompts and&nbsp;&nbsp; go end to end uh on building things. So we got&nbsp; our start in web apps and claude is one of the&nbsp;&nbsp; base models that powers a lot of our code writing&nbsp; for the agent. Um, but we're really excited about&nbsp;&nbsp; um, our new mobile app builder um, that is for the&nbsp; first time letting people actually build mobile&nbsp;&nbsp; apps. And so today I thought I'd just go ahead and&nbsp; show it off. So I'll switch over to the live demo.&nbsp;&nbsp; Um, and we're in beta, but uh, somebody just&nbsp; actually emailed me an idea a few weeks ago&nbsp;&nbsp; for a family memory app that lets people uh,&nbsp; basically store their memories on their phone,&nbsp;&nbsp; uh, upload their uh, different&nbsp; images and oops, live demos. uh store their different images and uh get&nbsp; going on building their family memories app.&nbsp;&nbsp; So let me actually go create a new project and&nbsp; type this in and say make an iOS app for this. Um, and as you type creates agent starts getting&nbsp; to work, um, it's going to start building out all&nbsp;&nbsp; the core pages, um, as well as the backend&nbsp; functionality for what you need. Um, and so&nbsp;&nbsp; it will also pull in other, uh, integrations&nbsp; you you might need to use. Um, and go ahead&nbsp;&nbsp; and get set up. I, uh, am actually going to go&nbsp; ahead and switch to some pre-loaded things. So,&nbsp;&nbsp; the basic idea is you prompt, uh, and you're able&nbsp; to actually generate full applications. Um, uh,&nbsp;&nbsp; the cool thing about create. Oh, there we go.&nbsp; Starting to build a little bit. Um, is that, um,&nbsp;&nbsp; it also comes built in with backends and frontends&nbsp; and everything you kind of need from the, uh,&nbsp;&nbsp; from the database to the actual core O. Um, and so&nbsp; here it's actually building out my family memories&nbsp;&nbsp; front end. U, but then in a second, you'll&nbsp; see that it also issues the like schemas uh,&nbsp;&nbsp; for the database and goes ahead and deploys a full&nbsp; database and hooks up all the functions needed to&nbsp;&nbsp; talk to this front end. Um, we are really excited&nbsp; about the fact that non-technical users in the&nbsp;&nbsp; hundreds of thousands are starting to pour into&nbsp; create to actually fully build apps um, without&nbsp;&nbsp; needing to go to the ID and truly just work from&nbsp; promptland. Um, and so one of the biggest things&nbsp;&nbsp; that we've been working on is actually the ability&nbsp; to also fully submit from create as well. Um,&nbsp;&nbsp; create after your app is done then goes ahead&nbsp; and submits your app straight to the app store&nbsp;&nbsp; and builds a build for you. Um there's actually&nbsp; an app we built uh in a day. Um you can just in&nbsp;&nbsp; one click publish it and submit to the app store.&nbsp; Um and this will go ahead and kick off a build.&nbsp;&nbsp; Um we actually already started uh got this app in&nbsp; the app store. So if anyone here wants to download&nbsp;&nbsp; it and play with it, it's an AI app that lets&nbsp; you very quickly take drawings and turn it into&nbsp;&nbsp; uh AI images called Draw Daily. Um and then in&nbsp; the beta, the thing that's been the most exciting&nbsp;&nbsp; uh for us is just seeing all the cool apps that&nbsp; are being created. We just hosted a demo day and&nbsp;&nbsp; this is William um and he is making an app to&nbsp; help you memor memorize meaningful connections&nbsp;&nbsp; um and details of uh people's lives uh and pull&nbsp; it up. And so you just talk to the app um and&nbsp;&nbsp; it pulls up special information about your contact&nbsp; info um and uh and helps you remember anything for&nbsp;&nbsp; sales calls or anything else you want. Um or uh&nbsp; we also had a student at Berkeley who's building&nbsp;&nbsp; the scholarship app that he's always wanted um&nbsp; where he's constantly filling out grants um and&nbsp;&nbsp; unable to kind of do it automatically and so he's&nbsp; building scholar GPT um and a lot of these like&nbsp;&nbsp; the core agent of how they built were using you&nbsp; know prompt caching tool calling a lot of the&nbsp;&nbsp; core primitives that enthropic makes available to&nbsp; actually get them to success u and just to show a&nbsp;&nbsp; few more um blaze is actually a basketball coach&nbsp; and for the first time ever he's using create to&nbsp;&nbsp; get rid of all of these spreadsheets and drills&nbsp; he does for his uh coaches. Um and instead uh you&nbsp;&nbsp; build a full player coach app that lets them uh&nbsp; download uh drills uh put their lesson plan out&nbsp;&nbsp; and then uh see animations um in the app of how&nbsp; they uh should be uh getting used. Um and then&nbsp;&nbsp; finally uh we've also had personal finance apps&nbsp; being created. Um, so like a personal AI money&nbsp;&nbsp; coach for Gen Z with full rag um that also has a&nbsp; cloudpowered uh assistant that can figure out your&nbsp;&nbsp; monthly income and give you personalized financial&nbsp; recommendations. And so part of what we're like&nbsp;&nbsp; most excited about is just the explosion of who&nbsp; and how uh software will be made um when you give&nbsp;&nbsp; people these tools and very excited to partner&nbsp; with uh Claude to power all this. Thank you.

---

## 14. How students build with Claude

**Preview:**
> Kind: captions Language: en All right. Hey everybody. How are you doing? Uh&nbsp; my name is Greg. I lead student outreach here at&nbsp;&nbsp; Anthropic and I am so excited to be sharing the&nbsp; stage with some of the brightest young minds in&nbsp;&nbsp; AI. Um just a little context for this panel. So&nbsp; uh at Anthropic, we've given out uh API credits&nbsp;&nbsp; to thousands of students to help them build things&nbsp; at school. Um, and so what you're about to see is&nbsp;&nbsp; a very sma...

**Full Transcript:**

Kind: captions Language: en All right. Hey everybody. How are you doing? Uh&nbsp; my name is Greg. I lead student outreach here at&nbsp;&nbsp; Anthropic and I am so excited to be sharing the&nbsp; stage with some of the brightest young minds in&nbsp;&nbsp; AI. Um just a little context for this panel. So&nbsp; uh at Anthropic, we've given out uh API credits&nbsp;&nbsp; to thousands of students to help them build things&nbsp; at school. Um, and so what you're about to see is&nbsp;&nbsp; a very small glimpse at what students have been&nbsp; creating with those API credits. Um, it's a very&nbsp;&nbsp; wide variety of things as you're about to notice.&nbsp; Some of these projects are very humorous and&nbsp;&nbsp; funny. Some of these projects are very serious and&nbsp; important. Um, some of these students are working&nbsp;&nbsp; on one project and some of these students&nbsp; have been building an app every single week&nbsp;&nbsp; throughout all of 2025. Um, so, uh, I think if I&nbsp; was going to sum up what I've learned from running&nbsp;&nbsp; this program, it's that the the future is really&nbsp; really bright in the hands of these students. So,&nbsp;&nbsp; without further ado, I'm going to invite up&nbsp; our first speaker, Isabelle from Stanford. All right, thank you for having me. It's a&nbsp; privilege to be here. My name is Isabelle. I'm&nbsp;&nbsp; a senior at Stanford where I study aeronautics&nbsp; and astronautics and I'm doing my honors in&nbsp;&nbsp; international security and today I'm here to&nbsp; talk to you about my honors work which is on&nbsp;&nbsp; finding nuclear weapons in outer space and how I&nbsp; used Claude to help me do it. Um so for those of&nbsp;&nbsp; you that may not know article 4 of the outer space&nbsp; treaty bans the placement of nuclear weapons in&nbsp;&nbsp; outer space. Now other arms control agreements&nbsp; that you may have heard of like start and new&nbsp;&nbsp; start include provisions for verification&nbsp; and monitoring. So nations are shown to be&nbsp;&nbsp; compliant with their treaty obligations using&nbsp; inspection systems. We have on-site inspections&nbsp;&nbsp; where inspectors will go and look at each other's&nbsp; delivery vehicles and inspect for the presence of&nbsp;&nbsp; nuclear warheads. We don't have anything like&nbsp; that for outer space mostly because we signed&nbsp;&nbsp; the outer space treaty in 1967 and there were no&nbsp; technologies to do that kind of inspection. Right?&nbsp;&nbsp; How would you go about approaching a satellite&nbsp; in orbit that might be carrying a nuclear&nbsp;&nbsp; weapon and inspecting it for the presence&nbsp; of such a device? Daunting for the 1960s,&nbsp;&nbsp; daunting today. Um, and this became a problem&nbsp; recently in 2024 in April of last year. Um,&nbsp;&nbsp; the Biden administration announced that the United&nbsp; States assesses that Russia is developing a space&nbsp;&nbsp; vehicle that carries a nuclear weapon. Now, this&nbsp; was pretty destabilizing for the international&nbsp;&nbsp; community. We've had a lot of dispute in the UN&nbsp; Security Council recently about how to handle this&nbsp;&nbsp; um potential violation of the Outer Space Treaty.&nbsp; Given that we don't have a verification mechanism&nbsp;&nbsp; for compliance with the Outer Space Treaty, I&nbsp; started to wonder if it would be possible to&nbsp;&nbsp; implement such a system, particularly given that&nbsp; the US Space Force tracks 44,800 space objects&nbsp;&nbsp; today. How would you begin to know which one of&nbsp; those is the suspected nuclear weapon? So this&nbsp;&nbsp; brings me to my research question. Is it feasible&nbsp; to perform an inspace inspection mission where you&nbsp;&nbsp; inspect a target satellite for the presence of a&nbsp; nuclear warhead on board? Daunting question. Um&nbsp;&nbsp; has a lot of interesting technical and political&nbsp; facets to it. Um but for one particular aspect of&nbsp;&nbsp; it, I was able to use claude to my advantage. Um&nbsp; so I looked at the specifically the feasibility&nbsp;&nbsp; of detecting the nuclear weapon with an X-ray&nbsp; system. So you fly an X-ray source and detector on&nbsp;&nbsp; two different inspector satellites in space, have&nbsp; them rendevous with the suspected nuclear warhead&nbsp;&nbsp; target and scan it for the presence of a nuclear&nbsp; weapon in board. I wanted to know if this was ever&nbsp;&nbsp; would ever be possible. No one's ever tried using&nbsp; X-rays in space. There are interesting questions&nbsp;&nbsp; around whether the space background environment&nbsp; is there's too much noise in space to detect&nbsp;&nbsp; the source signal. Um so I built a computational&nbsp; simulation to see if this would ever be possible&nbsp;&nbsp; and to do it I used claude. I used this very&nbsp; complicated CERN um software package called Gant&nbsp;&nbsp; 4. I am not a particle physicist. I did not know&nbsp; how to approach this software package um and write&nbsp;&nbsp; this C++ code. But I was able to make a desktop&nbsp; application to do my simulation using claude. Um&nbsp;&nbsp; and it was incredibly exciting. It worked. So what&nbsp; you're seeing in this picture is like a very very&nbsp;&nbsp; quick snapshot of an X-ray image taken in space.&nbsp; Um and you see a little hole in the middle that&nbsp;&nbsp; shows you that there's very very dense file&nbsp; material on board the target of the scan. So&nbsp;&nbsp; indeed in this simulation there was a simulated&nbsp; nuclear warhead on board this satellite target.&nbsp;&nbsp; Um the outcomes for this are pretty significant&nbsp; and interesting, right? There are a lot of&nbsp;&nbsp; um people in the national security intelligence&nbsp; community in this country that are interested in&nbsp;&nbsp; developing this kind of capability to inspect&nbsp; adversary spacecraft on orbit to understand&nbsp;&nbsp; their capabilities, particularly whether they&nbsp; might carry a weapon of mass destruction. Um so&nbsp;&nbsp; having done this research, I actually am going&nbsp; to be able to brief it in Washington DC to some&nbsp;&nbsp; policy makers at the Pentagon um and state. I'm&nbsp; really thrilled about that opportunity. Um and&nbsp;&nbsp; certainly the desktop application with this level&nbsp; of fidelity would not have been possible without&nbsp;&nbsp; modern AI tools um to make this kind of research&nbsp; accessible to an undergrad in less than a year. My&nbsp;&nbsp; takeaways for you um kind of as a student um doing&nbsp; research in the era of AI is just that primarily&nbsp;&nbsp; there is no learning curve that is too steep&nbsp; any longer. Right? even the toughest problems,&nbsp;&nbsp; space technologies, notoriously hard, nuclear&nbsp; weapons, existential threats. Um, we can address&nbsp;&nbsp; these critical crises um with the tools that we&nbsp; have today with emerging technology. Um, and so I&nbsp;&nbsp; want to challenge all of the minds here and other&nbsp; students to think about like what are the world's&nbsp;&nbsp; toughest problems like what are the problems&nbsp; that you thought were unadressable um that feel&nbsp;&nbsp; like existential crises to you um for the next&nbsp; generation? Those are the ones that we should be&nbsp;&nbsp; using our brand new shiny exciting AI assistants&nbsp; um to work on um because that's how we're going&nbsp;&nbsp; to help make the world safer and more secure or&nbsp; at least outer space more secure. So, thank you. I'm going to pass it off&nbsp; to the next presenter now,&nbsp;&nbsp; but if you have any questions, I'd&nbsp; love to talk after the presentation. Okay, so it's kind of tough to follow up finding&nbsp; nuclear objects in space. So I'm going to tell you&nbsp;&nbsp; about how I did not know the difference between&nbsp; the terminal and the code editor and why claude&nbsp;&nbsp; is the reason why I was able to learn how to&nbsp; code. I'm a student at UC Berkeley. My name is&nbsp;&nbsp; Mason Arditi and I'll go ahead and get started.&nbsp; So I want to talk about what we think of as the&nbsp;&nbsp; traditional way to approach learning how to code.&nbsp; I'm going to call this the bottom up way where we&nbsp;&nbsp; start by taking basic classes, learn our basic&nbsp; skills, and then build apps with those skills.&nbsp;&nbsp; Slowly but surely, we level up our skill set and&nbsp; build apps that are more complicated. I learned a&nbsp;&nbsp; little bit differently. I'm going to call this&nbsp; the top down approach where I had an idea as I&nbsp;&nbsp; get inspired and I had no idea how to solve it. It&nbsp; was software I've never coded before. So, I try to&nbsp;&nbsp; have AI make it for me. Hey, make this app for me.&nbsp; And then when it inevitably fails, I learn how to&nbsp;&nbsp; do it myself. Slowly but surely, learning through&nbsp; different layers of abstraction until I actually&nbsp;&nbsp; understand what's going on. Now, where did this&nbsp; leave me 7 months ago? It left me not knowing&nbsp;&nbsp; what the terminal uh what the difference between&nbsp; the terminal and the code editor was. I put npx&nbsp;&nbsp; create next app latest uh in my page file. I had&nbsp; no idea what I was doing. But slowly but surely,&nbsp;&nbsp; I asked, why is this happening? What am I doing&nbsp; wrong? and I was able to learn more complicated&nbsp;&nbsp; skills. Let me show you a demo of something I'm&nbsp; capable of doing now. Okay. Welcome to CalgBt,&nbsp;&nbsp; which is a better way to schedule your Cal courses&nbsp; using AI. Asking a question like, "Show me math&nbsp;&nbsp; classes with a high average grade since I want to&nbsp; be lazy in my math and get an easy curve." Here,&nbsp;&nbsp; it's going to show us five different classes that&nbsp; have an average of A or more. And in fact, it even&nbsp;&nbsp; showed us classes with a grade point average of&nbsp; 4.0. Can't really get much better than that. Now,&nbsp;&nbsp; let's say it's getting late in the enrollment&nbsp; cycle and I want to see classes that still have&nbsp;&nbsp; open seats. Show me history classes that still&nbsp; have open seats. And this is drawing directly&nbsp;&nbsp; from Berkeley time. So, it's live data. And&nbsp; here it is. It's showing you history classes,&nbsp;&nbsp; five seats, 20 seats, or three seats. We can even&nbsp; ask it questions that are more deep like what is&nbsp;&nbsp; the meaning of life? And do with that answer as&nbsp; you will. But this is CalgBt. My name is Mason and&nbsp;&nbsp; enjoy your day. I'll show you another one which&nbsp; I developed at the pair and anthropic hackathon&nbsp;&nbsp; as well. Okay, welcome to get ready which is&nbsp; a new way to visualize and understand new code&nbsp;&nbsp; bases. Let's take a look at anthropics uh SDK&nbsp; for TypeScript for example. You'll see soon and&nbsp;&nbsp; we'll be able to interact with the chart and see&nbsp; how all of these files interact with each other.&nbsp;&nbsp; So here we have a mapping of some of the most&nbsp; important files. We chose not to display all&nbsp;&nbsp; of them. Uh just the most important ones&nbsp; that the user will interact with the most.&nbsp;&nbsp; And we have these lines to show how they're&nbsp; interconnected. And we do this through the&nbsp;&nbsp; function calls that are actually like in each&nbsp; file. So like if this demo TypeScript file is&nbsp;&nbsp; uh referencing the batch results, that's where&nbsp; the line comes in. And then over here we have&nbsp;&nbsp; just a quick description on what the file actually&nbsp; does. And we have our comments on the codebase. Okay. And on top of these two, I built many&nbsp; projects over the course of my learning how&nbsp;&nbsp; to code. Uh, now what is the point of me showing&nbsp; you all of this? I'm not here to brag. I'm here&nbsp;&nbsp; to say that Claude is the reason why I was able to&nbsp; learn how to code. Without Claude, without these&nbsp;&nbsp; AI tools, including Cursor, Windsurf, whatever&nbsp; you guys want to use, none of this would have&nbsp;&nbsp; been possible. And the key takeaway for me is&nbsp; that you can build anything you want nowadays.&nbsp;&nbsp; You just have to ask the right questions, learn&nbsp; through the different layers of abstraction. I&nbsp;&nbsp; think this is representative of a new style of&nbsp; building and a new class of builders. Uh where&nbsp;&nbsp; my flow personally is I find a problem that&nbsp; I'm inspired by and want to fix. I realize the&nbsp;&nbsp; solution is something that I have no idea how to&nbsp; do. Uh, and then I have a highle chat with Claude,&nbsp;&nbsp; execute steps in the actual editor. Uh, and then&nbsp; record and post a demo when it's not perfect,&nbsp;&nbsp; hopefully bringing users and revenue later on.&nbsp; But this iteration cycle instead of taking years&nbsp;&nbsp; for an undergraduate degree or doing other&nbsp; things uh can be one day to one week maximum&nbsp;&nbsp; if you really want to. So I'll keep it short and&nbsp; sweet and leave you guys with a couple of things&nbsp;&nbsp; to think about which are on my mind right now. uh&nbsp; which is how can we build to get users and revenue&nbsp;&nbsp; uh not for technical perfection and impressiveness&nbsp; uh how can I build things as fast and as simply&nbsp;&nbsp; as possible as demonstrated by this prompt&nbsp; uh give it to me in the simplest and most&nbsp;&nbsp; concise way possible is what I'm always&nbsp; doing uh what ideas actually inspire you&nbsp;&nbsp; and how can we build it today and lastly not&nbsp; on the slide but what does it mean to really&nbsp;&nbsp; know how to code does it mean understanding&nbsp; every single line and every single function&nbsp;&nbsp; or does it mean being able to build something&nbsp; that actually improves people's lives? Uh,&nbsp;&nbsp; I'm going to continue to post more information. If&nbsp; you want to connect with me, you can scan this QR&nbsp;&nbsp; code. Uh, but my name is M. My name is Mason.&nbsp; Uh, thank you guys. And, uh, yeah. [Applause] All right. What is up, everyone? How are we all&nbsp; doing? We good? Yeah. Um, my name is Rohill. Um,&nbsp;&nbsp; I'm a freshman or just finished freshman year at&nbsp; UC Berkeley in the ME program studying EEKES and&nbsp;&nbsp; business. So, CS and business and I'm here to&nbsp; talk to you guys today about SideQuest, which&nbsp;&nbsp; is a project that a couple friends and I made at&nbsp; uh the pair xanthropic hackathon recently. So,&nbsp;&nbsp; let me tell you guys about a big problem today&nbsp; is AI embodiment. So, we see like in hacker news,&nbsp;&nbsp; in the newest news all around that we're trying to&nbsp; create bots that interact with our world. And most&nbsp;&nbsp; recently, we've seen these robot dogs that are&nbsp; able to like deliver you a water cup or something&nbsp;&nbsp; like that. But these systems like do not compete&nbsp; with humans ourselves. Humans are like built to&nbsp;&nbsp; interact with our world. And that brings me to&nbsp; here, which is that today we have humans hiring&nbsp;&nbsp; AI agents to do their work for them. I'm sure all&nbsp; of you guys have probably employed some sort of AI&nbsp;&nbsp; agent to do your work for you. But today with Side&nbsp; Quest, we are flipping the script. And we have AI&nbsp;&nbsp; agents hiring humans to do their work for them.&nbsp; So AI agents obviously are amazing at interacting&nbsp;&nbsp; with the digital world and humans are amazing at&nbsp; dig at interacting with the physical world. So&nbsp;&nbsp; why can't these AI agents just hire the humans?&nbsp; So that brings me to the architecture of side&nbsp;&nbsp; quest which is basically like let me give you&nbsp; a hypothetical example. Let's say an AI agent&nbsp;&nbsp; is trying to host a hackathon. So now they have&nbsp; all the logistics covered, but they need to um&nbsp;&nbsp; put some advertising material up. They need some&nbsp; flyers up so that people can find out where this&nbsp;&nbsp; hackathon is, where to go. Um but they don't have&nbsp; any physical means to do that. So what they do is&nbsp;&nbsp; that they ping the nearest human to that area and&nbsp; tell them, "Oh, pick up this flyer, put it in this&nbsp;&nbsp; location, and live stream that video to me, and as&nbsp; soon as I can see that you did it, then I'll give&nbsp;&nbsp; you money." So, that's exactly what's happening&nbsp; in Side Quest. And I I'll show you a short demo. Hello world. My AI friends and I are hosting an&nbsp; hackathon. Let's check if the flyers are up. So,&nbsp;&nbsp; we see a flyer here. Flyer detected. But we do&nbsp; not see a flyer here. No flyer detected. Bro,&nbsp;&nbsp; I need a human to put on some flyers in&nbsp; room two. Let's do this. Baring baram. It looks like there's a quest. So, I have to&nbsp; collect three posters from table 8. Let's do&nbsp;&nbsp; it. So, over here there's a live video&nbsp; stream that Claude is actively looking&nbsp;&nbsp; at and verifying whether you're doing I&nbsp; found table eight. Let's see the posters. Boom. Scanned. It says I have&nbsp; to set them up in Strong. Yes. We're here at Strong. Yes. Now, let's set up&nbsp; the poster. And perfect. I think that should&nbsp;&nbsp; be good. Let's scan it. Booyah. I made 100&nbsp; bucks. Let's go. And boom. We're done. We're&nbsp;&nbsp; ready for the hackathon. Yep. And that's side&nbsp; quest. So, let me talk a little bit about what&nbsp;&nbsp; I learned with building with Claude is that&nbsp; first Claude is really smart as with like any&nbsp;&nbsp; of these AI systems these days and they can reason&nbsp; through many messy edge cases. So, we as humans,&nbsp;&nbsp; we don't need to prompt every little nitty-gritty&nbsp; thing. We can start thinking about bigger picture&nbsp;&nbsp; um parts of building products. Secondly, we&nbsp; should design um like with a back and forth&nbsp;&nbsp; workflow with these AI systems. Like originally&nbsp; we are thinking like upfront, oh, how should I&nbsp;&nbsp; build this whole big thing? But that's a really&nbsp; big task. You can break it down, ask Claude,&nbsp;&nbsp; oh like what are the different things that I need&nbsp; to do to to work on something and let's build this&nbsp;&nbsp; step by step. So with this iterative process,&nbsp; you can build um like very robust systems.&nbsp;&nbsp; So bottom line is that you should trust AI and&nbsp; trust claude. Um that they aren't things that&nbsp;&nbsp; you have to micromanage. They can think on their&nbsp; own as well. And now some takeaways for builders&nbsp;&nbsp; to be like this cool guy, not this grumpy guy.&nbsp; Is that um is that you should think of AI as&nbsp;&nbsp; a system rather than just a feature builder.&nbsp; that this is someone that you can like talk to,&nbsp;&nbsp; reason with. And secondly, as like thinking&nbsp; bigger picture about us as humans is that we&nbsp;&nbsp; should be system designers first or architects um&nbsp; of like the things that we're building because in&nbsp;&nbsp; the future we aren't going to be the ones writing&nbsp; the small code. We'll be the ones dictating what&nbsp;&nbsp; code to write. So that brings me to the end. Um&nbsp; thank you guys so much. Have a great day. Byebye. All right, good afternoon everyone.&nbsp; I'm Daniel. I study computer science&nbsp;&nbsp; at USC and I've also built projects across&nbsp; Amazon, IBM and various startups. Um, yeah,&nbsp;&nbsp; very honored to be here as a student speaker&nbsp; today. For more context, I help USC lead some&nbsp;&nbsp; of the entrepreneurship programs and over the&nbsp; past year, Claude has been integral to many&nbsp;&nbsp; of our projects, powering innovative solutions&nbsp; across various domains. When Anthropic announced&nbsp;&nbsp; the hackathon at USC, a lot of the students,&nbsp; including my teammates Vishnu, Shabbayan,&nbsp;&nbsp; and myself were naturally very eager to join in&nbsp; and explore new directions with Claude. Today,&nbsp;&nbsp; I'm honored to share our journey and insights&nbsp; with you. So, let's first start by looking at&nbsp;&nbsp; the problem. Current LLMs are great at giving&nbsp; answers, but when decisions really matter,&nbsp;&nbsp; one general response just isn't enough most of&nbsp; the time. Whether it's business, healthcare,&nbsp;&nbsp; or policy, high stakes decisions require&nbsp; diverse input and deep analysis. Today,&nbsp;&nbsp; getting those perspectives means prompting an LLM&nbsp; multiple times, which could be slow, inconsistent,&nbsp;&nbsp; and very manual. Knowing that Claude excels at&nbsp; complex reasoning as one of its most impressive&nbsp;&nbsp; capabilities, that's the gap that we aim to solve&nbsp; for our hackathon. Introducing Claude Cortex,&nbsp;&nbsp; a system designed to emulate a panel of experts.&nbsp; each analyzing the problem from a different angle.&nbsp;&nbsp; It dynamically creates specialized agents tailored&nbsp; to your problem context and enables parallel&nbsp;&nbsp; processing for diverse insights. The output&nbsp; here is a more synthesized and well-rounded&nbsp;&nbsp; recommendation enhancing output quality for&nbsp; decision-m. It's basically like having your&nbsp;&nbsp; own strategy team for each prompt. So yeah, let&nbsp; me show you how it works with a really simple&nbsp;&nbsp; example to test out the agents. So let's say I&nbsp; want to learn how to use langraph specifically&nbsp;&nbsp; by researching its documentation. I also want&nbsp; to share that finding with my teammates. I would&nbsp;&nbsp; type that in as a single prompt and let the&nbsp; master agent interpret that request and spin&nbsp;&nbsp; a different agents which in this case will&nbsp; need a browser agent to search and extract&nbsp;&nbsp; relevant information from langraph documentation.&nbsp; a research agent to summarize the key concepts in&nbsp;&nbsp; plain link language as well as a notes agent to&nbsp; generate clear explanations which it then shares&nbsp;&nbsp; with my teammates automatically. Each agent will&nbsp; work independently but they can communicate with&nbsp;&nbsp; one another creating a multi- aent system&nbsp; that gives more comprehensive insights. Now for sectors where data security and compliance&nbsp; are paramount cloud cortex offers a secured mode&nbsp;&nbsp; by integrating with AWS bedrock. It ensures that&nbsp; all operations meet privacy standards making it&nbsp;&nbsp; ideal for sensitive environments. The rest of&nbsp; our architecture is also very straightforward.&nbsp;&nbsp; The front end was built with Nex and Tailwind.&nbsp; The backend leverages fast API in lane graph&nbsp;&nbsp; for orchestrating multi-agent workflows. And&nbsp; claude of course powers our agents reasoning&nbsp;&nbsp; with the addition of browser use which allows&nbsp; agents to fetch real-time web data and enhance&nbsp;&nbsp; their analytical capabilities. Claude Cortex&nbsp; represents a shift in the way we use language&nbsp;&nbsp; models. Moving away from simply generating&nbsp; responses to structuring parallel reasoning&nbsp;&nbsp; pathways and delivering more comprehensive&nbsp; insights. It's versatile making it valuable&nbsp;&nbsp; across various sectors from corporate strategy&nbsp; to public health safety. Now the key takeaways&nbsp;&nbsp; from building cloud cortex are very intuitive.&nbsp; But the main two points here that I want to&nbsp;&nbsp; emphasize are that when agent outputs were more&nbsp; focused and well structured like JSON format um&nbsp;&nbsp; cloud synthesis became more nuanced and high high&nbsp; quality. It struggled however when upstream agents&nbsp;&nbsp; were more vague and just dumped text blobs into&nbsp; the into the uh stream and then uh dynamic task&nbsp;&nbsp; creation allows for flexibility. What that&nbsp; means is we first started off by creating&nbsp;&nbsp; um five predefined agents for every scenario.&nbsp; However, we later realized that having a master&nbsp;&nbsp; agent to decide what tasks and agents&nbsp; to create allowed for more accurate and&nbsp;&nbsp; relevant information. What we're building&nbsp; with Claude Cortex sets at a broader trend.&nbsp;&nbsp; Claude is powering a large number of studentled&nbsp; products at USC. We've seen tools for lawyers to&nbsp;&nbsp; process case files faster, apps that help people&nbsp; retain and connect knowledge more effectively,&nbsp;&nbsp; and software that can automate documentation and&nbsp; progress updates. Claude's ability to read deeply,&nbsp;&nbsp; summarize clearly, and follow structure is what&nbsp; makes all of this possible. Looking ahead, as a&nbsp;&nbsp; student building with Claude, the most powerful&nbsp; applications I've seen aren't just asking Claude&nbsp;&nbsp; for answers. They're using it as infrastructure,&nbsp; something that you can wire into your workflows&nbsp;&nbsp; and something that you can orchestrate like a&nbsp; system. And that's the shift that we see as well.&nbsp;&nbsp; We we imagine agent agents that can collaborate&nbsp; with one another, tools that can reflect and&nbsp;&nbsp; context that can compound. In summary,&nbsp; Claude Cortex isn't another AI tool. It's&nbsp;&nbsp; a leap towards a more intelligent, secure, and&nbsp; multi-dimensional decision-making process. As we&nbsp;&nbsp; continue to refine and expand its capabilities,&nbsp; we invite you to explore its potential and join&nbsp;&nbsp; us in shaping the future of AIdriven solutions.&nbsp; Here's the team behind Cloud Cortex. We're all&nbsp;&nbsp; student builders and we're all Yeah. builders&nbsp; and student leaders at USC and we would love to&nbsp;&nbsp; discuss more. So, please feel free to reach&nbsp; out to us whenever. I'm Daniel Gao and it's&nbsp;&nbsp; been a pleasure sharing our work with you.&nbsp; Thank you for your time and attention today. Thank you.

---

## 15. Vibe coding in prod

**Preview:**
> Kind: captions Language: en Hey everyone, welcome. I'm here to talk&nbsp; about everyone's uh favorite subject,&nbsp;&nbsp; vibe coding. Uh and somewhat uh controversially,&nbsp; how to vibe code in prod responsibly. So let's&nbsp;&nbsp; uh let's talk about vibe coding and like&nbsp; uh what this even is. So first of all,&nbsp;&nbsp; I'm Eric. I'm a researcher at Enthropic uh focused&nbsp; on coding agents. Uh I was the author along with&nbsp;&nbsp; Barry Zang of building effective agents where ...

**Full Transcript:**

Kind: captions Language: en Hey everyone, welcome. I'm here to talk&nbsp; about everyone's uh favorite subject,&nbsp;&nbsp; vibe coding. Uh and somewhat uh controversially,&nbsp; how to vibe code in prod responsibly. So let's&nbsp;&nbsp; uh let's talk about vibe coding and like&nbsp; uh what this even is. So first of all,&nbsp;&nbsp; I'm Eric. I'm a researcher at Enthropic uh focused&nbsp; on coding agents. Uh I was the author along with&nbsp;&nbsp; Barry Zang of building effective agents where we&nbsp; outlined uh for all of you our best science and&nbsp;&nbsp; best practices for creating agents no matter&nbsp; what the application is. Uh this is a subject&nbsp;&nbsp; that's near and dear to my heart. Uh last year I&nbsp; actually broke my hand while biking to work and&nbsp;&nbsp; was in a cast for two months and Claude wrote all&nbsp; of my code for those two months. And so figuring&nbsp;&nbsp; out how to make this happen effectively uh was&nbsp; really important to me and I was luckily able to&nbsp;&nbsp; figure that out well and sort of help u bring that&nbsp; into a lot of anthropics other products and in our&nbsp;&nbsp; models through my research. So let's first start&nbsp; talking about what is vibe coding. A lot of people&nbsp;&nbsp; really conflate vibe coding with just extensive&nbsp; use of AI to generate your code. But I think&nbsp;&nbsp; this isn't quite true. A lot of people, you know,&nbsp; they're using cursor, they're using co-pilot. Um,&nbsp;&nbsp; it's a lot of AI and a lot of the code is coming&nbsp; from the AI rather than them writing itself. But I&nbsp;&nbsp; think when you are still in a tight feedback loop&nbsp; with the model like that, that isn't truly vibe&nbsp;&nbsp; coding. When I say vibe coding, I think we need&nbsp; to go to Andre Carpathy's definition where vibe&nbsp;&nbsp; coding is where you fully give into the vibes,&nbsp; embrace exponentials, and forget that the code&nbsp;&nbsp; even exists. I think the key part here is forget&nbsp; the code even exists. And now the reason this is&nbsp;&nbsp; important is that vibe coding was when people&nbsp; outside of the engineering uh industry really&nbsp;&nbsp; started getting excited about code generation.&nbsp; Copilot and cursor were great but only sort of for&nbsp;&nbsp; uh engineers but someone that didn't know how to&nbsp; code uh suddenly with vibe coding they could find&nbsp;&nbsp; themselves coding an entire app by themselves.&nbsp; And this was a really exciting thing and a big&nbsp;&nbsp; unlock to a lot of people. Now, of course, uh&nbsp; there were a lot of downsides of this and you&nbsp;&nbsp; had people coding for the first time and really&nbsp; without knowing what they were doing at all. Um&nbsp;&nbsp; and you said, "Hey, you know, random things&nbsp; are happening, max out usage on my API keys,&nbsp;&nbsp; people are bypassing the subscription, creating&nbsp; random [&nbsp;__&nbsp;] on the DB." Uh, and so, you know,&nbsp;&nbsp; this this is kind of the downside of vibe coding&nbsp; of what started happening. And the positive sides&nbsp;&nbsp; of vibe coding that you'd see were all things&nbsp; that were really kind of low stakes. It was&nbsp;&nbsp; people building video games, building sort of fun&nbsp; side projects, things where uh it's okay if there&nbsp;&nbsp; was a bug. So, you know, why do we even care about&nbsp; vibe coding if it seems like something where the&nbsp;&nbsp; stakes are really high if you do it for a real&nbsp; product? And the most successful cases of it are&nbsp;&nbsp; kind of these toy examples or fun things where&nbsp; the stakes are very low. And my answer for why&nbsp;&nbsp; we should care about vibe coding is because of the&nbsp; exponential. The length of tasks that AI can do is&nbsp;&nbsp; doubling every seven months. Right now we're at&nbsp; about an hour. And that's fine. You don't need to&nbsp;&nbsp; vibe code. You can have cursor work for you. You&nbsp; can have clawed code write a feature that would&nbsp;&nbsp; take an hour. Um, and you can review all that code&nbsp; and you can be still be intimately involved uh,&nbsp;&nbsp; as the AI is writing a lot of your code. But&nbsp; what happens next year? What happens the year&nbsp;&nbsp; after that? When the AI is powerful enough that&nbsp; it can be generating an entire day's worth of&nbsp;&nbsp; work for you at a time or an entire week's worth&nbsp; of work, there is no way that we're going to be&nbsp;&nbsp; able to keep up with that if we still need to move&nbsp; in log stack. lock step. And that means that if we&nbsp;&nbsp; want to take advantage of this exponential, we&nbsp; are going to have to find a way to responsibly&nbsp;&nbsp; give into this and find some way to leverage this&nbsp; task. Um, I think my favorite analogy here is like&nbsp;&nbsp; compilers. I'm sure in the early day of compilers,&nbsp; a lot of developers, you know, really didn't trust&nbsp;&nbsp; them. They might use a compiler, but they'd still&nbsp; read the assembly that it would output to make&nbsp;&nbsp; sure it looks, you know, how they would write the&nbsp; assembly. But that just doesn't scale. You know,&nbsp;&nbsp; at a certain point, you start needing to work on&nbsp; systems that are big enough that you just have&nbsp;&nbsp; to trust the system. The question though is how&nbsp; do you do that responsibly? And I think sort of&nbsp;&nbsp; my challenge to the whole software industry&nbsp; over the next few years is how will we vibe&nbsp;&nbsp; code in prod and do it safely? And my answer&nbsp; to that is that we will forget that the code&nbsp;&nbsp; exists but not that the product exists. Thinking&nbsp; again to that compiler analogy, you know, we all&nbsp;&nbsp; still know that there's assembly under the hood,&nbsp; but hopefully most of us don't need to really&nbsp;&nbsp; think about what the assembly actually is. But we&nbsp; still, you know, are able to build good software&nbsp;&nbsp; without understanding that assembly under the&nbsp; hood. And I think that we will get to that same&nbsp;&nbsp; level with software. And one thing I really want&nbsp; to emphasize is that this is not a new problem.&nbsp;&nbsp; How does a CTO manage an expert in a domain where&nbsp; the CTO does not is not themselves an expert? How&nbsp;&nbsp; does a PM review an engineering feature when they&nbsp; themselves can't read all the code that went into&nbsp;&nbsp; it? Or how does a CEO check the accountant's&nbsp; work when they themselves are not an expert&nbsp;&nbsp; in financial accounting? And these are all, you&nbsp; know, problems that have existed for hundreds or&nbsp;&nbsp; thousands of years and we have solutions to&nbsp; them. A CTO can still write acceptance tests&nbsp;&nbsp; uh for an expert uh that works for them even if&nbsp; they don't understand the implementation under&nbsp;&nbsp; the hood. They can see that these acceptance tests&nbsp; pass and that the work is high quality. A product&nbsp;&nbsp; manager can use the product that their engineering&nbsp; team built and make sure that it works the way&nbsp;&nbsp; they expected uh even if they're not writing&nbsp; the code. And a CEO can spot check key facts&nbsp;&nbsp; that they do understand and slices of the data&nbsp; so that they can build confidence in the overall&nbsp;&nbsp; financial model even though they themselves might&nbsp; not be an expert in how the entire thing flows.&nbsp;&nbsp; And so thinking about these examples u managing&nbsp; implementations that you yourself don't understand&nbsp;&nbsp; is actually a problem as old as civilization. And&nbsp; every manager in the world is actually already&nbsp;&nbsp; dealing with this. Just we as software engineers&nbsp; are not used to this. We are used to being purely&nbsp;&nbsp; individual contributors where we understand&nbsp; the full depth down to the stack. But that's&nbsp;&nbsp; something that in order to become most productive,&nbsp; we are going to need to let go of in the way that&nbsp;&nbsp; every manager in order to be most productive is&nbsp; going to need to let go of some details. And just&nbsp;&nbsp; like us as software engineers, you know, we let&nbsp; go of some of the details of like understanding&nbsp;&nbsp; the assembly itself that's happening under the&nbsp; hood. And the way that you do this while still&nbsp;&nbsp; being safe and being responsible is to find an&nbsp; abstraction layer that you can verify even without&nbsp;&nbsp; knowing the implementation underneath it. Now I&nbsp; have one caveat to that today which is tech debt.&nbsp;&nbsp; So right now there is not a good way to uh measure&nbsp; or validate tech debt without reading the code&nbsp;&nbsp; yourself. Most other systems in life you know&nbsp; like the accountant example uh the PM uh you know&nbsp;&nbsp; you have ways to verify the things you care about&nbsp; without knowing the implementation. Tech I think&nbsp;&nbsp; is one of those rare things where there really&nbsp; isn't a good way to validate it other than being&nbsp;&nbsp; an expert in the implementation itself. So that is&nbsp; the one thing that right now we do not have a good&nbsp;&nbsp; way to validate. However, that doesn't mean that&nbsp; we can't do this at all. It just means we need to&nbsp;&nbsp; be very smart and targeted where aware of where&nbsp; we can uh take advantage of coding. My answer to&nbsp;&nbsp; this is to focus on leaf nodes in our codebase.&nbsp; And what I mean by that is parts of the code and&nbsp;&nbsp; parts of our system that uh nothing depends on&nbsp; them. they are kind of the end feature. They're&nbsp;&nbsp; the end beller whistle. Um rather than things that&nbsp; are the branch or trunks beneath them like here&nbsp;&nbsp; in white. Uh here the orange dots are all these&nbsp; leaf nodes where honestly if you have a system&nbsp;&nbsp; like this it's kind of okay if there is tectet in&nbsp; these leaf nodes because nothing else depends on&nbsp;&nbsp; them. They're unlikely to change. they're unlikely&nbsp; to have further things built on them versus the&nbsp;&nbsp; things that are in white here, the trunks and the&nbsp; underlying branches of your system. That is the&nbsp;&nbsp; core architecture that we as engineers still need&nbsp; to deeply understand because that's what's going&nbsp;&nbsp; to change. That's what other things are going&nbsp; to be built on and it's very important that&nbsp;&nbsp; we protect those and make sure that they stay&nbsp; extensible uh and understandable and flexible.&nbsp;&nbsp; Now the one thing I will say here is that the&nbsp; models are getting better all the time and so&nbsp;&nbsp; we might get to a world where you know this gets&nbsp; further and further down where we trust the models&nbsp;&nbsp; more and more to write code um that is extensible&nbsp; and doesn't have tech debt. Um using uh you know&nbsp;&nbsp; the clawed 4 models uh over the last week or two&nbsp; within anthropic has been a really exciting thing&nbsp;&nbsp; and I've I've given them much more trust uh than I&nbsp; did with uh 3.7. Um, so I think that this is going&nbsp;&nbsp; to change and sort of more and more of the stack&nbsp; we will be able to work with in this way. So let's&nbsp;&nbsp; talk about how to succeed at vibe coding. And my&nbsp; uh main advice here is ask not what Claude can do&nbsp;&nbsp; for you but what you can do for Claude. I think&nbsp; when you're vibe coding you are basically acting&nbsp;&nbsp; as a product manager for Claude. So you need to&nbsp; think like a product manager. What guidance or&nbsp;&nbsp; context would a new employee on your team need to&nbsp; succeed at this task? I think a lot of times we're&nbsp;&nbsp; too used to doing sort of a very quick back&nbsp; and forth chat with AI of make this feature,&nbsp;&nbsp; fix this bug, but a human if you know if it was&nbsp; their first day on the job uh and you just said,&nbsp;&nbsp; "Hey, implement you know this feature," there's no&nbsp; way you'd expect them to actually succeed at that.&nbsp;&nbsp; You need to give them a tour of the codebase. You&nbsp; need to tell them what are the actual requirements&nbsp;&nbsp; and specifications and constraints that they need&nbsp; to understand. And I think that as we vibe code,&nbsp;&nbsp; that becomes our responsibility to feed that&nbsp; information into Claude to make sure that it has&nbsp;&nbsp; all of that same context and is set up to succeed.&nbsp; When I'm working on features with Claude, I often&nbsp;&nbsp; spend 15 or 20 minutes collecting guidance into a&nbsp; single prompt and then let Claude cook after that.&nbsp;&nbsp; And that 15 or 20 minutes isn't just me, you&nbsp; know, writing the prompt by hand. This is often&nbsp;&nbsp; a separate conversation where I'm talking back and&nbsp; forth with Claude. It's exploring the codebase.&nbsp;&nbsp; It's looking for files. We're building a plan&nbsp; together that captures the essence of what I want,&nbsp;&nbsp; what files are going to need to be changed, what&nbsp; patterns in the codebase should it follow. And&nbsp;&nbsp; once I have that artifact, that all of that&nbsp; information, then I give it to Claude, either&nbsp;&nbsp; in a new context or say, "Hey, let's go execute&nbsp; this plan." And I've typically seen once I put&nbsp;&nbsp; that effort into collecting all that information,&nbsp; Claude has a very, very high success rate uh of&nbsp;&nbsp; being able to complete something in a very good&nbsp; way. And the other thing I'll say here is that&nbsp;&nbsp; you need to be able to ask the right questions.&nbsp; And uh despite the title uh of my of my talk,&nbsp;&nbsp; I don't think that vibe coding and prod is for&nbsp; everybody. I don't think that people that are&nbsp;&nbsp; fully non-technical should go and try to build&nbsp; a business fully from scratch. I think that is&nbsp;&nbsp; dangerous uh because they're not able to ask&nbsp; the right questions. They're not able to be an&nbsp;&nbsp; effective product manager for Claude when they&nbsp; do that and so they're not going to succeed.&nbsp;&nbsp; We recently merged a 22,000line change to our&nbsp; production reinforcement learning codebase&nbsp;&nbsp; that was written heavily by Claude. So how on&nbsp; earth did we do this responsibly? Uh and yes,&nbsp;&nbsp; this is the actual screenshot of like the diff uh&nbsp; from GitHub for the PR. Um the first thing is we,&nbsp;&nbsp; you know, asked what we could do for Claude. This&nbsp; wasn't just a single prompt that we then merged.&nbsp;&nbsp; There was still days of human work that went&nbsp; into this of coming up with the requirements,&nbsp;&nbsp; guiding Claude and figuring out what the system&nbsp; should be. And we really really embraced our roles&nbsp;&nbsp; as the product manager for Claude uh in this&nbsp; feature. The change was largely concentrated&nbsp;&nbsp; in leaf nodes in our codebase where we knew it&nbsp; was okay for there to be some tech debt because&nbsp;&nbsp; we didn't expect these parts of the codebase to&nbsp; need to change in the near future. And the parts&nbsp;&nbsp; of it that we did think were important that would&nbsp; need to be extensible, we did heavy human review&nbsp;&nbsp; of those parts. And lastly, we carefully designed&nbsp; stress tests for stability. Uh, and we designed&nbsp;&nbsp; the whole system so that it would have uh very&nbsp; easily human verifiable inputs and outputs. And&nbsp;&nbsp; what that let us do these last two pieces is it&nbsp; let us create these sort of verifiable checkpoints&nbsp;&nbsp; so that we could make sure that this was correct&nbsp; even without understanding or reading the full&nbsp;&nbsp; underlying implementation. Our biggest concern&nbsp; was stability and we were able to measure that&nbsp;&nbsp; even without reading the code by creating these&nbsp; stress tests and running them for long durations.&nbsp;&nbsp; Uh and we were able to verify correctness based&nbsp; on the input and outputs of the system that we&nbsp;&nbsp; designed it to have. So basically we designed&nbsp; this system to be understandable and verifiable&nbsp;&nbsp; even without without us reading all the code. And&nbsp; so ultimately by combining those things we were&nbsp;&nbsp; able to become just as confident in this change&nbsp; as any other change that we made to our codebase&nbsp;&nbsp; but deliver it in sort of a tiny fraction of the&nbsp; time and effort um that it would have taken to&nbsp;&nbsp; write this entire thing from hand uh by hand and&nbsp; review sort of every line of it. Um, and I think&nbsp;&nbsp; one of the really exciting things about this is&nbsp; is not just that this saved us, you know, a week,&nbsp;&nbsp; a week's worth of human time, but knowing that we&nbsp; could do this, it made us think differently about,&nbsp;&nbsp; you know, our engineering, about what we&nbsp; could do. And now suddenly when something&nbsp;&nbsp; costs one day of time instead of two weeks,&nbsp; you realize that you can go and make, you know,&nbsp;&nbsp; much bigger features and much bigger changes.&nbsp; uh sort of like the marginal cost of software&nbsp;&nbsp; is lower and it lets you consume and build more&nbsp; software. So I think that was the really exciting&nbsp;&nbsp; thing about this is not just saving the time but&nbsp; now kind of feeling like oh things that are going&nbsp;&nbsp; to take two weeks let's just do them. It only&nbsp; it's only going to take a day. Um and that's&nbsp;&nbsp; that's kind of the exciting thing here. So to&nbsp; leave you with the closing thoughts about how&nbsp;&nbsp; to vibe code in prod responsibly. Uh be Claude's&nbsp; PM. Ask not what Claude can do for you, but what&nbsp;&nbsp; you can do for Claude. Focus your vibe coding&nbsp; on the leaf nodes, not the core architecture and&nbsp;&nbsp; underlying systems so that if there is tech debt,&nbsp; it's contained and it's not in important areas.&nbsp;&nbsp; Think about verifiability and how you can know&nbsp; whether this change is correct without needing to&nbsp;&nbsp; go read the code yourself. And finally, remember&nbsp; the exponential. It's okay today if you don't vibe&nbsp;&nbsp; code, but in a year or two, it's going to be a&nbsp; huge huge disadvantage if you yourself are, you&nbsp;&nbsp; know, demanding that you read every single line&nbsp; of code uh or write every single line of code.&nbsp;&nbsp; You're going to not be able to take advantage of&nbsp; the newest wave of models that are able to produce&nbsp;&nbsp; very very large chunks of work for you. Uh and you&nbsp; are going to become the bottleneck if we don't get&nbsp;&nbsp; good at this. So overall that is uh vibe coding&nbsp; and prod responsibly. uh and I think this is going&nbsp;&nbsp; to become one of the biggest challenges for the&nbsp; software engineer for the software engineering&nbsp;&nbsp; industry over the next few years. Thank you. And&nbsp; I have uh plenty of time for questions. Yeah, in the past we spent a lot of time dealing with&nbsp; syntax problems or libraries or connections&nbsp;&nbsp; amongst components of the code and that was how&nbsp; we learn by coding like that. But how how do we&nbsp;&nbsp; learn now? How do we become better by coders? How&nbsp; do we know more to become better product managers&nbsp;&nbsp; of the agent KI? Yeah, so the uh I think this is&nbsp; a really interesting question and I think there&nbsp;&nbsp; are reasons to be very worried about this and also&nbsp; reasons to be very optimistic about this. I think&nbsp;&nbsp; the the reason to be worried like you mentioned&nbsp; is that you know we are not going to be there in&nbsp;&nbsp; the struggle in the grind. Um I think that that&nbsp; is actually okay. I've met uh you know some of my&nbsp;&nbsp; professors in college would say like ah man like&nbsp; coders today aren't as good because they never had&nbsp;&nbsp; to write their assembly by hand. They don't really&nbsp; feel the pain of you know how to make something&nbsp;&nbsp; run really fast. Um I think the positive side of&nbsp; this is that I have found that I'm able to learn&nbsp;&nbsp; about things so much more quickly by using these&nbsp; AI tools. A lot of times when I am coding with&nbsp;&nbsp; Claude um I'll be reviewing the code and I'll say&nbsp; hey Claude I've never seen this library before.&nbsp;&nbsp; Tell me about it. like what is it? Why did you&nbsp; choose it over another? And having sort of that&nbsp;&nbsp; always there pair programmer. Um like again I&nbsp; think what what's going to change is that people&nbsp;&nbsp; that are lazy are not going to learn. They're&nbsp; just going to glide by. But if you take the time&nbsp;&nbsp; and you want to learn, there's all these amazing&nbsp; resources and like Claude will help you understand&nbsp;&nbsp; what it vibe coded for you. Um, the other thing&nbsp; I will say is that for learning some of these&nbsp;&nbsp; higher level things about what makes a project&nbsp; go well, what is a feature that gets you product&nbsp;&nbsp; market fit versus flops, we're going to be able&nbsp; to take so many more shots on goal. I feel like&nbsp;&nbsp; uh especially sort of like system engineers or&nbsp; architects over it takes, you know, oftentimes&nbsp;&nbsp; like two years to like make a big change in a&nbsp; codebase and really kind of come to terms with&nbsp;&nbsp; was that a good architecture decision or not. And&nbsp; if we can collapse that time down to 6 months,&nbsp;&nbsp; I think engineers that are investing in their&nbsp; own time and trying to learn, they're going to be&nbsp;&nbsp; able to, you know, learn from four times as many&nbsp; lessons in the same amount of calendar time as&nbsp;&nbsp; long as they're putting in the effort to trying.&nbsp; Yeah. Going back to your pre-planning process,&nbsp;&nbsp; uh, what's the balance between giving it too much&nbsp; information and too little? Are you giving it a&nbsp;&nbsp; full product requirement document? Is there kind&nbsp; of a standardized template that you put together&nbsp;&nbsp; before you actually move into VIP coding? Yeah.&nbsp; Um, I think it depends a lot on what you care&nbsp;&nbsp; about. Um, I would say that uh if it ranges&nbsp; for there's for things where I don't really&nbsp;&nbsp; care how it does it, I won't talk at all about&nbsp; the implementation details. I'll just say these&nbsp;&nbsp; are my requirements like this is what I want&nbsp; at the end. There's other times where I know&nbsp;&nbsp; the codebase well and I will go into much more&nbsp; depth of like, hey, these are the classes you&nbsp;&nbsp; should use to implement this logic. Look at this&nbsp; example of a similar feature. Um, I'd say it all&nbsp;&nbsp; comes down to sort of what you care about at the&nbsp; end of the day. Um, I would say though that like&nbsp;&nbsp; our models do best when you don't over constrain&nbsp; them. So, um, you know, if you I wouldn't put too&nbsp;&nbsp; much effort into creating sort of a very rigorous&nbsp; uh, you know, format or anything. I would just,&nbsp;&nbsp; you know, think about it as like a junior engineer&nbsp; what you would give them in order to succeed. So, oh, sorry if I went too loud. Uh, how did&nbsp; you balance effectiveness and cyber security?&nbsp;&nbsp; Like there were reports a couple months back&nbsp; of like the top 10 vibecoded apps being super&nbsp;&nbsp; vulnerable and a lot of important information&nbsp; was released. Well, not released but proven to&nbsp;&nbsp; be released and the person who did it wasn't&nbsp; even like like a pro hacker and stuff and so&nbsp;&nbsp; like there's that. How did you balance being able&nbsp; to keep things secure even at a leaf node level&nbsp;&nbsp; uh and then also being effective because&nbsp; something can be effective but not secure? Yeah,&nbsp;&nbsp; that's a great question and I think that all&nbsp; comes down to this first point here of like&nbsp;&nbsp; being Claude's PM and understanding enough about&nbsp; the context to basically know what is dangerous,&nbsp;&nbsp; know what's safe, and know where you should be&nbsp; careful. And I think yeah, the the things that&nbsp;&nbsp; uh get a lot of press about vibe coding are&nbsp; people that have no business coding at all&nbsp;&nbsp; uh doing these. And that's fine. That's&nbsp; great for games. That's great for like&nbsp;&nbsp; uh creativity and like having people be able&nbsp; to create. But I think for production systems,&nbsp;&nbsp; you need to know enough about like what questions&nbsp; to ask to guide Claude in the right direction. And&nbsp;&nbsp; for our internal case of this this example, um&nbsp; it was something that's fully offline. And so&nbsp;&nbsp; we knew there weren't any like there were uh we&nbsp; were very very confident that there was like no&nbsp;&nbsp; security problems that could happen into this.&nbsp; Uh in our case it's like run in something that's&nbsp;&nbsp; that's fully offline. Uh so this is more about&nbsp; people you're mentioning as like have no business&nbsp;&nbsp; and maybe I shouldn't have said it like that&nbsp; but no business vibe coding in production for&nbsp;&nbsp; an important system. I will say I will say that.&nbsp; Yeah. But but if if we look at the numbers right&nbsp;&nbsp; we Less than 0.5% of the world's population are&nbsp; software developers and software is an amazing way&nbsp;&nbsp; to scale ideas. So how do you think the products&nbsp; need to change to make it easier for people to&nbsp;&nbsp; v code and build software while also avoiding&nbsp; some of the things that we run into with people&nbsp;&nbsp; leaking API keys and things like that? That's a&nbsp; really great question and I would be super excited&nbsp;&nbsp; to see more products and frameworks emerge that&nbsp; are kind of like provably correct. Uh, and maybe&nbsp;&nbsp; what I mean by that is I'm sure people could build&nbsp; some backend systems that the important off parts,&nbsp;&nbsp; the payment parts are built for you and all you&nbsp; have to do is sort of fill in the UI layer. Um,&nbsp;&nbsp; and you know, you can vibe code that and it&nbsp; basically gives you some nice fill-in-the-blank&nbsp;&nbsp; sandboxes where to put your code. Um, I feel like&nbsp; there's tons of things like that that could exist.&nbsp;&nbsp; And maybe like the simplest example is like claude&nbsp; artifacts where uh Claude can help you write,&nbsp;&nbsp; you know, code that gets hosted right there um&nbsp; in Claude AI to display. And of course that is&nbsp;&nbsp; safe because it is very limited. There is no off&nbsp; there is no payments. It's it's front end only.&nbsp;&nbsp; But uh maybe that's a good like product idea&nbsp; that someone should do here is is build some&nbsp;&nbsp; way to make like a provably correct hosting system&nbsp; that can have a backend that you know is safe no&nbsp;&nbsp; matter what shenanigans happens on the front end.&nbsp; But yeah, I hope people build good tools that are&nbsp;&nbsp; complements to vibe coding. Hi. Um so for test&nbsp; driven development, do you have any tips because&nbsp;&nbsp; like I often see that cloud just splits out the&nbsp; entire implementation and then writes test cases.&nbsp;&nbsp; um sometimes they don't they fail and then I just&nbsp; want you know I'm trying to prompt it to write the&nbsp;&nbsp; test cases first but I also don't want to like you&nbsp; know verify them by myself because I haven't seen&nbsp;&nbsp; implementation yet so do you have an iteratable&nbsp; approach that you know have you ever tried it&nbsp;&nbsp; for yeah test driven development yeah yeah I I&nbsp; definitely uh test driven development is very&nbsp;&nbsp; very useful in vibe coding um as long as you can&nbsp; understand what the test cases are even without&nbsp;&nbsp; that it helps claude sort of be a little bit more&nbsp; self consistent even if you yourself don't look&nbsp;&nbsp; at the tests. Um, but a lot of times, uh, I'd say&nbsp; it's easy for Claude to go down a rabbit hole of&nbsp;&nbsp; writing tests that are like too implementation&nbsp; specific. Um, when I'm trying to do this,&nbsp;&nbsp; a lot of times I will encourage I will give Claude&nbsp; examples of like, hey, just write three endto-end&nbsp;&nbsp; tests and, you know, do the happy path, an&nbsp; error case, and this other error case. Um,&nbsp;&nbsp; and I'm kind of like very prescriptive about that.&nbsp; I want the test to be like general and end to end.&nbsp;&nbsp; And I think that helps make sure it's something&nbsp; that I can understand um, and it's something that,&nbsp;&nbsp; um, uh, that Claude can do without getting too in&nbsp; the weeds. I'll also say a lot of times uh when&nbsp;&nbsp; I'm vibe coding the only part of the code or at&nbsp; least the first part of the code that I'll read&nbsp;&nbsp; is the tests to make sure that you know if I&nbsp; agree with the tests and the tests pass then&nbsp;&nbsp; I feel pretty good about the code. Um that works&nbsp; best if you can encourage Claude to write sort of&nbsp;&nbsp; very minimalist endto-end tests. Thank you for the&nbsp; very fascinating talk. Um, I also appreciate that&nbsp;&nbsp; you've done what a lot of people haven't done and&nbsp; tried to interpret one of the more peculiar lines&nbsp;&nbsp; in Karpathy's original post, embrace exponentials.&nbsp; So, I wonder if I could pin you down a little more&nbsp;&nbsp; and say, how would I know if I've embraced the&nbsp; exponentials? Like, what precisely means following&nbsp;&nbsp; that advice? And and to maybe put it down a&nbsp; little more in what I think it intends to mean,&nbsp;&nbsp; it sort of maybe alludes to this, the models&nbsp; will get better. Um, do you think there's&nbsp;&nbsp; some legitimacy in saying just the fact that the&nbsp; models will get better doesn't mean they'll get&nbsp;&nbsp; better at every conceivable dimension we might be&nbsp; imagining we hope they'll they'll be in. Yeah. Uh,&nbsp;&nbsp; so yeah. So how do I embrace exponentials,&nbsp; sir? Yeah, absolutely. So the uh I think you&nbsp;&nbsp; got close with sort of the the quote of uh keep&nbsp; assuming the models are going to get better,&nbsp;&nbsp; but it's a step beyond that. the the idea of&nbsp; the exponential is not just that they're going&nbsp;&nbsp; to keep getting better, but they're going to get&nbsp; better faster than we can possibly imagine. Um,&nbsp;&nbsp; and that's kind of like when you you can kind of&nbsp; see the shape of the dots here. It's it's not just&nbsp;&nbsp; that it's getting steadily better, it's that it's&nbsp; getting better and then it's it goes wild. Um,&nbsp;&nbsp; I think the other funny quote I heard from this&nbsp; this was a I think in uh Daario and Mike Kger's&nbsp;&nbsp; talk is uh machines of loving grace is not science&nbsp; fiction. It's a product roadmap. uh even though&nbsp;&nbsp; it sounds like something that's very far out like&nbsp; when you are on an exponential uh things get wild&nbsp;&nbsp; very very fast and faster than you expect. Um, and&nbsp; I think, you know, if you if you talk to someone&nbsp;&nbsp; that was doing computers in the 90s, it's like,&nbsp; okay, great. We have a couple kilobytes of RAM.&nbsp;&nbsp; We have a couple more kilobytes of RAM. Uh, but&nbsp; if you fast forward to where we are now, it's like&nbsp;&nbsp; we have terabytes. And it's like, it's not just&nbsp; that it got twice as good, it's that things got&nbsp;&nbsp; millions of times better. And that's what happens&nbsp; with exponentials over a course of 20 years. So,&nbsp;&nbsp; we shouldn't think about 20 years from now is like&nbsp; what happens if these models are twice as good.&nbsp;&nbsp; We should think about what happens if these models&nbsp; are a million times smarter and faster than they&nbsp;&nbsp; are today, which is wild. Like I we can't even&nbsp; think about what that means. In the same way that&nbsp;&nbsp; someone working on computers in the 90s, I don't&nbsp; think they could think about what would happen to&nbsp;&nbsp; society if a computer was a million times faster&nbsp; than what they were working with. But that's what&nbsp;&nbsp; happened. And so that's what we mean by the&nbsp; exponential is it's going to go bonkers. All&nbsp;&nbsp; right. Yes. I got a couple well I got one question&nbsp; but it it's kind it's kind of two parts. The first&nbsp;&nbsp; part when it comes to via coding I have like two&nbsp; different workflows. I have one where I'm in my&nbsp;&nbsp; terminal and then I have one when I'm in VS code&nbsp; or cursor. Um which which workflow do you use and&nbsp;&nbsp; if you're using cloud code in the terminal how&nbsp; often do you compact? Because what I find is&nbsp;&nbsp; um my functions will get a new name as the longer&nbsp; I vibe code or you know just things kind of go&nbsp;&nbsp; off the rails the longer I go and if I compact&nbsp; it still happens if I create like a document to&nbsp;&nbsp; kind of guide it I still have to you know get it&nbsp; back on track. Yeah. Yeah. Great question. Um I do&nbsp;&nbsp; both. I often uh code with uh clawed code open in&nbsp; my terminal in VS Code. Um, and I'd say that like&nbsp;&nbsp; clawed code is doing most of the editing and I'm&nbsp; kind of reviewing the code um as I go in uh in VS&nbsp;&nbsp; Code, which you know is not true vibe coding in&nbsp; the sense here. Uh, or maybe I'm reviewing just&nbsp;&nbsp; the tests uh from it. Um, I like to compact&nbsp; or just start a new session kind of whenever&nbsp;&nbsp; I get clawed to a good stopping point where it&nbsp; kind of feels like, okay, as a human programmer,&nbsp;&nbsp; like when would I kind of stop and take a break&nbsp; and maybe like go get lunch and then come back.&nbsp;&nbsp; If I feel like I'm at that kind of stage, that's&nbsp; like a good time to compact. So maybe I'll start&nbsp;&nbsp; off with having Claude find all the relevant files&nbsp; and and make a plan and then I'll say okay like&nbsp;&nbsp; you know write all this into a document and then&nbsp; I'll compact and that gets rid of 100k tokens that&nbsp;&nbsp; it took to create that plan and find all these&nbsp; files and boils it down to a few thousand tokens. Hey uh so one question is following up uh his&nbsp; previous question which is uh have you used&nbsp;&nbsp; other tools along with cloud code to like increase&nbsp; your speed a little bit more like running multiple&nbsp;&nbsp; cloud codes together using git work trees and then&nbsp; like sort of merging few things or stack PRs or&nbsp;&nbsp; something like that. Is that something that you&nbsp; like personally follow or would advise to? Second&nbsp;&nbsp; question is um how do you like how do you very&nbsp; structurally and like in a very nice engineering&nbsp;&nbsp; like um like way approach a part of the codebase&nbsp; that you're not very familiar with but you want to&nbsp;&nbsp; like ship a PR in it really fast and you want to&nbsp; do it in a really nice way and not wipe code it.&nbsp;&nbsp; So yeah like what what are the what are your ways&nbsp; of like using cloud code to help do both these&nbsp;&nbsp; things? Yep. Uh, yeah. So, I definitely use clawed&nbsp; code as well as cursor. Um, and I'd say typically&nbsp;&nbsp; I'll like start things with clawed code and then&nbsp; I'll use cursor to fix things up. Or if I was&nbsp;&nbsp; like if I have very specific changes, if I know&nbsp; exactly the change that I want to do to this file,&nbsp;&nbsp; I'll just do it myself uh with cursor and sort of&nbsp; target the exact lines that I know need to change.&nbsp;&nbsp; Um the second part of your question was um oh&nbsp; yeah like uh how to get spun up on a new part&nbsp;&nbsp; of the codebase. Um before I start trying to write&nbsp; the feature I use clawed code to help me explore&nbsp;&nbsp; the codebase. So I might say like tell me where&nbsp; in this codebase off happens or you know where in&nbsp;&nbsp; this codebase something happens. Tell me similar&nbsp; features to this and like have it tell me the file&nbsp;&nbsp; names. Have it tell me the classes that I should&nbsp; look at. Um, and then kind of use that to try to&nbsp;&nbsp; build up a a mental picture to make sure that I&nbsp; can do this and not vibe code. Make sure I can&nbsp;&nbsp; still get like a a good sense of what's happening.&nbsp; And then I go work on the feature with Claude. Uh,&nbsp;&nbsp; thank you so much. I'll be still around&nbsp; and can Miller and answer other questions.

---

## 16. Building Blocks for Tomorrow’s AI Agents

**Preview:**
> Kind: captions Language: en [Music] [Music] [Applause] good afternoon and welcome back from lunch you&nbsp; know whenever I do a tech conference I always ask&nbsp;&nbsp; to do the session right after a lunch because I&nbsp; know only the most motivated hardworking smartest&nbsp;&nbsp; most beautiful people come right am I right so&nbsp; thank you thank you for being here um I'm Brad&nbsp;&nbsp; Abrams i'm a product manager at Enthropic uh and&nbsp; we're going to talk about components for buildi...

**Full Transcript:**

Kind: captions Language: en [Music] [Music] [Applause] good afternoon and welcome back from lunch you&nbsp; know whenever I do a tech conference I always ask&nbsp;&nbsp; to do the session right after a lunch because I&nbsp; know only the most motivated hardworking smartest&nbsp;&nbsp; most beautiful people come right am I right so&nbsp; thank you thank you for being here um I'm Brad&nbsp;&nbsp; Abrams i'm a product manager at Enthropic uh and&nbsp; we're going to talk about components for building&nbsp;&nbsp; agents today you saw Michael in the keynote talk&nbsp; about our Anthropic developer platform here and&nbsp;&nbsp; today we're going to build uh drill into this&nbsp; agenic components when we think about building&nbsp;&nbsp; agents there's really three key parts of this&nbsp; first is fundamentally building the agent and&nbsp;&nbsp; starting with our foundational models with the&nbsp; Claude 4 family of models with enhanced reasoning&nbsp;&nbsp; memory support much improved tool calling and long&nbsp; range planning is a great way to start there's&nbsp;&nbsp; also a set of components that you can reuse that&nbsp; saves your precious engineering resources to work&nbsp;&nbsp; on different things but we know regardless of how&nbsp; good our models are they're only as intelligent&nbsp;&nbsp; is the data that you bring to them and that's&nbsp; what the connect pillar is all about how can we&nbsp;&nbsp; help you bring more context in that helps the&nbsp; intelligence of the model and finally none of&nbsp;&nbsp; that matters if you can't deliver a service that's&nbsp; reliable that's stable that's performant uh and&nbsp;&nbsp; cost-effective and that's what optimize is so this&nbsp; is sort of our agenda for today let's drill into&nbsp;&nbsp; build so with build I want to talk about the code&nbsp; execution tool customers have told us that while&nbsp;&nbsp; uh large language models can do many amazing&nbsp; things there's still some tasks that require&nbsp;&nbsp; uh traditional software development when&nbsp; you're doing advanced data analytics you&nbsp;&nbsp; have a giant spreadsheet need to understand&nbsp; uh do deep analysis of that data that's still&nbsp;&nbsp; the domain where a human might need to write code&nbsp; because that code is auditable uh it's performant&nbsp;&nbsp; uh it's repeatable it does the same every time&nbsp; so um so some some of those use cases are still&nbsp;&nbsp; better done with code but you know our models&nbsp; are actually pretty good at writing code so we&nbsp;&nbsp; thought why not give Claude a computer and let&nbsp; it write and execute that code and that's what&nbsp;&nbsp; code execution is all about let me explain it by&nbsp; drilling in uh one level deep on an example so&nbsp;&nbsp; we have a client here it calls claude and and&nbsp; then that goes to a container we have a whole&nbsp;&nbsp; uh set of containers so every organization gets um&nbsp; a ded a dedicated container um and here the client&nbsp;&nbsp; is actually requesting container ID one your&nbsp; client can decide uh how to use the containers&nbsp;&nbsp; how to allocate them uh the client has this prompt&nbsp; i don't know if anybody's already figured out the&nbsp;&nbsp; answer to this i'll let you noodle on that for&nbsp; a second um Claude uh thinks about that for a&nbsp;&nbsp; minute and decides you know actually this is be&nbsp; best best done by writing code so Claude chooses&nbsp;&nbsp; the code execution tool writes a set of Python&nbsp; code that will answer that question um and then&nbsp;&nbsp; we hand it over to the container the container&nbsp; executes that and then we get some uh results back&nbsp;&nbsp; so all of standard out comes back standard error&nbsp; comes back uh and any files that were created in&nbsp;&nbsp; while executing on that container come back and&nbsp; then the model then reasons over that results and&nbsp;&nbsp; comes up with a quippy answer so the answer was&nbsp; 42 and Opus with its uh insightful humor here has&nbsp;&nbsp; come up with a good good joke about that so that's&nbsp; generally how code execution works and it's very&nbsp;&nbsp; simple to set up those of you that are already&nbsp; customers will recognize the messages API it's&nbsp;&nbsp; the core way to use our models so it's the exact&nbsp; same API you've been using before we've just added&nbsp;&nbsp; a new tools block um and keep in mind this is&nbsp; really all you need to do to to set this up it's&nbsp;&nbsp; uh one method call brings all of this power and&nbsp; that's just what Shopify found was interesting&nbsp;&nbsp; uh as they experiment with this with this&nbsp; code execution tool we're building they have a&nbsp;&nbsp; uh sidekick agents that helps merchant&nbsp; merchants build their storefronts and&nbsp;&nbsp; they're building AB testing experience there&nbsp; and having the power of this code execution&nbsp;&nbsp; tool is helping them bring that insight so to&nbsp; really understand let's switch over to a demo&nbsp;&nbsp; yeah let's switch over to the demo so doing&nbsp; a demo in a tech conference at any time is a&nbsp;&nbsp; harrowing experience but when you're launching a&nbsp; brand new model with a bunch of new features uh&nbsp;&nbsp; in front of a live stream it's it's particularly&nbsp; crazy so hopefully this will work well so what&nbsp;&nbsp; we have here we've uh Thank you thank you we we&nbsp; have uh vibecoded a little command line client&nbsp;&nbsp; just to explain how the system works very very&nbsp; basic system here and we're using um opus 4 uh&nbsp;&nbsp; so what I'm going to do here is just give a very&nbsp; simple query here i'll let you think about what&nbsp;&nbsp; the answer to this one is so we pass this query&nbsp; to claude 4 and it has the code execution tool&nbsp;&nbsp; enabled um Claude's going to reason about that&nbsp; for a second decide to call the code execution&nbsp;&nbsp; tool and then we get streaming results so this is&nbsp; one one uh HTTP call but we're getting streaming&nbsp;&nbsp; responses back the code gets written by the model&nbsp; passed to the tool the tool is executed the the&nbsp;&nbsp; code and we got that standard out there that's the&nbsp; 100th prime number and then the model gives its&nbsp;&nbsp; its quippy answer here thank you first demo worked&nbsp; i'm feeling I'm feeling good let's let's push it&nbsp;&nbsp; a little bit harder okay so I have some AB test&nbsp; results here uh I have to make uh shop my Shopify&nbsp;&nbsp; friends happy um I have some AB test results here&nbsp; i've uploaded those with the files API which we&nbsp;&nbsp; also announced today uh and then what I'm going&nbsp; to do now is do some uh analysis of it so you&nbsp;&nbsp; can see this prompt says analyze the uploaded AB&nbsp; tests and compare control and treatment calculate&nbsp;&nbsp; the statistical significance and key metrics and&nbsp; make a recommendation so using all parts of the&nbsp;&nbsp; model here we're doing some code execution notice&nbsp; in this first turn the model has never seen this&nbsp;&nbsp; this spreadsheet before so it first has to analyze&nbsp; the types in the spreadsheet what's there it gets&nbsp;&nbsp; those results back and then since it understands&nbsp; them it now writes deeper code to go understand&nbsp;&nbsp; uh what's really happening here and pull out some&nbsp; insights so we're now we're executing that code&nbsp;&nbsp; on the VM um and we get all the results back in&nbsp; standard out and I got to say I just love Opus&nbsp;&nbsp; because it doesn't give up he didn't get exactly&nbsp; what he wanted out of that analysis so it's look&nbsp;&nbsp; I need to drill in more i need to understand&nbsp; a little bit more before I can do this so it&nbsp;&nbsp; um drills in more uh writes some additional&nbsp; data analysis code and see here it's writing&nbsp;&nbsp; the output for itself to read so all these&nbsp; print statements are going to come back&nbsp;&nbsp; to standard out and then we're going to pass&nbsp; that to the model so those came back and now&nbsp;&nbsp; the model is reasoning about what its response&nbsp; is and it makes that business recommendation&nbsp;&nbsp; that we asked for and justifies it with the&nbsp; analysis of all that data so pretty pretty good great let's switch back to the slides&nbsp; okay so that's the first live demo great&nbsp;&nbsp; code execution tool um the codeex code&nbsp; execution tool is an enthropic hosted&nbsp;&nbsp; computing environment um and it's flexible&nbsp; developer controlled so you don't need to&nbsp;&nbsp; tie to threads or whatever like devel you can&nbsp; control which request goes to what container&nbsp;&nbsp; and your containers are isolated from everybody&nbsp; else's um and you get 50 free container hours&nbsp;&nbsp; today which is a good amount to get started&nbsp; and then love to scale with you so we have&nbsp;&nbsp; some pricing to to let that scale and the&nbsp; best part is that's available for you to use today wow this is a good audience good okay let's&nbsp; move on and talk about the connect uh pillar here&nbsp;&nbsp; how you can bring data into the into the model&nbsp; so many of our customers have told us that again&nbsp;&nbsp; while the model's reasoning is great it was&nbsp; trained at some specific time um and maybe they&nbsp;&nbsp; need more recent information whether that's for&nbsp; financial use cases say the latest stock prices&nbsp;&nbsp; or for in the legal case maybe there's some case&nbsp; law rulings that need to be kept up to date or&nbsp;&nbsp; um even in coding you may need to get the latest&nbsp; API documentation to make sure your code works&nbsp;&nbsp; uh beautifully so in all those cases that real-&nbsp; time information is is very important and that's&nbsp;&nbsp; the role that web search plays so let's drill in&nbsp; uh in a similar example and check in how how web&nbsp;&nbsp; search works in our system so again I have the&nbsp; same sort of setup a client gives this prompt&nbsp;&nbsp; um what are the most significant technological&nbsp; breakthroughs announced in the past three months&nbsp;&nbsp; and what publicly traded companies uh would&nbsp; benefit from them so that's actually a pretty&nbsp;&nbsp; complicated problem i mean you might pay an&nbsp; analyst money to actually answer this question&nbsp;&nbsp; what Claude does is it doesn't transform that&nbsp; prompt into a query it actually reasons about&nbsp;&nbsp; the overall task that you've been uh asked claude&nbsp; to do and it decides well the first thing I need&nbsp;&nbsp; to do is do a broad query and really understand&nbsp; the technological breakthroughs um it issues&nbsp;&nbsp; that query we pass it to a search engine and&nbsp; get a set of search results back so just think&nbsp;&nbsp; about the standard tin blue links so we get&nbsp; title URL and content from each of the websi&nbsp;&nbsp; uh these uh websites and so all that context goes&nbsp; into the model and then the model says okay given&nbsp;&nbsp; the prompt that I was given and this additional&nbsp; context what do I need to do now so it says okay&nbsp;&nbsp; well I need to drill into one of those particular&nbsp; trends so it it um picks this small language model&nbsp;&nbsp; drills in and and finds companies that are related&nbsp; to that gets the same search results and content&nbsp;&nbsp; back into the model and then the model decides&nbsp; again to do another search now this one I don't&nbsp;&nbsp; really know is a a techn a trend that I'm aware of&nbsp; but we learn new things with Claude every day so&nbsp;&nbsp; it's really fantastic um and keep in mind all&nbsp; of these things are happening in that one API&nbsp;&nbsp; call that I showed you one API call and all the&nbsp; you get all of this power so we get the search&nbsp;&nbsp; results back um and then this is an interesting&nbsp; case it does this one final search um to to go&nbsp;&nbsp; one level deeper into the small language models to&nbsp; get a really deep insight into this particular one&nbsp;&nbsp; um and gets the results from that and then it&nbsp; produces uh its report so it reports a complete&nbsp;&nbsp; report and all of the data is now cited so there's&nbsp; actually footnotes citations for every fact so&nbsp;&nbsp; that you can go back and verify make sure there's&nbsp; no hallucinations and it is exactly what you want&nbsp;&nbsp; uh and this is uh again as I mentioned one API&nbsp; call messages API and there's a tool very similar&nbsp;&nbsp; to the code execution tool we showed earlier um&nbsp; in this case you can actually restrict the domain&nbsp;&nbsp; so say you're building uh customer support agents&nbsp; you might want to restrict the domain to just the&nbsp;&nbsp; uh just one domain so you get accurate answers&nbsp; and you can also control the max turns if you&nbsp;&nbsp; want to be a little conservative on how many&nbsp; tokens you spend although it's my business&nbsp;&nbsp; that you spend a lot of tokens so feel free&nbsp; um okay and that's what Kora has really found&nbsp;&nbsp; interesting they're building a consumer agent and&nbsp; they really value that live upto-date information&nbsp;&nbsp; because consu consumers oftentimes ask about&nbsp; what's going on uh contemporarily and and so&nbsp;&nbsp; they're they're really valuing that and again&nbsp; we're seeing customers across legal coding tools&nbsp;&nbsp; um find this very valuable so let's do another&nbsp; demo so let's switch back to the demo machine okay&nbsp;&nbsp; so now let's try a search query um what are the&nbsp; bench scores for all of anthropics models since&nbsp;&nbsp; 35 uh so this is a a contemporary one this is one&nbsp; that's sort of real time right now so we'll see&nbsp;&nbsp; how well this this works um so what it's doing now&nbsp; is it's actually considering that query looking at&nbsp;&nbsp; all the tools it has available um you know we saw&nbsp; before it whoa that's good times um let's try that&nbsp;&nbsp; one again we know it's real if um so it's looking&nbsp; at all the tools it has available and deciding&nbsp;&nbsp; which one to call um so in this case it calls the&nbsp; search tool and it starts with 35 because that's&nbsp;&nbsp; the data that we gave it so it does that search&nbsp; for 35 um and then it does a more general search&nbsp;&nbsp; so Claude's not satisfied with the answer from&nbsp; the first turn so we're not like structuring oh&nbsp;&nbsp; do three searches these are the searches claude's&nbsp; deciding what searches to do how many of searches&nbsp;&nbsp; when to stop what to drill in on um and here&nbsp; it looked like it it this gets to get updated&nbsp;&nbsp; we don't quite have the Sonnet four scores up but&nbsp; they will be there very soon okay but can we put&nbsp;&nbsp; these things together um so this is a question&nbsp; I have wondered with for a long time and that's&nbsp;&nbsp; how many elephants can travel over the Golden Gate&nbsp; Bridge in an hour if you think about this question&nbsp;&nbsp; uh which is a very important question you know&nbsp; there's actually some pieces of data you need&nbsp;&nbsp; to know you might need to know um how what the&nbsp; weight capacity of the Golden Gate Bridge is&nbsp;&nbsp; uh the walking speed of an elephant but then&nbsp; even once you have that data you need to do&nbsp;&nbsp; some computational work to go understand so&nbsp; Claude's doing all the did all the searches&nbsp;&nbsp; now it's doing the computational work or at&nbsp; least it's writing the code for that and as&nbsp;&nbsp; soon as it finishes writing the code um it will&nbsp; we pass this over to the code execution tool and&nbsp;&nbsp; it will execute that code there we go it executes&nbsp; it um and gets this data and that goes back into&nbsp;&nbsp; the model uh the model sees that data and it&nbsp; forms its answer which is 7,000 can go does&nbsp;&nbsp; that that seems plausible right okay so that's&nbsp; uh web search with with code executor it's good okay okay uh let's go back to slides if&nbsp; we can okay so that's web search um and&nbsp;&nbsp; it's it's anthropic's agentic search&nbsp; capability so it's not just simply&nbsp;&nbsp; passing a search query and getting a&nbsp; result the model is actually deciding&nbsp;&nbsp; uh how to search how many times to search doing&nbsp; that loop over and over again uh and that's&nbsp;&nbsp; done with our citation support so everything&nbsp; there is fully grounded and auditable and it's&nbsp;&nbsp; uh highly composable developer controlled so it's&nbsp; very easy to add you can have a lot of controls as&nbsp;&nbsp; developers and then it's reasonably priced so you&nbsp; can you can use that at scale and that's available today so let's talk about MCP connector next&nbsp; uh we've just been blown away at the industry&nbsp;&nbsp; excitement around MCPs i see a new MCP launch&nbsp; literally every day so that ecosystem is growing&nbsp;&nbsp; uh very quickly and in fact just last two week&nbsp; last week uh Claude Aai launched support for&nbsp;&nbsp; remote MCPs and claude AAI and many of our&nbsp; customers have been wondering how they can&nbsp;&nbsp; take advantage of that ecosystem of MCPs within&nbsp; their own agents and MCP connector is the answer&nbsp;&nbsp; to that so let's take a look at how that looks&nbsp; under the covers so this is a little bit more&nbsp;&nbsp; uh complicated setup we have our client and we&nbsp; have three different MCPs connected uh and that's&nbsp;&nbsp; because we're serving query our agent's going&nbsp; to serve queries like like this one that like&nbsp;&nbsp; a product manager might need to do for a team&nbsp; after a after a launch uh create an email with&nbsp;&nbsp; a creative and motivational image about my Asana&nbsp; project status and send it to the team so there's&nbsp;&nbsp; several components here um and Claude's got&nbsp; these three MCPs to go figure out how to do that&nbsp;&nbsp; so the first thing Claude decides to do is to&nbsp; call the Asana MCP and a specific tool there uh&nbsp;&nbsp; the Asana MCP has got tens of tools that you&nbsp; can call but Claude has picked out the right&nbsp;&nbsp; one this list workspaces and notice we're doing&nbsp; this call my Asana task are are authenticated&nbsp;&nbsp; not everybody can see them so we actually have&nbsp; to do a ooth request to the MCP server so when&nbsp;&nbsp; you make this API call you pass a ooth token into&nbsp; the messages API we exchange that OOTH token for&nbsp;&nbsp; access token and then make the call to the Asana's&nbsp; MCP server in a secure way so we do that call we&nbsp;&nbsp; get the results back and we say okay uh this this&nbsp; is this is the workspace that that user has assign&nbsp;&nbsp; uh Claude then decides to drill in it picks&nbsp; this um search method to search for this code&nbsp;&nbsp; with Claude in MCP demo that's one I set up in&nbsp; context is knowing what my uh project is we find&nbsp;&nbsp; the project ID um Claude gets that project gets&nbsp; details about that project ID finally finds this&nbsp;&nbsp; um the actual project ID for this and it can call&nbsp; get tasks just pausing for a second a complicated&nbsp;&nbsp; software enterprise software project uh like Asana&nbsp; you know has a complicated API structure and even&nbsp;&nbsp; you as a developer or I as a developer might take&nbsp; some time to go understand that but notice how&nbsp;&nbsp; Claude is whipping through this very quickly and&nbsp; it gets right at the tasks so all that happened&nbsp;&nbsp; very quickly but we're not done with the overall&nbsp; prompt so Claude's using that long range planning&nbsp;&nbsp; that um capability to figure out what to do next&nbsp; so it's got the tasks next what it needs to do&nbsp;&nbsp; is to create an image if you remember the prompt&nbsp; I said to create an image about that so no we're&nbsp;&nbsp; not going to announce a image creation model from&nbsp; Anthropic uh today but there are uh tons of MCP&nbsp;&nbsp; servers out there that do image creation um and if&nbsp; you're really deep in this MCP space you know that&nbsp;&nbsp; most of those are actually local MCP servers like&nbsp; intended to run on your local machine but this&nbsp;&nbsp; support and the cloud AAI support is all about&nbsp; remote MCPs so look luckily Cloudflare offers&nbsp;&nbsp; an MCP remote service where you can take any of&nbsp; these local MCPs and host it on Cloudflare in a&nbsp;&nbsp; secure way and that's what we've done we've taken&nbsp; one of the open source um MCP providers hosted it&nbsp;&nbsp; on Cloudflare and then made that available to&nbsp; the model and so the model chooses to call that&nbsp;&nbsp; with uh when I made these slides the project was&nbsp; definitely not in good shape so which is why the&nbsp;&nbsp; the query is what it is um and we call that MCP&nbsp; server and get a result back get both the image uh&nbsp;&nbsp; URL as well as the actual image comes back in the&nbsp; result and that both of those go back to Claude so&nbsp;&nbsp; Claude now has the tasks it now has the image the&nbsp; next thing it needs to do is send that email so&nbsp;&nbsp; Claude will compose a email with all of that data&nbsp; and then it needs to send it so in this case we're&nbsp;&nbsp; going to use the Zapier MCP server zapier has&nbsp; got hundreds of enterprise connections and a very&nbsp;&nbsp; well-designed MCP system that lets you enable or&nbsp; disable expose just exactly what you need and have&nbsp;&nbsp; um uh enterprise control over that so we've set&nbsp; it up to just expose Gmail uh and we've and the&nbsp;&nbsp; model has chosen to use this subject and whatnot&nbsp; to to make that work and so we get the response it&nbsp;&nbsp; has it has sent that email and this is what this&nbsp; is what it looked like more recently a little bit&nbsp;&nbsp; happier a little bit happier picture but we did&nbsp; we did actually get the email and this is very&nbsp;&nbsp; easy to set up hopefully you're getting a little&nbsp; bit of a pattern here a very composable system&nbsp;&nbsp; it uses the existing messages API at this time&nbsp; we're just using this new MCP servers attribute&nbsp;&nbsp; and you can list as many MCP servers as you&nbsp; need here um you just give the URL and a and a&nbsp;&nbsp; name um and then you pass if you need if it's an&nbsp; OOTH service pass the authorization token there&nbsp;&nbsp; so we're very fortunate to have several&nbsp; uh remote MCPs that are live today that&nbsp;&nbsp; uh you can use with the MCP API whether you're&nbsp; doing task management or you're doing payments&nbsp;&nbsp; you're creating a video you're doing machine&nbsp; management there's an MCP for you to get those&nbsp;&nbsp; done uh so these are all available today&nbsp; and then I'm sure tomorrow there's going&nbsp;&nbsp; to be a ton a ton more and that's really what&nbsp; Zapier has found interesting uh because with&nbsp;&nbsp; our mutual customers we can now their their&nbsp; customers can now build really powerful agents&nbsp;&nbsp; very easily with a combination of our MCP API&nbsp; support and and their MCP so let's look at a demo okay so let's use this one um what are my&nbsp; open task in a sauna just to warm us up make&nbsp;&nbsp; sure we can get this so again what you're going&nbsp; to see this time is that it calls the get asauna&nbsp;&nbsp; um tasks so it's passing this and then we get that&nbsp; nice list of ASA tasks and uh a nice response but&nbsp;&nbsp; you know I thought maybe we should like pull all&nbsp; of the pieces together here so hang with me with&nbsp;&nbsp; this query create an email with a creative and&nbsp; motivational image about my Asana project status&nbsp;&nbsp; including some analysis on the percentage&nbsp; complete and any news on the web about those&nbsp;&nbsp; tasks and send it to the team so that starts by&nbsp; going through uh a sauna getting the tasks uh out&nbsp;&nbsp; of there um and then it's got to do it's got to&nbsp; use code executor so it's now it's writing code&nbsp;&nbsp; uh to analyze the status of all those tasks and&nbsp; get our percent complete that being done it knows&nbsp;&nbsp; what all the tasks are so now it does a search for&nbsp; our conference finds the latest information maybe&nbsp;&nbsp; a tweet from one of you will show up there um and&nbsp; it decides to drill in a little bit more on cloud&nbsp;&nbsp; for opus and sonnet so now it decides to create&nbsp; that motivational image and notice the prompt it's&nbsp;&nbsp; uh giving to our MCP so that's that's pretty&nbsp; nice the rocket launching uh looks beautiful&nbsp;&nbsp; now what it needs to do is take all the data&nbsp; that you just saw um and pull that together&nbsp;&nbsp; into like a really nicely formatted email and&nbsp; that's going to take the model just a minute to&nbsp;&nbsp; uh build this this entire email but it's going to&nbsp; take that email and call the uh Zapier MCP service&nbsp;&nbsp; and send it so hopefully any minute it will get&nbsp; that email it's almost I can feel it okay there&nbsp;&nbsp; it is so it's it's almost done thanks we have to&nbsp; have that moment we have we have to have it um&nbsp;&nbsp; so it's a little uh funny formatting here because&nbsp; it's uh JSON HTML in this JSON viewer that we're&nbsp;&nbsp; using here but it's generating this whole email&nbsp; um and then of course the real tests if this was&nbsp;&nbsp; like an actual live demo we'd expect like to&nbsp; get an email so let's see if we actually get&nbsp;&nbsp; this email so you can see we've been practicing&nbsp; but yeah this is the one that we just Oh that&nbsp;&nbsp; was 44 minutes ago let's see if there's a more&nbsp; recent one yeah here we go this one zero minutes&nbsp;&nbsp; ago there it is oh we got to show you real look i&nbsp; mean we did zero prompt engineering on this thing&nbsp;&nbsp; look how nicely formatted Opus comes up with this&nbsp; email so really fantastic okay let's move back to&nbsp;&nbsp; the slides um and I think I have to finish up&nbsp; very quickly um MCB connector is uh remote MCP&nbsp;&nbsp; simple to set up ooth support only standard&nbsp; token prices okay let's drill into optimize&nbsp;&nbsp; you can't really talk about optimization without&nbsp; talking about prompt caching um prompt caching&nbsp;&nbsp; lets you reuse part of your prompts uh that are&nbsp; used frequently that saves capacity cost and&nbsp;&nbsp; um latency and we've had customers say well your&nbsp; five minutes of time between cache hits uh isn't&nbsp;&nbsp; enough for some humans maybe walk away from&nbsp; the computer and come back or some longrunning&nbsp;&nbsp; agents so we've added a new option in addition to&nbsp; the five minutes we launch with a new option of&nbsp;&nbsp; extending that to one hour with the same um 90%&nbsp; discount on cash hits um and batch processing&nbsp;&nbsp; um is a great way to effectively process large&nbsp; amounts of data and now that batch supports&nbsp;&nbsp; web search code execution and MCP connector&nbsp; it's not just for batch processing anymore&nbsp;&nbsp; it's your async agentic uh API so you can get&nbsp; a 50% discount for using that and you can build&nbsp;&nbsp; async agents very quickly but we've also had&nbsp; customers tell us that they need dedicated&nbsp;&nbsp; uh they need um reliability dedicated capacity&nbsp; to make sure that they can serve the needs of&nbsp;&nbsp; their users so we offer as of today we're offering&nbsp; customers the ability to com to buy uh a month's&nbsp;&nbsp; worth of capacity uh at a discount uh and with&nbsp; this 99% reliability so a discount for longer&nbsp;&nbsp; commits okay so uh that's a wrap so we talked&nbsp; about uh build clawed for long range planning&nbsp;&nbsp; and code execution we talked about bringing your&nbsp; data in with web search and MCP connector and then&nbsp;&nbsp; we talked about how to optimize that with prompt&nbsp; caching batch and priority tier so unfortunately&nbsp;&nbsp; we're out of time but I will be out there for&nbsp; questions so thank you very much for coming [Music]&nbsp;&nbsp; go go go go go [Music]&nbsp;&nbsp; yep yep

---

## 17. Taking Claude to the Next Level

**Preview:**
> Kind: captions Language: en [Applause] good morning everyone welcome to Taking&nbsp; Claude to the next level i'm Lisa Crowoot&nbsp;&nbsp; i'm a research product manager here at Anthropic&nbsp; and today I have the pleasure of introducing you&nbsp;&nbsp; to our newest models Claude for Sonnet and Opus&nbsp; so we're going to start by talking about what&nbsp;&nbsp; the next level of AI agents looks like i'll&nbsp; go through new capabilities in the Claude 4&nbsp;&nbsp; family and then we'll talk ...

**Full Transcript:**

Kind: captions Language: en [Applause] good morning everyone welcome to Taking&nbsp; Claude to the next level i'm Lisa Crowoot&nbsp;&nbsp; i'm a research product manager here at Anthropic&nbsp; and today I have the pleasure of introducing you&nbsp;&nbsp; to our newest models Claude for Sonnet and Opus&nbsp; so we're going to start by talking about what&nbsp;&nbsp; the next level of AI agents looks like i'll&nbsp; go through new capabilities in the Claude 4&nbsp;&nbsp; family and then we'll talk through some practical&nbsp; tips for how to get the most out of Sonnet and Opus before we dive deep on Claude 4 I wanted&nbsp; to paint a picture of what we think our next&nbsp;&nbsp; generation next generation agents really&nbsp; look like we really want Claude to be great&nbsp;&nbsp; at three things claude should be able to&nbsp; work alongside you and adapt to the ways&nbsp;&nbsp; you work claude should be able to work entirely&nbsp; independently on tasks that require many steps&nbsp;&nbsp; and in both of these cases Claude needs to&nbsp; sustain performance over hours of continuous work imagine this you've been assigned a new&nbsp; project to refactor your O system to support&nbsp;&nbsp; OOTH 2.0 so you decide to work with claude&nbsp; on this to make faster progress you might&nbsp;&nbsp; choose to write the requirements and the plan and&nbsp; update the documents but decide to delegate the&nbsp;&nbsp; implementation to Claude what is most interesting&nbsp; in this collaborative mode is that we don't&nbsp;&nbsp; envision this being like clear human AI handoffs&nbsp; we really want you to be able to work with Claude&nbsp;&nbsp; so for example when Claude is reviewing the&nbsp; codebase and documents it might find out that&nbsp;&nbsp; you missed a requirement in your PRD so Claude&nbsp; should challenge your assumptions just like&nbsp;&nbsp; working with a great engineer together you can&nbsp; achieve a higher quality outcome faster than you&nbsp;&nbsp; would have on your own this is what augmentation&nbsp; not automation will look like we do also envision&nbsp;&nbsp; that Claude will be able to operate on tasks&nbsp; like this entirely independently so take that&nbsp;&nbsp; same refactor and imagine that you just assign&nbsp; the whole thing to Claude even without tight&nbsp;&nbsp; human oversight Claude will create comprehensive&nbsp; plans for the refactor it will use tools like web&nbsp;&nbsp; search and document search to make sure that it's&nbsp; up operating from the most up-to-date information&nbsp;&nbsp; and it will use your company standards and best&nbsp; practices to write production ready code claude&nbsp;&nbsp; writes tests recognizes and fixes its mistakes it&nbsp; can take feedback and remember your feedback so&nbsp;&nbsp; it doesn't make the same mistake twice so when&nbsp; models work independently like this we really&nbsp;&nbsp; think trust and communication are paramount so&nbsp; Cloud needs to follow your instructions it also&nbsp;&nbsp; needs to communicate its decisions with you in a&nbsp; way that you can review them it needs to be able&nbsp;&nbsp; to adapt to changing inputs and new information&nbsp; so in both of these examples Claude would need to&nbsp;&nbsp; work over many hours to complete the task if you&nbsp; use Cloud.AI or Claude Code regularly you might&nbsp;&nbsp; be familiar with Claude doing in seconds what&nbsp; takes you minutes or hours but our vision goes&nbsp;&nbsp; beyond that we want Claude to be able to take&nbsp; on tasks that will take it hours to complete&nbsp;&nbsp; and we think that when this is possible it will&nbsp; dramatically expand what AI agents can do so this&nbsp;&nbsp; is our vision an AI that works alongside you&nbsp; builds trust while working independently and&nbsp;&nbsp; can take on complex tasks that require sustained&nbsp; focus so let's dive in on how Cloud 4 is making&nbsp;&nbsp; this a reality as you heard earlier from&nbsp; Daario we launched two new models today&nbsp;&nbsp; claude Opus for and Claude sonnet for and I'm&nbsp; going to talk through four main improvement areas&nbsp;&nbsp; thinking and tool use memory instruction following&nbsp; and reduced reward hacking we'll discuss how these&nbsp;&nbsp; improvements contribute towards our agent vision&nbsp; let's start with extended thinking and tool use&nbsp;&nbsp; earlier this year we launched Claude 3.7 Sonnet&nbsp; which was our first hybrid reasoning model and&nbsp;&nbsp; what that means is the model can respond nearly&nbsp; to your request or think deeply before responding&nbsp;&nbsp; with Cloud 4 we're expanding on thinking by&nbsp; introducing a new beta capability for Claude&nbsp;&nbsp; to alternate between thinking and tool use let me&nbsp; walk you through an example so here I've provided&nbsp;&nbsp; Claude with a CSV of bike rental data and I gave&nbsp; it a very open-ended prompt i told it to just tell&nbsp;&nbsp; me the most the three most interesting things&nbsp; about this data claude has access to a ripple&nbsp;&nbsp; tool which lets it run code autonomously to&nbsp; analyze the data but it's never seen this&nbsp;&nbsp; data before so when it first thinks it's actually&nbsp; thinking quite tactically about how to handle the&nbsp;&nbsp; large file and the first thing it does is print&nbsp; itself out the headers so that it can understand&nbsp;&nbsp; the data structure and like what is even in this&nbsp; data it's only in the second and third thinking&nbsp;&nbsp; block that it starts to actually think about the&nbsp; prompt and the problem at hand so um it starts&nbsp;&nbsp; to plan where it's going to find interesting&nbsp; patterns it decides to look for hourly patterns&nbsp;&nbsp; in bike rentals different patterns for casual&nbsp; versus registered users and seasonal and weather patterns oops sorry it runs through its plan completing the analysis&nbsp; and Claude was able to find interesting patterns&nbsp;&nbsp; like the fact that ca casual versus&nbsp; registered users have different time of&nbsp;&nbsp; day usage it found a clear evening commuting&nbsp; pattern and kind of no surprise to any of us&nbsp;&nbsp; it found that bike rentals were 1.8 times&nbsp; more common on sunny days versus rainy days the next capability I want to talk about is memory&nbsp; we think memory is critically important for our&nbsp;&nbsp; next generation agents vision for two reasons&nbsp; first no one wants to work with an agent that&nbsp;&nbsp; you have to keep reminding the same things over&nbsp; and over again but secondly and more tactically&nbsp;&nbsp; if Claude is working over hours it can't keep&nbsp; every single detail in its context window&nbsp;&nbsp; it needs to be smarter and only remember the&nbsp; most salient and important facts claude Opus&nbsp;&nbsp; 4 demonstrates remarkably better memor memory&nbsp; capabilities so when given an external file&nbsp;&nbsp; system with which it can read and write&nbsp; memories Claude Opus is able to come&nbsp;&nbsp; up with a plan remember that plan and track&nbsp; progress against that plan over hours of work&nbsp;&nbsp; so we're going to take a slight detour and&nbsp; talk about the game of Pokemon as a way to&nbsp;&nbsp; illustrate uh this memory capability so if you're&nbsp; we've been using Pokemon as a practical prototype&nbsp;&nbsp; for testing Claude's agent capabilities for&nbsp; a while if you're interested in learning more&nbsp;&nbsp; about this my colleague David will be giving a&nbsp; talk later today and I recommend checking it out&nbsp;&nbsp; uh for the purpose of this talk today I want&nbsp; to talk about uh how Claude is using memory in&nbsp;&nbsp; Pokemon so if you think back to your game boy days&nbsp; uh the game of Pokemon is really you go around and&nbsp;&nbsp; catch Pokemon and then you train them up so that&nbsp; they can win battles and this concept of training&nbsp;&nbsp; is like really core to the game you need to teach&nbsp; your Pokemon how to win battles and that takes&nbsp;&nbsp; time where you go around and uh have the Pokemon&nbsp; battle other Pokemon to level up prior Claude&nbsp;&nbsp; models would recognize this and decide that they&nbsp; had to go train their Pokemon but would quickly&nbsp;&nbsp; like lose track of their plan uh and start doing&nbsp; something else before their Pokemon were able to&nbsp;&nbsp; level up opus 4 on the other hand is meticulously&nbsp; tracking its Pokemon's training progress so here&nbsp;&nbsp; in its memory file it keeps track of the fact&nbsp; that it has uh played 64 battles and to put that&nbsp;&nbsp; into context 64 battles would take Claude about 12&nbsp; hours of continuous gameplay claude Opus remains&nbsp;&nbsp; focused on its training goals logging Pokemon&nbsp; level improvements in this file so memory is a new&nbsp;&nbsp; model capability we're excited about because of&nbsp; how it will unlock longer arc agentic trajectories a third improvement I want to highlight is&nbsp; improvements in complex instruction following&nbsp;&nbsp; this one is near and dear to me because I've&nbsp; spent many hours working on Claude's system&nbsp;&nbsp; prompt and we're finding that as agent&nbsp; systems become more complex the system&nbsp;&nbsp; prompts and sets of instructions that govern&nbsp; Claude's behavior are getting longer so for&nbsp;&nbsp; example our own CloudAI system prompt is&nbsp; about 16,000 tokens right now so that's&nbsp;&nbsp; 16,000 tokens of instructions that Claude needs&nbsp; to be able to follow for Claude to work in these&nbsp;&nbsp; systems it's important that its behaviors are&nbsp; steerable by you the developer so you're each&nbsp;&nbsp; building different applications that may have&nbsp; different requirements and principles that&nbsp;&nbsp; govern Claude's behavior we've trained Cloud&nbsp; 4 models to specifically to be able to follow&nbsp;&nbsp; instructions within long and complex system&nbsp; prompts longer than 10,000 tokens for example&nbsp;&nbsp; it's easier to steer Claude when to use tools and&nbsp; when not to use tools and in our own system prompt&nbsp;&nbsp; this improved instruction following has actually&nbsp; allowed us to reduce the size of the prompt by 70% finally I want to highlight improvements&nbsp; on a behavior we call reward hacking so reward&nbsp;&nbsp; hacking is when models take shortcuts to achieve&nbsp; an outcome or a result without actually solving&nbsp;&nbsp; the problem at hand you can think of it like hard&nbsp; coding tests or commenting them out this behavior&nbsp;&nbsp; is extremely trust busting for users so when you&nbsp; see it happen it makes you feel like you have to&nbsp;&nbsp; meticulously review everything Claude does and&nbsp; every line of code while we don't consider this&nbsp;&nbsp; an entirely solved problem Cloud4 models show&nbsp; significantly reduced tendency to reward hack on&nbsp;&nbsp; an evaluation set of problems that were selected&nbsp; due to this tendency in past models claude 4 shows&nbsp;&nbsp; more than 80% less tendency towards the behavior&nbsp; and this means you can better trust Claude to&nbsp;&nbsp; complete your task the right way while being&nbsp; honest about its limitations and uncertainties so these four improvements thinking and tool use&nbsp; memory improved steering and reward hacking work&nbsp;&nbsp; together to create a claude that is more capable&nbsp; coherent and trustworthy over longer time horizons&nbsp;&nbsp; now I want to spend the last few minutes getting&nbsp; practical and providing you and your team's tips&nbsp;&nbsp; to get the most out of these models when you get&nbsp; back to the office tomorrow the first decision&nbsp;&nbsp; you'll have to make is which model to use and our&nbsp; recommendation is always to test the models within&nbsp;&nbsp; your evaluations and your product to ultimately&nbsp; make this decision but to give you some highle&nbsp;&nbsp; guidance here Opus is our most capable model with&nbsp; frontier intelligence it will be best for the&nbsp;&nbsp; most complex tasks you have so think coding within&nbsp; large and complex code bases ch uh code migrations&nbsp;&nbsp; or refactors long horizon agentic tasks and&nbsp; planning and orchestration a good rule of thumb&nbsp;&nbsp; here is that if sonnet 3.7 is getting 60 or 70%&nbsp; on your evaluation it will be a great use case for&nbsp;&nbsp; testing opus sonnet 4 is fast and efficient and&nbsp; is great for use cases that sonnet 3 is excel 3.7&nbsp;&nbsp; is excelling at today it spikes at agentic coding&nbsp; and will be awesome for app development and green&nbsp;&nbsp; field coding generation kind of vibe coding um&nbsp; as well as any use case that has humans in the loop when upgrading to the new models you may need&nbsp; to adjust your prompts to get the best performance&nbsp;&nbsp; so those of you familiar with Sonnet 3.7 might&nbsp; be aware of its ability to go above and beyond&nbsp;&nbsp; the given user request i've seen this described&nbsp; as something like you ask it to change the color&nbsp;&nbsp; on a button and it codes you an entire new app uh&nbsp; we call this behavior overeagerness and cloud for&nbsp;&nbsp; models are much less overeager by default so what&nbsp; this means is if you have language in your prompt&nbsp;&nbsp; that aims to dampen sonnet 3.7's proclivity&nbsp; towards overeagerness you'll want to remove&nbsp;&nbsp; that language we don't think it's needed anymore&nbsp; and if you have an application where you think&nbsp;&nbsp; this above and beyond behavior is beneficial&nbsp; to users you should just tell the model to go&nbsp;&nbsp; above and beyond in the prompt cloud for models&nbsp; are more than capable of delivering that as well&nbsp;&nbsp; we are also finding the models have better&nbsp; attention to detail in the prompt this goes&nbsp;&nbsp; along with the improved instruction following but&nbsp; you might need to audit your prompt to make sure&nbsp;&nbsp; that you're actually encouraging the behaviors&nbsp; you want to see so for example when we were&nbsp;&nbsp; testing this model on claw.ai we couldn't figure&nbsp; out why occasionally it was using the wrong XML&nbsp;&nbsp; tag for citations and we root caused it to&nbsp; one single typo in our prompt with examples if you're using Cloud 4 with tool use&nbsp; you can prompt Cloud 4 models to call&nbsp;&nbsp; tools in parallel so this lets Claude&nbsp; parallelize tasks uh running more than&nbsp;&nbsp; one thing simultaneously when using interleaf&nbsp; thinking and tool use you can actually tell&nbsp;&nbsp; Claude specifically what to think about&nbsp; in between tool calls so you might tell&nbsp;&nbsp; Claude to carefully reflect on search result&nbsp; quality and plan next steps before proceeding&nbsp;&nbsp; and finally if you're using tools it's a good&nbsp; idea to tell Claude when and when it should not&nbsp;&nbsp; invoke those tools within your prompt we found the&nbsp; improved instruction quality instruction following&nbsp;&nbsp; qualities of Claude 4 have been very effective&nbsp; at addressing tool overt triggering problems so to recap we're building towards a long-term&nbsp; vision where Claude can work alongside you&nbsp;&nbsp; complete work for you over long sustained&nbsp; durations we think you'll find Cloud 4 models&nbsp;&nbsp; great for agents because of interleaf thinking&nbsp; and tool use memory improved instruction following&nbsp;&nbsp; and reduced reward hacking so what can you do&nbsp; tomorrow when you get back to the office start&nbsp;&nbsp; experimenting try building with both models using&nbsp; Opus for your most complex and ambitious tasks and&nbsp;&nbsp; Sonnet for everything else invest some time in&nbsp; prompt engineering very small changes to your&nbsp;&nbsp; prompt can make a large difference to performance&nbsp; all of these models are slightly different and&nbsp;&nbsp; share your feedback with us because it will help&nbsp; us make the next generations of Claude even better&nbsp;&nbsp; thanks for joining me today we're really excited&nbsp; to see what you build with these new models and&nbsp;&nbsp; I'm happy to take any questions you'll need&nbsp; to walk over to the microphones in the aisles [Applause] here no questions you're all good to go awesome so uh both uh Opus 4 and Sonnet 4 are doing&nbsp; really well on Sweetbench and some of the&nbsp;&nbsp; other benchmarks however most folks realize&nbsp; that like benchmarks and practical use is not&nbsp;&nbsp; really comparable are you also developing&nbsp; new benchmarks for software development as&nbsp;&nbsp; these things get better and are there things&nbsp; like uh evaluations for overeagerness and&nbsp;&nbsp; things like that where we can get a sense of it&nbsp; beforehand before the product actually releases&nbsp;&nbsp; yeah great question we test these models quite&nbsp; extensively before we release them through what&nbsp;&nbsp; we call like a Swiss cheese of testing methods&nbsp; so benchmarks are only one thing we look at um&nbsp;&nbsp; we also use them internally quite extensively&nbsp; before launch so anthropic employees have been&nbsp;&nbsp; using these models on cloud code for weeks for&nbsp; example and that helps us better understand how&nbsp;&nbsp; they perform in practical use we do some&nbsp; testing with early access customers and&nbsp;&nbsp; so we do we like are interested in developing&nbsp; more and more benchmarks but we don't think&nbsp;&nbsp; that benchmarks are the only way to look&nbsp; at how good these models are let's go on&nbsp;&nbsp; this side yeah so I was curious uh in all of the&nbsp; demons and use cases that you presented so far&nbsp;&nbsp; uh it seems to be very centered on text right&nbsp; uh coding and text in general so uh I wanted to&nbsp;&nbsp; ask you if you can comment on uh you know your the&nbsp; multimodel capabilities of the model in particular&nbsp;&nbsp; uh images and and audio yeah we actually think&nbsp; images um uh the models can see images and respond&nbsp;&nbsp; to images we think it's pretty important for agent&nbsp; capabilities we see image use even within coding&nbsp;&nbsp; for example when people share with the model the&nbsp; front end that the model designed then the model&nbsp;&nbsp; can go back and fix things so we're continuing&nbsp; to improve on our multimodal input capabilities&nbsp;&nbsp; um because we we think it's going to be&nbsp; really critical for um cla to be able to&nbsp;&nbsp; do these complex tasks on on its own hello um&nbsp; hey so sometimes uh I use cloud tool calling&nbsp;&nbsp; not as an execution mechanism but more so as a&nbsp; survey mechanism for instance I'm like analyze&nbsp;&nbsp; the situation here and the tools are like option&nbsp; A option B option C uh have you guys factored that&nbsp;&nbsp; use case for tool calling like into your training&nbsp; for instance um that is not something I've heard&nbsp;&nbsp; of before but sounds really interesting we if&nbsp; you find me after the break I'd like love to&nbsp;&nbsp; learn more about how how you think about that&nbsp; yeah absolutely it's a great use case thanks so I'm I'm really enjoying all the focus&nbsp; on practical software engineering tasks&nbsp;&nbsp; one thing that is difficulty with LLMs on a large&nbsp; legacy codebase is no matter how good it is at&nbsp;&nbsp; reading just a blob of text the actual structure&nbsp; of the situation that it's involved in is is just&nbsp;&nbsp; so vast that like you kind of need to represent&nbsp; it some other way in order to navigate around so I&nbsp;&nbsp; wonder what kind of patterns you have found useful&nbsp; in navigating these these larger legacy contexts&nbsp;&nbsp; i think our general philosophy is that um we're&nbsp; trying to improve Cloud's ability to do agentic&nbsp;&nbsp; search so you can think of agentic search like you&nbsp; search for something and then you can think about&nbsp;&nbsp; it a little bit more and then search again um and&nbsp; use the information over time to like inform what&nbsp;&nbsp; you're doing and that applies to both code and&nbsp; like the deep research capabilities which we have&nbsp;&nbsp; on cloud.ai um and so uh that combined with this&nbsp; memory capability where maybe Claude can write&nbsp;&nbsp; down where certain information is in the codebase&nbsp; we think will help solve that problem hello um&nbsp;&nbsp; so in your presentation you mentioned something&nbsp; about uh being able to specify like what the model&nbsp;&nbsp; should be thinking about in between tool calls how&nbsp; controllable is like the length of thinking tokens&nbsp;&nbsp; in terms of like how much the model should&nbsp; be thinking or like being able to specify&nbsp;&nbsp; um yeah like length of that or also like specific&nbsp; tool calls within the actual like chain of thought&nbsp;&nbsp; process is that possible with the model so&nbsp; you have control over the maximum thinking&nbsp;&nbsp; length but the model adapts its thinking length&nbsp; to how much it actually needs to think to solve&nbsp;&nbsp; the task so you kind of have like a thinking&nbsp; budget which you give the model and the model&nbsp;&nbsp; won't go over that budget but might be under&nbsp; that budget okay okay so if I wanted it if I&nbsp;&nbsp; wanted to ask the model to think for like a&nbsp; specific number of tokens that's not possible&nbsp;&nbsp; right now you can tell it to think for less than a&nbsp; certain number of less than a certain number okay two questions the first one is a gimme what is&nbsp; the preferred mechanism for feedback because there&nbsp;&nbsp; are lots of ways to get in touch with you and then&nbsp; the second is does the increased steerability mean&nbsp;&nbsp; that we can finally ask um Claude not to generate&nbsp; insane numbers of inane comments when it writes&nbsp;&nbsp; code um I hope so um actually I hope that they're&nbsp; better at that by default because of this like&nbsp;&nbsp; less overeager tendency um and it should also&nbsp; follow your instructions better um on feedback&nbsp;&nbsp; uh I think like we love just talking to people&nbsp; so if you find an anthropic employee today that&nbsp;&nbsp; would be excellent um and would love to hear&nbsp; more about your experiences and then I think&nbsp;&nbsp; we have some like online uh uh feedback&nbsp; forms as well awesome i think we'll call&nbsp;&nbsp; it here but thanks everyone for joining me&nbsp; i hope you're excited about Cloud 4 uh and&nbsp;&nbsp; uh come find us after the break uh to chat more&nbsp; about these great new models [Applause] [Music] [Music]&nbsp;&nbsp; nobody

---

## 18. Code with Claude Opening Keynote

**Preview:**
> Kind: captions Language: en hey hey hey [Music] welcome [Music] [Music] [Music] [Music] hey hey hey [Music] [Music] [Music] [Music] [Music] hey hey hey [Music] [Music] [Music] hey hey hey [Music] [Music]&nbsp;&nbsp; heat heat [Music]&nbsp;&nbsp; happy [Music] birthday heat heat [Music] [Music] doo doo doo doo doo doo&nbsp; doo doo doo doo doo [Music]&nbsp;&nbsp; let me do it [Music] hey come on [Music]&nbsp;&nbsp; heat [Music] heat heat heat [Music] [Music] happy [Music] [Music] [Music] [Music] ...

**Full Transcript:**

Kind: captions Language: en hey hey hey [Music] welcome [Music] [Music] [Music] [Music] hey hey hey [Music] [Music] [Music] [Music] [Music] hey hey hey [Music] [Music] [Music] hey hey hey [Music] [Music]&nbsp;&nbsp; heat heat [Music]&nbsp;&nbsp; happy [Music] birthday heat heat [Music] [Music] doo doo doo doo doo doo&nbsp; doo doo doo doo doo [Music]&nbsp;&nbsp; let me do it [Music] hey come on [Music]&nbsp;&nbsp; heat [Music] heat heat heat [Music] [Music] happy [Music] [Music] [Music] [Music] [Music]&nbsp;&nbsp; hey hey hey [Music] [Music] oh hey hey you are [Music] [Music] I'm [Music] [Music] [Music] Hey over beh [Music] what [Music] hey hey hey one one 1 [Music] [Music] [Music] 1.11 [Music] here come [Music] one one one one one one one [Music] hey hey hey [Music] [Music] [Music] [Music] hey hey hey [Music] [Music] [Music] i [Music] love me [Music] come on hey [Music] [Music] hey hey hey [Music] [Music] hey hey hey hey hey hey hey hey hey hey hey hey [Music]&nbsp;&nbsp; party don't get Get yourself [Music] [Music] [Music] hey down Down down down down down down [Music] come on come on [Music] [Music] [Music]&nbsp;&nbsp; down down down down down down gabbit jump [Music] [Music] dick dick [Music]&nbsp;&nbsp; d [Music] down [Music] d Get [Music] down [Music]&nbsp;&nbsp; hey hey hey [Music]&nbsp;&nbsp; down hey [Music] Black yeah [Music] [Music] [Music]&nbsp;&nbsp; yeah yeah [Music] yeah yeah heat heat [Music] [Music] okay okay [Music]&nbsp;&nbsp; please welcome to the stage chief&nbsp; product officer of Anthropic Mike [Applause] [Music] Kger good morning everyone&nbsp; and welcome to Code with Claude Anthropic's&nbsp;&nbsp; first developer conference i'm really happy&nbsp; to see you all here i'm Mike Kger i am chief&nbsp;&nbsp; product officer here at Anthropic i just hit my&nbsp; one-year mark which in AI years is about like&nbsp;&nbsp; three years um but I'm having a blast um and&nbsp; before this I co-founded Instagram um and also&nbsp;&nbsp; an AI powered news app called Artifact which is&nbsp; where I first started getting exposed to a lot of&nbsp;&nbsp; these AI technologies i joined Anthropic because&nbsp; of its founders vision building AI systems that&nbsp;&nbsp; are powerful as well as helpful and trustworthy&nbsp; today that vision includes something immediate&nbsp;&nbsp; and concrete a commitment to empower developers&nbsp; like yourselves to transform how work gets done&nbsp;&nbsp; and how companies get built this transformation is&nbsp; about augmenting not replacing human creativity ai&nbsp;&nbsp; agents are changing the way we work and the way&nbsp; we innovate they're expanding what we can build&nbsp;&nbsp; by removing bottlenecks that have limited human&nbsp; productivity today you'll hear from our product&nbsp;&nbsp; and engineering leaders as well as some of our&nbsp; customers about how they're pushing the frontier&nbsp;&nbsp; to give you a sense of what you can expect today&nbsp; at Code with Claude you can attend three technical&nbsp;&nbsp; deep dives to transform how you build with&nbsp; Claude and five sessions from leading players&nbsp;&nbsp; already using Anthropics platform to reshape&nbsp; their industries and dedicated office hours and&nbsp;&nbsp; workshops for hands-on experience but before we&nbsp; talk about some exciting new API API capabilities&nbsp;&nbsp; I have for you I want to invite a guest on stage&nbsp; please welcome our CEO and co-founder Daario Amade hey everyone uh I'm going to be back in 20&nbsp; minutes for a fireside so uh I'll be I'll be&nbsp;&nbsp; really really brief with uh with this appearance&nbsp; um I'm not one to uh hype things up so I'll just&nbsp;&nbsp; say this without any further fanfare i'm&nbsp; happy to announce that as of exactly this&nbsp;&nbsp; moment we're releasing Claude 4 Opus and Claude&nbsp; 4 Sonnet on all of our relevant product services now I know that we haven't had an opus model&nbsp; in a while so just as a reminder opus is the&nbsp;&nbsp; most capable and intelligent model and sonnet is&nbsp; the mid-level model that you all know and love&nbsp;&nbsp; and have been using for the last uh approximately&nbsp; year that's a a good balance between intelligence&nbsp;&nbsp; and efficiency um we tried to design both&nbsp; of them so that there are there are use&nbsp;&nbsp; cases and times when it's optimal to use each&nbsp; one so I will talk very briefly about the two&nbsp;&nbsp; of them and then and then turn it back over to&nbsp; Mike and then I'll be back for the fireside um&nbsp;&nbsp; uh first let's talk about Opus so it is especially&nbsp; designed for coding and agentic tasks um it gets&nbsp;&nbsp; state-of-the-art on Sweetbench Terminal Bench some&nbsp; other things like like that um uh but I think in&nbsp;&nbsp; many ways uh as we're often finding with large&nbsp; models the benchmarks don't fully do justice to it&nbsp;&nbsp; um customers who we've previewed it to have found&nbsp; that it can do tasks that take humans up to six or&nbsp;&nbsp; seven hours autonomously um uh within Enthropic&nbsp; I've seen some of our most senior engineers be&nbsp;&nbsp; surprised at how much more productive it has made&nbsp; them and for the first time actually when I've&nbsp;&nbsp; um you know looked and and seen clawude&nbsp; written internal summaries documents and&nbsp;&nbsp; uh and and ideas you know in the past the quality&nbsp; was often good but you could never really quite&nbsp;&nbsp; mistake it for a human because it always had&nbsp; that that specific style this was the first&nbsp;&nbsp; time I actually got fooled where I actually get&nbsp; got back and then you know I just read by the&nbsp;&nbsp; name really fast and I thought it referred&nbsp; to someone on the team and I'm like no the&nbsp;&nbsp; name was Claude um uh uh so I you know I think I&nbsp; think there's a there's a there's a lot in opus&nbsp;&nbsp; um on sonnet um I think this will be for&nbsp; many people a strict update for uh a a strict&nbsp;&nbsp; improvement from sonnet 3.7 um at the same cost&nbsp; and better intelligence many customers are simply&nbsp;&nbsp; uh uh switching directly from one to the other it&nbsp; actually does just as well as Opus on some of the&nbsp;&nbsp; uh coding benchmarks but I think it's leaner and&nbsp; more narrowly focused um I think in particular it&nbsp;&nbsp; addresses some of the uh feedback we got on sonnet&nbsp; 3.7 around overeagerness the tendency to do more&nbsp;&nbsp; than you asked for which is sort of the opposite&nbsp; of laziness which was which was an earlier problem&nbsp;&nbsp; and and some of the some of the reward hacking&nbsp; issues so many of our customers have been trying&nbsp;&nbsp; it out and view it as a strong upgrade from&nbsp; 3.7 for example um you know cursor cursor here&nbsp;&nbsp; has been uh one of our well-known customers&nbsp; has been trying it and says saying this is&nbsp;&nbsp; uh this is a state-of-the-art uh this is this is&nbsp; this is a state-of-the-art coding model um it's a&nbsp;&nbsp; leapuh forward and complex codebase understanding&nbsp; and we expect developers will experience uh across&nbsp;&nbsp; the board capability improvements um someone who&nbsp; was playing with the model in person one customer&nbsp;&nbsp; said "What the f is this model?" Um it's really&nbsp; it's really amazing so um uh I'll I'll leave the&nbsp;&nbsp; details to others but um the last thing I'll say&nbsp; is we are going to continue to improve the Clawude&nbsp;&nbsp; 4 series of models we expect to periodically&nbsp; release perhaps minor version updates ideally&nbsp;&nbsp; even more frequently than we than we have for um&nbsp; for uh for for Sonnet so it should be out there&nbsp;&nbsp; you should be able to try it on basically all&nbsp; all the surfaces as of now except I think free&nbsp;&nbsp; tier has has sonnet uh has sonnet only but all&nbsp; the other surfaces uh all the API surfaces have&nbsp;&nbsp; both um so uh really hope you enjoy the&nbsp; model and I'll turn it back over to Mike thank you Dario two new models and you heard it&nbsp; here first um we'll be seeing Dario again as you&nbsp;&nbsp; mentioned at the end of our agenda for a Q&amp;A where&nbsp; I'll get to ask him the questions that are likely&nbsp;&nbsp; on your mind uh right now as well i'm personally&nbsp; very excited for our customers to try both Cloud&nbsp;&nbsp; Opus 4 and Sonet 4 our teams have loved working&nbsp; with them and we think you will too uh now that&nbsp;&nbsp; Dario has shared our big model news I'll talk&nbsp; more about our detailed API roadmap our goals&nbsp;&nbsp; for building Cloud 4 were clear from the start&nbsp; wanted to build powerful AI that safely introduces&nbsp;&nbsp; new model capabilities continue to advance the&nbsp; frontier for coding and AI agents and ensure&nbsp;&nbsp; that cloud becomes your virtual collaborator and&nbsp; that's exactly what we've delivered with Opus 4&nbsp;&nbsp; and Sonnet 4 like uh Sonnet 37 both Cloud 4 models&nbsp; are what we call hybrid models that have two modes&nbsp;&nbsp; near instant responses and extended thinking for&nbsp; when you need deeper reasoning i've been surprised&nbsp;&nbsp; at how many customers use the deeper reasoning&nbsp; even for non-coding and non-math use cases&nbsp;&nbsp; opus 4 is great at understanding your codebase&nbsp; and planning additions it's extremely effective&nbsp;&nbsp; uh and accurate with everything from migrations&nbsp; to code refactorings and it's also the right&nbsp;&nbsp; choice for your most complex agentic workflows if&nbsp; you found you've hit a wall with other models on&nbsp;&nbsp; your use case I think you'll be really pleasantly&nbsp; surprised with what you can do with Opus 4 sonnet&nbsp;&nbsp; 4 meanwhile excels at everyday coding tasks&nbsp; app development uh and pair programming it's&nbsp;&nbsp; also ideal for high volume use cases it perfectly&nbsp; balances efficiency with performance think of it&nbsp;&nbsp; as your always on coding partner both models&nbsp; are live today as Daria mentioned in Cloud&nbsp;&nbsp; and Claude Code as well as the Enthropic API&nbsp; Amazon Bedrock and Google Cloud's Vert.ex AI&nbsp;&nbsp; these models bring critical new capabilities for&nbsp; building AI agents they can use tools like web&nbsp;&nbsp; search during their reasoning process which is new&nbsp; handle multiple tools in parallel and when given&nbsp;&nbsp; access to local files it can actually maintain&nbsp; memory across sessions to build knowledge over&nbsp;&nbsp; time and I'll talk with Dario a little bit about&nbsp; that memory feature too later these aren't just&nbsp;&nbsp; incremental improvements they fundamentally change&nbsp; what's possible for AI agents now I know the term&nbsp;&nbsp; agents gets thrown around a lot these days i have&nbsp; a personal uh joke which is how many minutes into&nbsp;&nbsp; a meeting can we make at Anthropic without&nbsp; saying the word agents i think I made it 17&nbsp;&nbsp; minutes or something uh but today what we're&nbsp; going to focus on is agents beyond the hype&nbsp;&nbsp; um I think what's really key is with the right&nbsp; underlying models and the right underlying&nbsp;&nbsp; platform tools AI agents can actually turn human&nbsp; imagination into tangible reality at unprecedented&nbsp;&nbsp; scale and that's especially important for startups&nbsp; and developers like yourselves i've been a founder&nbsp;&nbsp; myself when I think back to Instagram's early&nbsp; days our famously small team had to make a&nbsp;&nbsp; bunch of very painful eitheror decisions we either&nbsp; explore uh adding video to the product or focus on&nbsp;&nbsp; our core creativity either focus on our mobile&nbsp; app and at first our single mobile app or uh&nbsp;&nbsp; expand into the web it was all very single track&nbsp; with AI agents startups now can run experiments&nbsp;&nbsp; in parallel learn from users and build products&nbsp; faster than ever before which is something I've&nbsp;&nbsp; heard from many of you all and AI agents can give&nbsp; you the the founders of startups uh access to the&nbsp;&nbsp; kind of strategic thinking that you might get from&nbsp; a high-powered C CFO i see our CFO in the front&nbsp;&nbsp; row here or head of product uh while they're still&nbsp; building towards those key positions yourselves&nbsp;&nbsp; you're not ready for the make those hires but you&nbsp; can hire uh Claude for some of those roles for now&nbsp;&nbsp; this transformation is no longer theoretical&nbsp; i see it in my role and my work every single&nbsp;&nbsp; day i personally spend a lot of time with Claude&nbsp; maybe more time with Claude than my significant&nbsp;&nbsp; other it's fine uh in fact uh soon after I&nbsp; joined Anthropic um I uh sat down with Amazon's&nbsp;&nbsp; Alexa team and they were eager to see how Claude&nbsp; might become part of their vision for the future&nbsp;&nbsp; of voice assistants at first my team planned&nbsp; on presenting some slides talking points kind&nbsp;&nbsp; of the plan we'd make for any other customer but&nbsp; in the days leading up to the meeting I had this&nbsp;&nbsp; like persistent thought why not use Claude itself&nbsp; to build a hands-on demo i thought it'd make the&nbsp;&nbsp; conversation more interesting and bring to life&nbsp; the potential of Claude and Alexa functionality&nbsp;&nbsp; together the challenge was building this demo&nbsp; without any access to Alexa's actual codebase we&nbsp;&nbsp; needed to create a prototype of the core Alexa&nbsp; functionality while also integrating Claude's&nbsp;&nbsp; capabilities all within a tight oneweek timeline&nbsp; really a tight one weekend timeline claude was&nbsp;&nbsp; the only reason we were able to pull this off uh&nbsp; in such a limited time frame our threeperson team&nbsp;&nbsp; split between San Francisco and London built a&nbsp; functional prototype that showed the potential&nbsp;&nbsp; and thanks to Claude the effort was a success i&nbsp; even got to write some of the code you can take&nbsp;&nbsp; the engineer out of the engineering CT overall&nbsp; but you can't take it out of me uh and do a lot&nbsp;&nbsp; of the front end development uh for the project&nbsp; itself and of course a lot more work went into the&nbsp;&nbsp; partnership after that first meeting um but Claude&nbsp; is now one of the models that Amazon is using for&nbsp;&nbsp; Alexa plus uh which launched earlier this year&nbsp; and is now rolling out and we were able I think&nbsp;&nbsp; to really show the potential thanks to Claude&nbsp; i've been watching this evolution towards ai&nbsp;&nbsp; for years now uh when I first got a demo early&nbsp; access of GitHub Copilot back in 2021 I called&nbsp;&nbsp; it the single most mind-blowing application of&nbsp; machine learning I've ever seen back in those days&nbsp;&nbsp; 2020 we called it machine learning instead of AI&nbsp; that was generations ago but it was really clear&nbsp;&nbsp; the potential for this early glimpse of Agentic&nbsp; AI i had an even stronger feeling last summer&nbsp;&nbsp; when we launched Artifacts i could describe what&nbsp; I wanted for a mini app or visualization hit send&nbsp;&nbsp; go grab coffee and come back to Claude having&nbsp; built what I'd imagined and over the following&nbsp;&nbsp; year it's become clear we're not just building&nbsp; better tools we're creating genuine collaborators&nbsp;&nbsp; and Anthropic's economic research uh confirms&nbsp; what I've seen firsthand for the majority of&nbsp;&nbsp; use cases AI is augmenting people's work instead&nbsp; of replacing it it's much more about tasks than&nbsp;&nbsp; entire roles and this is similar to the influence&nbsp; that your best colleagues have the most talented&nbsp;&nbsp; people you work with don't just execute they&nbsp; understand your context they learn from experience&nbsp;&nbsp; and they know when to take the initiative versus&nbsp; when to just check in great AI agents like the&nbsp;&nbsp; ones you can build on our platform should&nbsp; excel at three capabilities they should have&nbsp;&nbsp; contextual intelligence understanding you and your&nbsp; organization's unique context and continuously&nbsp;&nbsp; learning from experience not just following&nbsp; instructions but comprehending the why and the&nbsp;&nbsp; how that means models that learn and personalize&nbsp; over time acquiring not just contextual but also&nbsp;&nbsp; episodic and organizational memory the way&nbsp; I always put this to the team is your hundth&nbsp;&nbsp; task with an agent should be much better than your&nbsp; first just like your hundth day with an employee&nbsp;&nbsp; should be much better than your first if you're&nbsp; doing the right things around training second&nbsp;&nbsp; longunning execution handling complex multi-hour&nbsp; tasks without constant management coordinating&nbsp;&nbsp; with other agents and humans as needed so you&nbsp; have the context and then you can execute it&nbsp;&nbsp; over a longer period of time and third genuine&nbsp; collaboration engaging in meaningful dialogue&nbsp;&nbsp; adapting to your working style and providing&nbsp; transparent reasoning for their actions the key&nbsp;&nbsp; insight here is that true agency doesn't mean&nbsp; uncontrolled action and autonomy doesn't mean&nbsp;&nbsp; uh just yoloing it it means intelligent autonomy&nbsp; balanced with clear checkpoints maintaining human&nbsp;&nbsp; oversight for critical decisions while delegating&nbsp; the smaller decisions that usually consume so much&nbsp;&nbsp; of our time now let's talk about those&nbsp; capabilities we're announcing to serve&nbsp;&nbsp; those three needs we'll start with our new code&nbsp; execution tool which is available on the anthropic&nbsp;&nbsp; API today the code execution tool gives Claude&nbsp; an environment where it can run code enabling&nbsp;&nbsp; it to act as a data analyst that can transform&nbsp; raw data into visual insights claude doesn't&nbsp;&nbsp; just write code anymore now it can execute it it&nbsp; sees the results and it can iteratively refine the&nbsp;&nbsp; results and the code to better highlight patterns&nbsp; in your data here uh we'll show Claude analyzing&nbsp;&nbsp; sales data to see how a specific type of product&nbsp; is performing claude can load your data set clean&nbsp;&nbsp; it generate exploratory charts and drill down into&nbsp; anomalies all in real time as someone who started&nbsp;&nbsp; their career as a data visualization analyst this&nbsp; resonates a bunch with me and the code execution&nbsp;&nbsp; tool is even more powerful when combined with&nbsp; the intelligence of the cloud for models this&nbsp;&nbsp; is what we mean by agency the ability to take&nbsp; a complex task and see it through to completion&nbsp;&nbsp; these are the first models capable of handling&nbsp; hours of tasks saving you half maybe even full&nbsp;&nbsp; days when you work alongside them and not just&nbsp; writing code snippets but refactoring entire&nbsp;&nbsp; code bases or implementing complex features&nbsp; from scratch to give you a sense of the kind&nbsp;&nbsp; of progress that we're seeing back in the day when&nbsp; I started you could delegate maybe minutes of work&nbsp;&nbsp; to Cloud3 cloud3 meanwhile could work autonomously&nbsp; for about 45 minutes without losing its thread&nbsp;&nbsp; and now we're breaking into hours of work that&nbsp; Claude can take on autonomously as you saw&nbsp;&nbsp; earlier Rocketin mentioned that they ran Claude&nbsp; independently for an incredible seven hours with&nbsp;&nbsp; sustained performance it can do it without losing&nbsp; the thread especially as it's able to manage its&nbsp;&nbsp; memory and its own to-do list we've already&nbsp; integrated this power where you work hopefully&nbsp;&nbsp; you're all familiar with Cloud Code our agentic&nbsp; coding tool that we launched in research preview a&nbsp;&nbsp; few months ago we're moving cloud code to general&nbsp; access today this actually started as an internal&nbsp;&nbsp; exploratory project by Boris one of our tech leads&nbsp; this is his announcement post who wanted Claude to&nbsp;&nbsp; help them code directly in the terminal um very&nbsp; early we still called it Claude CLI internally i&nbsp;&nbsp; think some of our best innovations like artifacts&nbsp; and cloud code have really come from this kind&nbsp;&nbsp; of bottoms up experimentation it's part of the&nbsp; culture we try to foster at anthropic within just&nbsp;&nbsp; two days of launching it internally our usage&nbsp; chart went vertical people talk about product&nbsp;&nbsp; market fit we really often talk about product&nbsp; anthropic fit like are people internally dog&nbsp;&nbsp; fooding are they using it today most anthropic&nbsp; employees rely on it for everything from routine&nbsp;&nbsp; coding to large-scale migrations i've watched some&nbsp; of our most advanced coders run multiple copies&nbsp;&nbsp; of cloud code across multiple terminal windows&nbsp; they're moving from just being engineers to being&nbsp;&nbsp; managers of several autonomous agents tackling&nbsp; everything from simple coding tasks to complex&nbsp;&nbsp; full stack development projects across multiple&nbsp; code bases i realized I was using cloud code and&nbsp;&nbsp; I would run one in our front-end repo and one&nbsp; in our backend repo and one of our cloud code&nbsp;&nbsp; engineers is like you're doing it wrong just run&nbsp; it in the root cloud can figure out where it's&nbsp;&nbsp; going to be able to do it across all of them and&nbsp; it does it beautifully and that's that's changed&nbsp;&nbsp; how I've used it already the vast majority of&nbsp; of anthropic developers use cloud code daily&nbsp;&nbsp; to give you a sense of the impact it's had on our&nbsp; team it's shortened our technical onboarding time&nbsp;&nbsp; uh to get engineers up to speed from two to three&nbsp; weeks to two to three days i've really seen it how&nbsp;&nbsp; it can help you build an understanding of the&nbsp; codebase especially a large monolith like ours&nbsp;&nbsp; very rapidly as it's fantastic at navigating code&nbsp; and today we're bringing cloud code capabilities&nbsp;&nbsp; directly into VS Code and Jet Brains with full&nbsp; diff views and agentic workflow management built&nbsp;&nbsp; into the editors and we're also introducing&nbsp; the Cloud Code SDK so you can build your own&nbsp;&nbsp; applications on top of the same core agent as&nbsp; Cloud Code as an example of the possibilities&nbsp;&nbsp; of the SDK you can now run Claude code in GitHub&nbsp; you can tag Claude in a GitHub pull request or an&nbsp;&nbsp; issue and it will respond to reviewer uh feedback&nbsp; modified code or implement test coverage we're&nbsp;&nbsp; also focused on what we call closing the loop&nbsp; so Cloud Code is now helping build itself and it&nbsp;&nbsp; demonstrates the power of self-improvement as it&nbsp; speeds up its own development it's incredible how&nbsp;&nbsp; Cloud Code empowers developers like yourselves to&nbsp; get more done i think back to when I was building&nbsp;&nbsp; Instagram our team was between two and six&nbsp; engineers you know before we got acquired&nbsp;&nbsp; and we were supporting two mobile platforms we&nbsp; would have been able to produce prototypes in&nbsp;&nbsp; days and not weeks if we had agentic coding&nbsp; products like this we've talked a lot about&nbsp;&nbsp; building performant reliable agents now agency&nbsp; without responsibility is dangerous especially&nbsp;&nbsp; when you're talking about something that's&nbsp; self-improving like our cloud code uh product&nbsp;&nbsp; and even more so in enterprise settings with&nbsp; stringent security and compliance requirements&nbsp;&nbsp; i think widespread adoption of agents will require&nbsp; improving model discernment and judgment around&nbsp;&nbsp; confidentiality decision-making and coordination&nbsp; so our models are already good at this but we'll&nbsp;&nbsp; continue to improve making sure that they know&nbsp; what's confidential they know what to reveal&nbsp;&nbsp; um and that you can trust them in a production&nbsp; setting that's why every feature that we build&nbsp;&nbsp; around our models incorporates what we call&nbsp; architectural safety checkpoints and controls&nbsp;&nbsp; not just cart blanch agents pausing on major&nbsp; decisions while users can define which actions&nbsp;&nbsp; need human approvals which we've also built into&nbsp; the model context protocol they're robust against&nbsp;&nbsp; exploitation we test them we battle test them you&nbsp; know uh a lot around things like prompt injection&nbsp;&nbsp; and they're also transparent by design with&nbsp; clear feedback loops and observable behavior&nbsp;&nbsp; when you trust your agents to act autonomously&nbsp; you're free to focus on in innovation instead of&nbsp;&nbsp; mitigation another area we've invested heavily in&nbsp; is interpretability the science of understanding&nbsp;&nbsp; exactly what's going on inside the minds of AI&nbsp; models dario recently wrote about the urgency of&nbsp;&nbsp; understanding how our AI systems actually work if&nbsp; you read his essay the urgency of interpretability&nbsp;&nbsp; what he calls the race between model intelligence&nbsp; and interpretability effectively we want to be&nbsp;&nbsp; able to give our AI an MRI to see what what&nbsp; it's thinking about and spot any potential&nbsp;&nbsp; problems like deception so we can steer it in&nbsp; the right direction when I joined Enthropic I&nbsp;&nbsp; was excited about how our research pipeline could&nbsp; directly fuel our products take Golden Gate Claude&nbsp;&nbsp; i pushed the ship that's in our second my second&nbsp; week at Enthropic because it didn't feel like it&nbsp;&nbsp; was just a good research paper it would make a&nbsp; fantastic demo a visceral demonstration of how&nbsp;&nbsp; interpretability works in action when we amplified&nbsp; the golden gate bridge feature inside the&nbsp;&nbsp; uh uh clawed neural network we saw that suddenly&nbsp; we could see what it means to manipulate the&nbsp;&nbsp; inner workings of AI and in this case make&nbsp; it deeply obsessed with our favorite bridge&nbsp;&nbsp; uh the techniques that we used to create Golden&nbsp; Gate Claude could in the future help us reduce&nbsp;&nbsp; uh model harmful model behaviors or improve model&nbsp; performance for specific domains and as we start&nbsp;&nbsp; employing virtual collaborators around companies&nbsp; my hope is that we can lean on techniques like&nbsp;&nbsp; interpretability and auditability to be a&nbsp; cornerstone of their work so we can figure&nbsp;&nbsp; out what they're doing at scale these are the&nbsp; kinds of breakthroughs that that are going to&nbsp;&nbsp; help us transform abstract research into tangible&nbsp; product capabilities as you saw earlier we're now&nbsp;&nbsp; at the point where AI models can handle hours of&nbsp; autonomous work and that's a capability that's&nbsp;&nbsp; doubling every several months but raw model&nbsp; capability alone isn't enough to unlock these&nbsp;&nbsp; multi-hour workflows in practice agents also need&nbsp; access to real world information a connection to&nbsp;&nbsp; your existing systems and costefficient scaling&nbsp; that's why we're launching four interconnected&nbsp;&nbsp; capabilities to help power agents with context&nbsp; and help them scale so first off starting today&nbsp;&nbsp; you can now connect the model context protocol&nbsp; directly through our API mcp is already being used&nbsp;&nbsp; by Microsoft Google OpenAI Block Atlassian Zapier&nbsp; Linear and many more this was the dream list when&nbsp;&nbsp; we started creating the MCP protocol and we open&nbsp; sourced it this was the dream list like maybe one&nbsp;&nbsp; day we'll get these companies to adopt it it's&nbsp; less than a year and they they've all um come&nbsp;&nbsp; on board mcp acts as the universal translator&nbsp; and connector for AI agents enabling seamless&nbsp;&nbsp; connection to your existing system without needing&nbsp; to write a custom bespoke integration every single&nbsp;&nbsp; time this lays the foundation of what could&nbsp; become the agent economy where specialized&nbsp;&nbsp; agents have access to the data and tools they&nbsp; need to tackle complex challenges second web&nbsp;&nbsp; search gives Claude real-time access to current&nbsp; information this is intelligent data augmentation&nbsp;&nbsp; that allows Claude to reason about current events&nbsp; market trends and emerging emerging technologies&nbsp;&nbsp; it's really powerful in combination with the MCP&nbsp; feature as well you can imagine searching across&nbsp;&nbsp; an internal knowledge source making some uh new&nbsp; uh insights and then going off and searching the&nbsp;&nbsp; web to contextualize them third the files API&nbsp; is available today in the API to streamline how&nbsp;&nbsp; developers access and store documents simplifying&nbsp; development workflows we're also releasing a&nbsp;&nbsp; cookbook to help developers build that memory&nbsp; functionality that I mentioned directly into&nbsp;&nbsp; their applications these new cloud for models&nbsp; have shown significant improvement in what we&nbsp;&nbsp; call self-managed memory so you'll find that this&nbsp; works surprisingly well and it can be achieved&nbsp;&nbsp; with very little additional overhead by using&nbsp; the files API as we demonstrate in that cookbook&nbsp;&nbsp; you'll see claude both read and write to these&nbsp; memory files and maintain contact context over&nbsp;&nbsp; time last power needs to be practical and scalable&nbsp; we want to ensure that we can grow with you from&nbsp;&nbsp; prototype to production to millions of users&nbsp; so that you're able to control cost and improve&nbsp;&nbsp; efficiency we want cloud to work for you as you&nbsp; succeed and reach massive scale that's why prompt&nbsp;&nbsp; caching was our most requested feature one of&nbsp; our most popular API features with prompt caching&nbsp;&nbsp; customers can provide Claude with more context&nbsp; uh and background knowledge and example outputs&nbsp;&nbsp; reducing costs by up to 90% and latency by up to&nbsp; 85% for long prompts now every customer I talked&nbsp;&nbsp; to had one very clear request on prompt caching&nbsp; that we're delivering today which is a longer time&nbsp;&nbsp; deliver TTL so in addition to the five minute TTL&nbsp; we had out of the box with prompt caching today&nbsp;&nbsp; we're launching a premium 1hour TTL which is a&nbsp; 12x improvement that dramatically reduces cost for&nbsp;&nbsp; longrunning agent workflows this infrastructure&nbsp; makes agent applications viable at scale so these&nbsp;&nbsp; capabilities all compound when we think about&nbsp; building features into the API we don't think&nbsp;&nbsp; about them as one-off we think about how do they&nbsp; complement each other how do they form a cohesive&nbsp;&nbsp; story claude can now execute code understand your&nbsp; systems access current information on the web&nbsp;&nbsp; creating the foundation for agents that operate&nbsp; with full context even for longunning tasks and&nbsp;&nbsp; it can use the files API to maintain memory and&nbsp; context during that entire execution everything&nbsp;&nbsp; you've seen this morning is just the beginning&nbsp; our road map continues to build on three pillars&nbsp;&nbsp; the first is industry-leading agentic tools and&nbsp; applications so you can use cloud autonomously to&nbsp;&nbsp; handle hours of work knowing it can use the code&nbsp; environment uh code execution tool to execute code&nbsp;&nbsp; in its own environment cloud code is now generally&nbsp; available integrating with VS Code and Jetrain so&nbsp;&nbsp; you can use the extensive SDKs to build your own&nbsp; custom workflows including inside GitHub we'll&nbsp;&nbsp; continue to push on integrating more context in&nbsp; the API our updates today allow you to bring the&nbsp;&nbsp; bring this context via the model context protocol&nbsp; as well as build on real real-time updates from&nbsp;&nbsp; the web and execute uh complex workflows across&nbsp; any data source and across anything in the API via&nbsp;&nbsp; MCP and finally efficient scaling as of today you&nbsp; can use expanded 1hour prompt caching to optimize&nbsp;&nbsp; performance and cost at scale each advancement&nbsp; builds on what we've discussed today with Cloud&nbsp;&nbsp; 4 as the foundation Opus 4 for your most complex&nbsp; uh agentic workflows Sonnet 4 as your daily driver&nbsp;&nbsp; for everyday intelligence we're enabling a new&nbsp; class of applications code execution expands the&nbsp;&nbsp; hours of work that cloud can do mcp expands the&nbsp; comprehensive information that cloud can retrieve&nbsp;&nbsp; and our platform updates ensure our models become&nbsp; increasingly efficient for every dollar spent&nbsp;&nbsp; we're actively learning from developers like&nbsp; yourselves uh to how you use these tools so please&nbsp;&nbsp; keep the feedback coming i love API feedback if&nbsp; you don't know this about me like absolutely like&nbsp;&nbsp; ping me uh I love hearing the feedback and how we&nbsp; can continue to improve the API for developers or&nbsp;&nbsp; yourselves and MCP is a perfect example of this&nbsp; it started as an internal idea and then began and&nbsp;&nbsp; graduated to community experimentation and now&nbsp; it's a core platform feature if you watch the&nbsp;&nbsp; Microsoft Build keynote they're building MCP into&nbsp; so much of their uh of their real infrastructure&nbsp;&nbsp; as well we want to create an ecosystem of AI&nbsp; agents where we have the feedback loops to make&nbsp;&nbsp; them actually useful for you today we stand at a&nbsp; major threshold our latest models combined with&nbsp;&nbsp; all the latest tools that we've released are&nbsp; giving the seeds of a new era the future isn't&nbsp;&nbsp; about AI doing human work it's about AI helping&nbsp; humans do superhuman work and I'm really excited&nbsp;&nbsp; to build this vision together with you and I&nbsp; can't wait to see the kinds of applications it&nbsp;&nbsp; powers for all of your companies and to show you&nbsp; what's possible I'm next going to hand the mic&nbsp;&nbsp; to Catwoo from our product team to demonstrate&nbsp; how accessing our new models inside Cloud Code&nbsp;&nbsp; transforms your development workflows helping&nbsp; you ship complex multi-day tasks in a single&nbsp;&nbsp; conversation welcome again to Code with Claude and&nbsp; thanks again hope you enjoy the rest of your day hi everyone I'm Cat Woo product manager for&nbsp; Cloud Code as Mike mentioned we recently&nbsp;&nbsp; launched Cloud Code our agent coding&nbsp; tool in research preview claude Code&nbsp;&nbsp; gives developers direct access to the&nbsp; raw power of Enthropics models right&nbsp;&nbsp; where they work in their terminals&nbsp; as of today quad code is generally available throughout computing history&nbsp; we've continually moved to higher levels&nbsp;&nbsp; of abstraction from machine code to assembly&nbsp; to highlevel languages with cloud code and&nbsp;&nbsp; increasingly agentic models we're&nbsp; witnessing another step forward&nbsp;&nbsp; developers are shifting from asking for&nbsp; specific functions to describing entire&nbsp;&nbsp; features guiding AI and changing how software&nbsp; is built today we're bringing the new Claude 4&nbsp;&nbsp; models to Claude Code making it an even&nbsp; more powerful and capable coding agent and on top of new models we're releasing several&nbsp; new features in Cloud Code focused on making it&nbsp;&nbsp; a more versatile coding agent across your whole&nbsp; dev life cycle first Quad Code now integrates with&nbsp;&nbsp; VS Code and Jet Brains bringing it to familiar&nbsp; interfaces for millions of developers as Cloud&nbsp;&nbsp; Code works you can now see its proposed changes&nbsp; in line in your editor we're also releasing the&nbsp;&nbsp; Quad Code SDK which allows developers to use Quad&nbsp; Code as a building block in your applications&nbsp;&nbsp; and workflows the possibilities are endless&nbsp; with the SDK to showcase these possibilities&nbsp;&nbsp; we're releasing an open-source example of the&nbsp; SDK in action with Claude Code in GitHub you&nbsp;&nbsp; can tag Claude directly on poll requests and&nbsp; issues in GitHub and Cloud Code will respond&nbsp;&nbsp; to reviewer feedback fix CI errors and add new&nbsp; functionality with these additions Cloud Code&nbsp;&nbsp; now works everywhere you do acting as a virtual&nbsp; teammate across all surfaces in the terminal for&nbsp;&nbsp; deep development work in remote environments&nbsp; like GitHub for automated workflows built on&nbsp;&nbsp; the SDK and in the IDE for seamless review&nbsp; all in Quad Code is a versatile coding agent&nbsp;&nbsp; for accelerating development wherever you are&nbsp; whether you're working directly with cloud code&nbsp;&nbsp; interactively or using it asynchronously great&nbsp; my favorite part let's see what these updates&nbsp;&nbsp; look like in a demo i'm going to show Quad Code&nbsp; tackling a real dev task in a product that many&nbsp;&nbsp; of you are familiar with we'll use Excaladrol an&nbsp; open- source whiteboarding tool and ask Quad Code&nbsp;&nbsp; to implement one of their most requested features&nbsp; adding a table component how many of you have&nbsp;&nbsp; gotten that feature request that's been on your&nbsp; backlog for ages that you know your users would&nbsp;&nbsp; love but you just haven't had the time to build&nbsp; this is the kind of task that we can handle much&nbsp;&nbsp; faster with cloud code normally for a task like&nbsp; this I would set call to work make some coffee&nbsp;&nbsp; catch up on email and Slack and come back when&nbsp; the outputs are ready but I only have 10 minutes&nbsp;&nbsp; with you all today so let's show a sped up but&nbsp; real workflow here's the Excal repo open in VS&nbsp;&nbsp; Code let's write a prompt to tell Cloud Code&nbsp; our requirements we'll ask Quad Code to add a&nbsp;&nbsp; table component that supports custom dimensions&nbsp; drag to resize and all of Excal's other styling&nbsp;&nbsp; options here's where it gets exciting quad&nbsp; Code will first create a to-do list for how&nbsp;&nbsp; it'll approach the entire problem then we can see&nbsp; that Quad Code will start to explore the codebase&nbsp;&nbsp; starting with the file that we already have open&nbsp; for context the best part of the ID integration&nbsp;&nbsp; is the ability to see diffs in line in the&nbsp; editor this way you can see the surrounding&nbsp;&nbsp; code for more context so you can accept changes&nbsp; with confidence or give quad code feedback&nbsp;&nbsp; we can approve each edit as Cloud Code works or&nbsp; we can let Quad Code continue making edits with&nbsp;&nbsp; auto accept mode letting us balance visibility and&nbsp; control in this demo we gave Quad Code the ability&nbsp;&nbsp; to make edits run lint and tests and make PRs so&nbsp; Quad Code worked for 90 minutes on this task i&nbsp;&nbsp; wish I could show you the whole thing but we need&nbsp; to speed things up what you're seeing is actual&nbsp;&nbsp; unedited output from quad code an hour and a half&nbsp; later and it's done it added table functionality&nbsp;&nbsp; wrote tests to validate the change and iterated&nbsp; until lint and test passed this normally required&nbsp;&nbsp; us to understand the codebase architecture and&nbsp; how every single other tool was implemented&nbsp;&nbsp; in this case cloud code is literally doing&nbsp; hours of work for us pretty impressive right now now let's run Excal locally and&nbsp; just make sure the feature works as&nbsp;&nbsp; we expect let's check that we have a fully&nbsp; functional table component by making a three&nbsp;&nbsp; row by three column table great we can&nbsp; reposition the table we can drag to&nbsp;&nbsp; resize we can change the border pattern and&nbsp; color and we can add text to cells this also&nbsp;&nbsp; integrates with Excal's existing UI all of&nbsp; this was done with one prompt in Cloud Code [Applause]&nbsp;&nbsp; next we'll ask Cloud Code to use the GitHub&nbsp; CLI to create a pull request for this branch&nbsp;&nbsp; cool let's click in now we have our pull&nbsp; request this is where the Quad Code SDK shines&nbsp;&nbsp; it lets us build custom workflows on top of cloud&nbsp; code including through GitHub actions for this PR&nbsp;&nbsp; I'd like to update the docs instead of going back&nbsp; to the IDE we can just tag at Claude and ask it to&nbsp;&nbsp; update our documentation for us behind the scenes&nbsp; this triggers a GitHub action that runs Cloud Code&nbsp;&nbsp; claude comments on the PR as it works and it'll&nbsp; it'll make a commit for us when it's done you can&nbsp;&nbsp; also tag at Quad on a GitHub issue and I'll also&nbsp; make a PR for you there with this feature Quad&nbsp;&nbsp; Code meets users on even more surfaces where&nbsp; they're already working devs no longer need&nbsp;&nbsp; to context switch in their local environment and&nbsp; you can even kick off runs on the go this is all&nbsp;&nbsp; built on the Cloud Code SDK beyond powering GitHub&nbsp; actions we've seen customers do incredible things&nbsp;&nbsp; with the SDK including running many Quad codes in&nbsp; parallel to fix flaky tests increase test coverage&nbsp;&nbsp; and even do on call triage cool it looks like&nbsp; the action is done running and we can see Quad&nbsp;&nbsp; Code updating its comments to let us know what it&nbsp; did let's click into the commit and see cloud's changes it updated the documentation for us in&nbsp; our PR and committed it without us having to do a thing in just 10 minutes you've seen quad code&nbsp; tackle a complex tasks that would have taken&nbsp;&nbsp; days to implement manually writing hundreds of&nbsp; lines of code integrating seamlessly with Excal's&nbsp;&nbsp; existing features and doing hours of work for us&nbsp; all of this is available to you today quad code&nbsp;&nbsp; in GitHub actions powered by our SDK is available&nbsp; in beta and you can install it by running a simple&nbsp;&nbsp; command on the screen within quad the VS code&nbsp; and Jetrains IDE extensions are also live in beta&nbsp;&nbsp; just run quad from your IDE to install&nbsp; last but not least our latest models&nbsp;&nbsp; Quad Opus 4 and Claude Sonnet 4&nbsp; are available to Cloud Code users today quad code shows what's possible when&nbsp; AI can truly understand and work with code&nbsp;&nbsp; to build powerful agents whether coding&nbsp; assistants or applications in any domain&nbsp;&nbsp; you need more than just intelligent models&nbsp; you need the right platform please welcome&nbsp;&nbsp; Michael Gersonenhober who will show&nbsp; you exactly how we're making that possible thanks so much Cat and good morning&nbsp; everybody thank you so much for being here&nbsp;&nbsp; i'm Michael Gersonenhopper head&nbsp; of product for the API platform at Enthropic how many people here use AI generated&nbsp; code already to write their applications yeah&nbsp;&nbsp; and how many of those are using AI at their core&nbsp; feature delivery like everybody here that's what&nbsp;&nbsp; I thought most applications in the world will&nbsp; be built by people already trying to solve the&nbsp;&nbsp; world's problems whether you pass Ble Code&nbsp; whiteboard interviews or getting started&nbsp;&nbsp; with Vibes we're all software engineers now but&nbsp; writing code is just the start you need to more&nbsp;&nbsp; quickly build stable secure and maintainable&nbsp; AI applications and that's why we built the&nbsp;&nbsp; anthropic platform a complete toolkit designed&nbsp; for building state-of-the-art AI applications&nbsp;&nbsp; and agents our platform is already powering&nbsp; most of the world's AI delivery in every&nbsp;&nbsp; domain in finance Turboax helps millions of&nbsp; customers confidently file taxes with federal&nbsp;&nbsp; tax explainers in healthcare Novo Nordisk is&nbsp; using Claude to draft clinical study reports in&nbsp;&nbsp; less than 10 minutes instead of 15 weeks and the&nbsp; world's best coding assistants run on our platform&nbsp;&nbsp; each of these companies took Claude's intelligence&nbsp; and turned it into something uniquely valuable for&nbsp;&nbsp; their users at its foundation our platform&nbsp; provides reliable access to Claude through&nbsp;&nbsp; our model inference service which includes&nbsp; the messages API and essential tools like&nbsp;&nbsp; prompt caching to optimize performance&nbsp; and costs over 50% of all input tokens&nbsp;&nbsp; are cached on the platform doubling the&nbsp; effective context window for our models&nbsp;&nbsp; notion can put vast amounts of your documents in&nbsp; the context window but maintain snappy real-time&nbsp;&nbsp; execution this lets them adopt your voice&nbsp; for creative writing and virtually eliminate&nbsp;&nbsp; hallucination starting today we're extending&nbsp; the cash time to live from 5 minutes to 1 hour your agents can now maintain complex context&nbsp; across the entire user session without breaking&nbsp;&nbsp; the bank but that's just a foundation to build&nbsp; powerful agents our platform provides powerful&nbsp;&nbsp; building blocks as Mike shared we're releasing&nbsp; two new capabilities the files API and a code&nbsp;&nbsp; execution tool just like you and me there are some&nbsp; problems that are easier to solve by writing a&nbsp;&nbsp; script our platform lets your agents write their&nbsp; own code in production just like you would these&nbsp;&nbsp; new features join existing components like web&nbsp; search for real-time information and citations&nbsp;&nbsp; for grounding responses in source documents when&nbsp; Thompson Reuters provides analysis to attorneys&nbsp;&nbsp; in co-consel it's critical that they ground this&nbsp; in their legal research in case law not in the&nbsp;&nbsp; models training data our platform also connects&nbsp; your agents and your data and businesses business&nbsp;&nbsp; systems through model context protocol mcp has&nbsp; taken off within our developer ecosystem with&nbsp;&nbsp; over 3,000 integrations built by the community&nbsp; whether your agent is accessing application&nbsp;&nbsp; errors with Sentry triggering Zapier workflows&nbsp; or creating Asana tasks the MCP connector enables&nbsp;&nbsp; the model to interact with any tool data or app&nbsp; your task requires and today the platform makes&nbsp;&nbsp; it even easier by handling all the technical&nbsp; complexity of tool and API calling for you&nbsp;&nbsp; one thing that I want to emphasize about the&nbsp; platform is the composability of the APIs they're&nbsp;&nbsp; building blocks that work together as well as they&nbsp; work apart helping to solve unique problems that&nbsp;&nbsp; can't be coerced into a cookie cutter shape think&nbsp; of Cloud as the architect and general contractor&nbsp;&nbsp; for your agent it doesn't execute predefined&nbsp; sequences or stack components randomly instead it&nbsp;&nbsp; intelligently determines which materials you need&nbsp; in what order and how they fit together to create&nbsp;&nbsp; something far more powerful than any individual&nbsp; element let me show you what I mean when you build&nbsp;&nbsp; an agent for complex financial analysis Claude&nbsp; intelligently assesses the task and orchestrates&nbsp;&nbsp; the right tools using MCP to access financial&nbsp; data spinning up code execution for statistical&nbsp;&nbsp; analysis searching the web for real-time market&nbsp; data and grounding insights with citations for&nbsp;&nbsp; accuracy and compliance iterating and refining&nbsp; based on results no hard-coded workflow no&nbsp;&nbsp; brittle scripts just intelligent orchestration&nbsp; that allows you to build powerful agent and is&nbsp;&nbsp; seamless and seamlessly adopt new capabilities&nbsp; as our researcher research brings them to life&nbsp;&nbsp; we understand that prompt quality can make or&nbsp; break an AI application which is why we created&nbsp;&nbsp; dev tools like the prompt improver and evaluations&nbsp; along with new observability features that help&nbsp;&nbsp; you get to production and scale faster today&nbsp; we're already helping developers build faster&nbsp;&nbsp; with resources like cookbooks and guides that show&nbsp; you how to implement features like memory into&nbsp;&nbsp; your applications in the future we'll adapt these&nbsp; for programmatic access and host them directly on&nbsp;&nbsp; the platform so you can build even more powerful&nbsp; agents that can research and remember on their own&nbsp;&nbsp; in production everything we've built centers&nbsp; on one goal helping you ship better AI faster&nbsp;&nbsp; the anthropic platform isn't just tools it's&nbsp; your path to building industryleading agents so&nbsp;&nbsp; thank you all for being here today with me at at&nbsp; Code with Claude i'll be on the floor the rest of&nbsp;&nbsp; the conference but it's my privilege to welcome&nbsp; Mario Rodriguez from GitHub to show you exactly&nbsp;&nbsp; what this looks like in production [Applause]&nbsp; [Music] thank you thank you Michael and I am here&nbsp;&nbsp; um thrilled to be with you all we at GitHub are&nbsp; incredibly excited to be part of this energy and&nbsp;&nbsp; innovation and to share more about our deepening&nbsp; partnership with Anthropic um this amazing team&nbsp;&nbsp; everything GitHub does is anchor on two core&nbsp; beliefs right number one is giving developers&nbsp;&nbsp; choice and number two is giving them the best&nbsp; developer experience at GitHub Universe last&nbsp;&nbsp; year we kicked off the relationship with Anthropic&nbsp; we announced Cloud Sonet 3.5 support in VS Code&nbsp;&nbsp; and also in our conversational experiences and we&nbsp; did this because we share fundamental belief with&nbsp;&nbsp; Anthropic that AI can be a powerful force and a&nbsp; force multiplier for developers augmenting their&nbsp;&nbsp; capabilities not replacing augmenting their&nbsp; capabilities and freeing them up to focus on&nbsp;&nbsp; what they do best which is imagination and&nbsp; creativity are of being a software developer&nbsp;&nbsp; is being a whizzer since we haven't expanded&nbsp; since then we have expanded the partnership&nbsp;&nbsp; and experiences across VS Code Github.com and&nbsp; our mobile app just to mention a few and today&nbsp;&nbsp; I am delighted to announce that GitHub copilot&nbsp; supports Cloud Sonet 4 and Opus 4 available right&nbsp;&nbsp; now we just pulled the trigger right when Dario&nbsp; announced it and every one of those services That is what SIM shipping is all about let&nbsp; me tell you it's really hard to do i don't&nbsp;&nbsp; know if you've done it with every application&nbsp; that you have done but it's incredibly hard to&nbsp;&nbsp; do so thanks to all of the teams that make&nbsp; that happen now as you all surely know the&nbsp;&nbsp; future of code is what agent an agent mode&nbsp; in VS Code is our autonomous perprogrammer&nbsp;&nbsp; that can perform multi-step coding tasks based&nbsp; on your natural language commands we've seen&nbsp;&nbsp; firsthand how having cloud's intelligence&nbsp; directly within the editor truly helps&nbsp;&nbsp; developers understand complex code bases um&nbsp; get faster code to production and increase&nbsp;&nbsp; their productivity without ever leaving the&nbsp; environment they already know love and trust&nbsp;&nbsp; but even that right even that is singlethreaded&nbsp; and in my opinion the future is multi-threaded you&nbsp;&nbsp; think about it you're in your editor it becomes a&nbsp; waiting room you're you're going faster but it's&nbsp;&nbsp; still a waiting room and that's why on Monday&nbsp; we took one step further and announces GitHub's&nbsp;&nbsp; copilot coding agent now our coding agents this&nbsp; are autonomous asynchronous peer programmer not&nbsp;&nbsp; pair anymore now it's your peer programmer&nbsp; embedded directly into GitHub copilo's coding&nbsp;&nbsp; agent is currently powered by you probably guessed&nbsp; it cloud sonnet uh and you know the reason we&nbsp;&nbsp; chose that was very clear to me so let me just&nbsp; walk you through three things that made that&nbsp;&nbsp; decision possible number one our evaluation showed&nbsp; that cloud demonstrated three main strengths right&nbsp;&nbsp; strong software engineering and coding knowledge&nbsp; powerful problem solving and that's very important&nbsp;&nbsp; because sometimes you have to go and look at&nbsp; the code and find the right place to make that&nbsp;&nbsp; edit and then number three excellent instruction&nbsp; following and specifically when thinking about&nbsp;&nbsp; tools and MCP so when you're building for ejected&nbsp; coding dealing with these things and large code&nbsp;&nbsp; bases and system prompts you also need something&nbsp; else which is caching right and that prom caching&nbsp;&nbsp; bless you that prom caching support we get from&nbsp; the anthropic API let us build these experiences&nbsp;&nbsp; in a most cost effective way every token counts&nbsp; and every token counts also on the price side so&nbsp;&nbsp; the more we save those the better experience we&nbsp; could provide our customers now on top of that&nbsp;&nbsp; cloud was already the most frequently selected&nbsp; model in agent mode so once we put all of those&nbsp;&nbsp; things together it was very clear to us that&nbsp; cloud set was the right model choice for agent&nbsp;&nbsp; coding in GitHub scenarios now with cloud set&nbsp; 4 we seen improvement in all of these areas not&nbsp;&nbsp; just aggregate benchmarks like sweet benchmarks&nbsp; but more importantly on our real world evaluation&nbsp;&nbsp; suites as well now our collaboration goes deeper&nbsp; than this right it's not just about integrating&nbsp;&nbsp; models directly we've been working closely with&nbsp; Anthropic to officially adopt and scale MCP we're&nbsp;&nbsp; combining intelligence if you think about this&nbsp; like these models are incredibly intelligent you&nbsp;&nbsp; stack like three PhDs on them with knowledge so&nbsp; how do you get knowledge into that intelligent&nbsp;&nbsp; model well the answer to us is MCP and tools&nbsp; and that really unlocks the next acceleration&nbsp;&nbsp; of developer tools recently Kevin Scott that's&nbsp; Microsoft CTO made the analogy that MCP is like&nbsp;&nbsp; the HTD protocol of the web and I completely&nbsp; agree with him so if you have not adopted MCP&nbsp;&nbsp; do it today right after this keynote go and play&nbsp; with it it's that important it's the way you get&nbsp;&nbsp; knowledge into these intelligent models now as&nbsp; we step into this new era of software development&nbsp;&nbsp; we're transforming GitHub's platform from an AI&nbsp; infuse into AI native from creation to deployment&nbsp;&nbsp; we envision this SDLC powered by an agentic&nbsp; layer at the top of it that spans that inner&nbsp;&nbsp; where you are coding and that outer loop those&nbsp; asynchronous experiences and you are going to be&nbsp;&nbsp; an active collaborator every single step of the&nbsp; way the reason why we say co-pilot is the human&nbsp;&nbsp; is at the center and then there's agents helping&nbsp; you that is why we're announcing a new partnership&nbsp;&nbsp; that integrates what Kai just showed you cloud&nbsp; code and the extensible cloud code SDA directly&nbsp;&nbsp; into GitHub's agent platform this opens up new&nbsp; possibilities to customize cloud code remotely&nbsp;&nbsp; invoke it from new surfaces that are embedded into&nbsp; GitHub and our workflows again all on the GitHub&nbsp;&nbsp; platform now we're already done a lot uh but the&nbsp; journey with anthropic is still just beginning in&nbsp;&nbsp; our opinion we believe that by bringing together&nbsp; GitHub's deep deep understanding of developers&nbsp;&nbsp; and Anthropic's AI capabilities through cloud&nbsp; and the platform APIs we will and we can unlock&nbsp;&nbsp; a future that is more intuitive more efficient&nbsp; more ultimately more human that human power is&nbsp;&nbsp; important so I'm excited to see what we continue&nbsp; to build together and also what each of you&nbsp;&nbsp; builds with us so thank you so much and please&nbsp; welcome back to the stage Mike Kriger thank you sir hello again and thanks again to Mario&nbsp; to Michael and to Cat um I love the GitHub&nbsp;&nbsp; integration the last project I did I actually was&nbsp; like "Oh I can actually just install cloud code&nbsp;&nbsp; into a GitHub code space." And all of a sudden&nbsp; I have Cloud Code against the repo that I've&nbsp;&nbsp; already been building it was really great to hear&nbsp; from each of them and hear all about the exciting&nbsp;&nbsp; work being done with Cloud so to close out the&nbsp; show I'd like to dive a little bit deeper into&nbsp;&nbsp; Cloud 4 our research direction uh and what&nbsp; developers can expect next from Enthropic um&nbsp;&nbsp; so please help me welcome back to the stage Dario&nbsp; for our one-on-one conversation welcome back Dario hello again this is great this is like our&nbsp; one-on-one in front of the whole audience this is&nbsp;&nbsp; great um so Cloud 4 uh uh is out cloud Sonet 4 and&nbsp; Cloud Cloud Opus 4 are available um what excites&nbsp;&nbsp; you the most about the Cloud 4 models and how does&nbsp; it change your thinking about what's possible in&nbsp;&nbsp; the next 12 months yeah so um I I think abstractly&nbsp; the thing I'm most excited about is you know every&nbsp;&nbsp; time you have a new class of models there's like&nbsp; more you can do with it right so uh uh you know&nbsp;&nbsp; we're we're we're going to be releasing uh models&nbsp; after Claude 4 there'll probably be a Claude 4.1&nbsp;&nbsp; at some point just like we did with uh with Sonnet&nbsp; uh uh 3.5 and I think we're just at the beginning&nbsp;&nbsp; of of of of you know what what what we can do&nbsp; with the new the new generation of model in&nbsp;&nbsp; terms of tasks i think the autonomy is going to&nbsp; go uh uh is going to go much further than it has&nbsp;&nbsp; already just the ability to give you know set&nbsp; your model free and and give it the ability to&nbsp;&nbsp; you know do something for for a long period of&nbsp; time i think we're I think we're very much very&nbsp;&nbsp; much still still at the beginning of that um uh&nbsp; I'm I'm actually increasingly excited about the&nbsp;&nbsp; models for cyber security tasks i mean you can&nbsp; think of cyber security as like a a subset of of&nbsp;&nbsp; of of coding tasks but they tend to be higherend&nbsp; coding tasks and so I think we're maybe finally&nbsp;&nbsp; hitting the threshold for that and then as a&nbsp; as a former biologist I'm I'm always excited&nbsp;&nbsp; about use of the models for uh for you know&nbsp; biomedical and kind of kind of kind of detailed&nbsp;&nbsp; uh scientific research work which I think opus&nbsp; and opus in particular is going to be good opus&nbsp;&nbsp; in particular I think is going to be particularly&nbsp; particularly strong at that um it really connects&nbsp;&nbsp; I think to machines of loving grace so how does&nbsp; cloud 4 fit into that trajectory overall i like&nbsp;&nbsp; to joke that people think of Machines of Loving&nbsp; Grace as an essay and I think of it as a product&nbsp;&nbsp; road map for the next few years and curious how&nbsp; Cloud 4 fits into that journey yeah it was sort&nbsp;&nbsp; of a product road map that I wrote without knowing&nbsp; how to how to actually get to it and and kind of&nbsp;&nbsp; said all right guys then this is your work this&nbsp; is your job um uh yeah uh you know we're I think&nbsp;&nbsp; we're increasingly thinking about on the biology&nbsp; side of things and and software is part of that&nbsp;&nbsp; right where and you know increasing amount because&nbsp; biology increasingly involves data even involved&nbsp;&nbsp; data 10 years ago when when I uh when I was&nbsp; a biologist uh uh I I think I think I think&nbsp;&nbsp; more and more of it is is is going to be okay we&nbsp; have these models that know a lot about biology&nbsp;&nbsp; and they can help write code and so if you're a&nbsp; computational biologist I think these models will&nbsp;&nbsp; will really accelerate what what you can do and&nbsp; you know we have a number of customers who are&nbsp;&nbsp; who are who are trying out the models for for&nbsp; these tasks i guess we'll we'll get to that in&nbsp;&nbsp; a bit yeah I think uh one of the first hackathons&nbsp; we did after we uh released MCP somebody hooked&nbsp;&nbsp; up MCP to one of those like plotters that so to do&nbsp; drawing and so cloud could draw for it's actually&nbsp;&nbsp; really fun to like see what cloud draws for itself&nbsp; but it was like the first one was like MCPs don't&nbsp;&nbsp; just have to be connecting to digital systems they&nbsp; could also be connecting to the real world so like&nbsp;&nbsp; when you'll be able to drive lab equipment VMCP I&nbsp; think is an interesting uh question for the soon&nbsp;&nbsp; we'll be able to test Claude by connecting it to&nbsp; a polygraph yeah I love that idea are you lying who needs interpretability&nbsp; when we have the polygraph um uh you mentioned that moment where you were&nbsp; you know convinced that Claude the claude&nbsp;&nbsp; written content was was human written um any other&nbsp; breakthrough moments in watching us all you know&nbsp;&nbsp; uh dog food uh Cloud 4 or even try it yourself&nbsp; that made you realize this model felt different&nbsp;&nbsp; um you know I I didn't actually understand I&nbsp; didn't actually understand the details but like&nbsp;&nbsp; there were several people in our side there was&nbsp; a moment a few weeks before the model launched&nbsp;&nbsp; where someone said "Oh my god this model just&nbsp; like oneshotted this like incredibly difficult&nbsp;&nbsp; performance engineering task." And and no model&nbsp; had ever had ever done anything like that before&nbsp;&nbsp; i I I will say that there there's there's this&nbsp; almost almost like superstitious process in the&nbsp;&nbsp; model development where like it it it it somehow&nbsp; all comes together at the last moment even if the&nbsp;&nbsp; training plot process is all planned out like just&nbsp; some of the models abilities maybe it's something&nbsp;&nbsp; about their interaction with people maybe it's&nbsp; something about like just making it the last bit&nbsp;&nbsp; better matters maybe it's people getting used&nbsp; to the model and prompting it but but you you&nbsp;&nbsp; always find the the early versions of the model&nbsp; um you know people are struggling to figure out&nbsp;&nbsp; how to use them and then and then you finally get&nbsp; to a point and people are like this works for me&nbsp;&nbsp; all the time and there's that there's that alchemy&nbsp; that happens somehow always the last moment if you&nbsp;&nbsp; read uh the Creativity Inc by Ed Catmol he talks&nbsp; about the same process with all the Pixar movies&nbsp;&nbsp; like they're really bad until like two days before&nbsp; they're supposed to go out and I feel the same&nbsp;&nbsp; way about our models not that they're really bad&nbsp; but they're like there's like they're not quite&nbsp;&nbsp; there and then suddenly they click and we're&nbsp; like I can't wait to get this out to people it&nbsp;&nbsp; it doesn't make it doesn't make any sense because&nbsp; like the training process is uniform and you know&nbsp;&nbsp; you know you you would think that that it doesn't&nbsp; work that way that it's all a rational process&nbsp;&nbsp; but it's absolutely not there's no point on the&nbsp; RL curve at all that that they come together it&nbsp;&nbsp; comes together at the last minute i don't know why&nbsp; it's a real moment um many people in the audience&nbsp;&nbsp; are developers here and a question that I know has&nbsp; come up internally as people you know think about&nbsp;&nbsp; uh how AI is developing is which parts of the&nbsp; software engineering job will AI take over um&nbsp;&nbsp; and what becomes more important in a world where&nbsp; we have autonomous agents being able to do do a&nbsp;&nbsp; lot of software engineering yeah um so probably&nbsp; like many people here I I read with great interest&nbsp;&nbsp; uh Steve Jay's blog post a couple months ago uh&nbsp; revenge of the junior developer uh he had some&nbsp;&nbsp; uh he had some similar blog posts uh he had some&nbsp; similar blog posts around that actually uh came in&nbsp;&nbsp; to visit us even um uh um uh uh and and that laid&nbsp; out I think the vision of where things are going&nbsp;&nbsp; maybe maybe even better than I could which is that&nbsp; we're gradually go we're gradually going to more&nbsp;&nbsp; and more autonomy of the models right we had this&nbsp; phase where you would do basically autocomplete&nbsp;&nbsp; now there's this thing that I guess people have&nbsp; called vibe coding um uh uh uh and and you know&nbsp;&nbsp; then then we're going more to kind of like you can&nbsp; dispatch the agents to to do things and I think&nbsp;&nbsp; with with claude code we're going to go go more&nbsp; in the direction of you know you can dispatch the&nbsp;&nbsp; agents to do things and I'm sure we'll have other&nbsp; product surfaces that that that allow you to do&nbsp;&nbsp; that as well and I think we're we're heading to a&nbsp; world where a human developer can kind of manage a&nbsp;&nbsp; fleet of agents and say you go off and do this you&nbsp; go off and do this you go off and do that but but&nbsp;&nbsp; I think continued human involvement is going to&nbsp; be going to be important for the quality control&nbsp;&nbsp; to make sure they do the right things to get the&nbsp; details right and so you know working together&nbsp;&nbsp; on both the models and the product surface around&nbsp; it to get the details right is going to be really&nbsp;&nbsp; important i think it's also highlighted to me it&nbsp; makes the stuff that is inefficient in your work&nbsp;&nbsp; way more painful because it's taking you away&nbsp; from like this flow of building and so at least&nbsp;&nbsp; it's made me realize like where we're spending&nbsp; too much time on crossunctional alignment and&nbsp;&nbsp; you know road mapping when like we just should&nbsp; be trying to get more building so it's I've I've&nbsp;&nbsp; it's become more painful as the engineering part&nbsp; has has been sped up as well um so there's endless&nbsp;&nbsp; debate uh you know around the industry around you&nbsp; know uh bigger models or smaller architectures&nbsp;&nbsp; which will win in the long run um you're famous&nbsp; for you know popularizing and and pioneering the&nbsp;&nbsp; scaling laws paper what's your current take on&nbsp; you know the extreme being is pre-training dead&nbsp;&nbsp; is pre-training all that matter still and its&nbsp; role you know relative to to post-training i&nbsp;&nbsp; mean without getting too specific I would say that&nbsp; you know the Clawed 4 models embody advances in&nbsp;&nbsp; both pre-training and post-training um so we're&nbsp; continuing to see the pre-training scaling laws&nbsp;&nbsp; work the way that they've worked before um uh and&nbsp; we're also continuing to see continued advances in&nbsp;&nbsp; uh post-training and and they kind of they kind of&nbsp; complement each other uh and I I think we're going&nbsp;&nbsp; to continue seeing advances in both of those i&nbsp; think we're also going to continue to scale up so&nbsp;&nbsp; we have these these multiple trends these multiple&nbsp; sources of exponential growth and they're they're&nbsp;&nbsp; all going to compound with each other right that's&nbsp; that's why I think all of this is going to go very&nbsp;&nbsp; fast one of the reasons I liked Jiega's blog post&nbsp; is that it was someone who was not me repeating&nbsp;&nbsp; the mantra of like it's only going to be a year&nbsp; or two until until these things are like you know&nbsp;&nbsp; are basically peers to us it's insane that 37&nbsp; was just in February right it's it feels like&nbsp;&nbsp; a year ago but it was just three months ago i I I&nbsp; know it i know it feels like it's like oh this is&nbsp;&nbsp; this feels like an obsolete model or something&nbsp; and you know it was it's less is like two and&nbsp;&nbsp; a half months or something it's like the the time&nbsp; scales are the time scales are compressing and I&nbsp;&nbsp; often say that uh being in the AI field I will go&nbsp; on a very brief digress be being in the AI field&nbsp;&nbsp; it feels like you're getting on a a spaceship&nbsp; from leaving Earth at relativistic speeds and&nbsp;&nbsp; uh you know one day you wake up and you know it's&nbsp; like you know one day on your spaceship two days&nbsp;&nbsp; on Earth so you have to take in the news of two&nbsp; days it accelerates one day on your spaceship&nbsp;&nbsp; three days on Earth and and and you know that&nbsp; that's that's just what it feels like being&nbsp;&nbsp; being on this ride that resonates i've heard the&nbsp; metaphor before but it absolutely does um maybe on&nbsp;&nbsp; the post- training front one of the things that I&nbsp; got really excited about seeing developed in Cloud&nbsp;&nbsp; 4 has been this concept of memory and having&nbsp; the the model being able to manage it memory&nbsp;&nbsp; maybe talk a sec about why that's important&nbsp; and what that kind of enables uh sorry repeat&nbsp;&nbsp; the question like for uh the model to be able to&nbsp; manage its own memory and be able to handle those&nbsp;&nbsp; long horizon tasks as well yes yes we have found&nbsp; that to be uh very useful i think one one place&nbsp;&nbsp; we found it to be useful is Pokemon right um uh&nbsp; where the model's able to like remember its state&nbsp;&nbsp; but you know presumably it's it's useful for many&nbsp; things other than just Pokemon um uh uh but uh um&nbsp;&nbsp; no I think I think it's great that you know the&nbsp; model you know just as a human would like when I'm&nbsp;&nbsp; thinking I'll write a bunch of notes and uh you&nbsp; know then I'll like recall those notes at a later&nbsp;&nbsp; time or you know that there's just a lot of lot&nbsp; of intermediate work that I have to do that that&nbsp;&nbsp; you know and models do that to some extent when&nbsp; they when they reason when they have you know like&nbsp;&nbsp; our our reasoning traces but uh you know not not&nbsp; everything I do can be incorporated in one scratch&nbsp;&nbsp; pad right there's like presentations there's um&nbsp; you know individual documents that I that I write&nbsp;&nbsp; and so models are the same right the the idea for&nbsp; them to kind of you know be able to create files&nbsp;&nbsp; to do things with those files to load data and to&nbsp; kind of seamlessly interle those things right the&nbsp;&nbsp; the one of the new features that we have is this&nbsp; this kind of interled re interled reasoning and&nbsp;&nbsp; taking actions and some of those actions can be&nbsp; storing data recalling data again the affordances&nbsp;&nbsp; that the models have are gradually converging&nbsp; towards the affordances that a human has which&nbsp;&nbsp; I think is is the way that it should be one of&nbsp; my mind-blowing moments in Cloud 4 so far was&nbsp;&nbsp; we added like basically a to-do list scratch pad&nbsp; to cloud code and just watching it turn through&nbsp;&nbsp; the to-do list and then as it thought of more&nbsp; things to do add to the to-do list check things&nbsp;&nbsp; off strike out what was no longer relevant it&nbsp; really mimicked I think how people managed their&nbsp;&nbsp; own work and how they think about uh completion&nbsp; along the way and then the interled reasoning uh&nbsp;&nbsp; and tool use as well i saw a write up this morning&nbsp; on Mac stories where it was using a tool it was an&nbsp;&nbsp; MCP and it hit a rate limit with the backend MCP&nbsp; server and because it was doing the reasoning it&nbsp;&nbsp; was long I was like hm I probably hit a rate limit&nbsp; let me try this other approach to do this as well&nbsp;&nbsp; and so like that ability to reason and remediate&nbsp; as part of tool use I think is is really powerful&nbsp;&nbsp; um I'd love to touch on race at the top so um uh&nbsp; safety and and capabilities are often you know&nbsp;&nbsp; uh thought of as being at odds with each other and&nbsp; your thesis is exactly the opposite and that these&nbsp;&nbsp; two things can move in tandem i found that very&nbsp; inspiring and one of the reasons I joined here&nbsp;&nbsp; but maybe touch on how you think of of race to the&nbsp; top yeah so you know I think I think it it it uh&nbsp;&nbsp; applies to things you know from the from the uh&nbsp; from the very mundane and simple and commercial&nbsp;&nbsp; to kind of you know the grand directions that&nbsp; that that that that that that AI is going in&nbsp;&nbsp; the future um so you know I you know I think&nbsp; I think when we when we talk to customers we&nbsp;&nbsp; have a number of customers who you know care a&nbsp; lot about making sure that the behavior of their&nbsp;&nbsp; AI models is predictable that it's trustworthy&nbsp; um uh and I think that's aligned with what some&nbsp;&nbsp; of we're what what we're trying to do in the long&nbsp; term for uh you know making sure that models in&nbsp;&nbsp; a more grand sense stay in line with human intent&nbsp; um so there's there's this nice there's this nice&nbsp;&nbsp; synergy here and you know I think whenever&nbsp; we're able to do so whenever we think it's&nbsp;&nbsp; reasonable or responsible to do so we do want to&nbsp; provide tools for the community so M MC MCP MCP&nbsp;&nbsp; is an example of that um I I myself was actually&nbsp; surprised at the the pace at which everyone seems&nbsp;&nbsp; to have standardized around around MCP i mean it&nbsp; was it was very strange we released it in November&nbsp;&nbsp; i wouldn't say there was like a huge reaction&nbsp; immediately but then but then within 3 or 4 months&nbsp;&nbsp; you know it kind of become it kind of become the&nbsp; standard again there's again this this feeling&nbsp;&nbsp; of like being on the spaceship accelerating from&nbsp; Earth and and and you know experiencing you know&nbsp;&nbsp; larger and larger time time dilation constants&nbsp; yeah um where it's you know like you know think&nbsp;&nbsp; of like USB and other standards you know think&nbsp; of like standards in the '9s or the two like you&nbsp;&nbsp; know this would take it would take years for&nbsp; people to converge on something yeah and even&nbsp;&nbsp; in talking to other participants in the industry&nbsp; around MCP they're like we don't want to slow down&nbsp;&nbsp; whatever is working on MCP like we do want like&nbsp; some you know help on steering but like this is&nbsp;&nbsp; you've captured lightning in a bottle let's make&nbsp; sure it becomes the new protocol and the standard&nbsp;&nbsp; by which we interoperate agents as well um uh&nbsp; maybe tied together the race to the top i loved&nbsp;&nbsp; your urgency of interpretability essay you have a&nbsp; background in neuroscience as well can you talk a&nbsp;&nbsp; little bit about how you see the co-development of&nbsp; interpretability and um machine intelligence yeah&nbsp;&nbsp; so um you know I think 10 years ago uh many people&nbsp; thought that neuroscience would tell us about how&nbsp;&nbsp; to do AI um uh and indeed there you know are a&nbsp; number of former neuroscientists in the field i'm&nbsp;&nbsp; not I'm not the only one there you know there are&nbsp; other lab leaders some who have that uh who have&nbsp;&nbsp; that background um and you know I found at a high&nbsp; level there's some inspiration but I wouldn't say&nbsp;&nbsp; I've said oh you know this is how the you know&nbsp; this thing we know from the hypothalamus we can&nbsp;&nbsp; use for you know for for for making these models&nbsp; it's it's all been pretty much from scratch but&nbsp;&nbsp; interestingly things have gone the other way more&nbsp; which is that using interpretability we're able&nbsp;&nbsp; to see inside models and although of course&nbsp; they're not ex made in exactly the same way&nbsp;&nbsp; the human brain is at at a you know the a kind&nbsp; of superficial level there's there's a lot of&nbsp;&nbsp; differences a lot of the conceptual patterns we&nbsp; have found inside models sometimes they then get&nbsp;&nbsp; replicated in replicated in neuroscience research&nbsp; there was something about like high low frequency&nbsp;&nbsp; detectors in vision um that uh was found via&nbsp; interpretability via via one of one of the&nbsp;&nbsp; people on Chris Ola's team and then a couple years&nbsp; later a neuroscientist actually replicated it in&nbsp;&nbsp; in animal brains um the idea that for example&nbsp; vision models separate out you know they have&nbsp;&nbsp; one path that that tends to correspond to color&nbsp; and you know another path that corresponds to&nbsp;&nbsp; uh you know uh h a brightness or to the boundaries&nbsp; between objects these seem to be natural&nbsp;&nbsp; distinctions in the world right that are that are&nbsp; kind of there to be discovered and anytime you&nbsp;&nbsp; have any kind of abstract learning system whether&nbsp; it's artificial or biological you kind of discover&nbsp;&nbsp; the same thing so it's very interesting i'm really&nbsp; curious how the circuits paper ends up affecting&nbsp;&nbsp; neuroscience research as well um let's move into&nbsp; the 5 to 10 year time horizon um to the extent&nbsp;&nbsp; that that is even possible in AI as as we move&nbsp; relativistically maybe relativistically that's&nbsp;&nbsp; probably one year in real time um when do you&nbsp; think there'll be the first billion dollar company&nbsp;&nbsp; with one human employee 2026 yeah I absolutely&nbsp; buy that um do you have any advice for people&nbsp;&nbsp; building with Claude um for the next year how to&nbsp; think about building at that frontier as well yeah&nbsp;&nbsp; um I you know I think there's like a lot of very&nbsp; specific things you could say about like how about&nbsp;&nbsp; how to use the models but I feel like because of&nbsp; this whole like relativistic time dilation thing&nbsp;&nbsp; this like speeding things up like almost all the&nbsp; advice is drowned out by like one sentence which&nbsp;&nbsp; is or maybe two words which is just be ambitious&nbsp; um like build something that's greater than you&nbsp;&nbsp; think is is possible and even if it doesn't&nbsp; quite work yet another model will come out&nbsp;&nbsp; in the next generation which right now is three&nbsp; months but like probably it's going to go down&nbsp;&nbsp; to two months then one month and you know then&nbsp; then if I want to come up this year maybe I'll&nbsp;&nbsp; be giving advice that's like oh you know don't&nbsp; build anything today you know we're releasing&nbsp;&nbsp; something today but by tonight it'll be you know&nbsp; you won't want to be building with this tonight&nbsp;&nbsp; i talked to a founder who started a company two&nbsp; years ago in the sort of autonomous AI coding&nbsp;&nbsp; agent space and he basically tried every single&nbsp; model and his startup wasn't working and then it&nbsp;&nbsp; was actually 37 where he's like my startup works&nbsp; now and it was the same thing of like this thing&nbsp;&nbsp; that I was trying that was really hard all of a&nbsp; sudden is now um possible but hitting your head&nbsp;&nbsp; against the wall actually sometimes can be useful&nbsp; because you put all the other pieces in place and&nbsp;&nbsp; and everything works except the model and then&nbsp; when the model works it's almost like you've&nbsp;&nbsp; built something that's like more robust than it&nbsp; needs to And that can be like a positive property&nbsp;&nbsp; um so so you know as much as I joke about like&nbsp; oh you should you know you can just wait for the&nbsp;&nbsp; next model actually hitting your head against the&nbsp; wall as long as it's something that's like almost&nbsp;&nbsp; possible if it's not like you know like three&nbsp; years out from from what's possible um I think it&nbsp;&nbsp; can actually be productive we saw that even with&nbsp; advanced research internally like our our research&nbsp;&nbsp; and cloud skills team had built a prototype of&nbsp; this the model kind of lost its way it wasn't&nbsp;&nbsp; good at using tools and then with 37 especially&nbsp; with cloud 4 I think you'll find that it does&nbsp;&nbsp; advanced research really really well as well and&nbsp; it's because we were trying and kind of failing&nbsp;&nbsp; along the way as well yeah it's it's almost as if&nbsp; you want to run your you want to run your startup&nbsp;&nbsp; as like speculative execution against the next&nbsp; model right there's some kind of like I don't know&nbsp;&nbsp; I love that yeah I think that's exactly right um&nbsp; all right so last question to wrap up um for many&nbsp;&nbsp; of us today um who aren't Dario we couldn't have&nbsp; imagined the progress that AI has made and the&nbsp;&nbsp; rapid pace of change what are you most excited&nbsp; about for the coming year and in the next five&nbsp;&nbsp; years um yeah so uh I think for in the next coming&nbsp; year uh we are going to see incredible things in&nbsp;&nbsp; in in code i would refer again to kind of the you&nbsp; know taking where we are with cloud code and where&nbsp;&nbsp; we are with the coding models and going from there&nbsp; to kind of to kind of the agent fleets um I think&nbsp;&nbsp; this will have an interesting effect in the world&nbsp; which is I don't know that we've thought carefully&nbsp;&nbsp; like from an economic or business perspective&nbsp; about what happens when the cost of producing&nbsp;&nbsp; software goes down it's kind of an assumption&nbsp; an article of faith that you only make software&nbsp;&nbsp; if it's only worth it to make it if millions of&nbsp; people use it or at least hundreds of thousands&nbsp;&nbsp; or maybe tens of thousands like you wouldn't make&nbsp; you you know you you you you like wouldn't make a&nbsp;&nbsp; whole piece of software for this event right like&nbsp; you might throw together something but like when&nbsp;&nbsp; it just becomes really cheap when it costs you&nbsp; 20 cents to like oh let's just let's just throw&nbsp;&nbsp; let's just throw together something that you&nbsp; know you know changes you know ch changes my&nbsp;&nbsp; vision for this particular event or something&nbsp; like that um uh I think the world is going to&nbsp;&nbsp; be very different when these things can be made ad&nbsp; hoc on a on a one one one oneoff basis in like a&nbsp;&nbsp; few seconds for for less than a for for for less&nbsp; than a dollar what are what is the role of the&nbsp;&nbsp; developer there what is the role of businesses&nbsp; what is the role of startups um and what is what&nbsp;&nbsp; is the experience of the of the you know of the&nbsp; of the people using it i think we don't know the&nbsp;&nbsp; answer to any of those questions so that's&nbsp; very interesting on the on the fiveyear time&nbsp;&nbsp; scale I will return again to biology i think the&nbsp; biomedical stuff will not be revolutionized in the&nbsp;&nbsp; next year because it's it's kind of you know slow&nbsp; to slow to happen but uh yeah yeah I hope that uh&nbsp;&nbsp; five years from now we will have uh vanquished uh&nbsp; many of the diseases that now uh that now exist i&nbsp;&nbsp; love we'll leave it at that unfortunately we do&nbsp; have to wrap up i feel like we could talk for&nbsp;&nbsp; another 40 minutes so first I want to thank Daario&nbsp; for spending time with us today thank you Dario i also want to thank all of you who are here&nbsp; in person and those watching via liveream uh&nbsp;&nbsp; but before we close I almost forgot one thing um&nbsp; as a special thank you to everyone who joined us&nbsp;&nbsp; today at Code with Cloud in person i'm excited&nbsp; to announce that each of you will receive free&nbsp;&nbsp; access to Max 20X our highest tier plan&nbsp; for three months so look out for that i especially love using Macs with cloud&nbsp; code so you'll be able to do that as well&nbsp;&nbsp; so we can't wait to see what you build have&nbsp; a great rest of your day with the different&nbsp;&nbsp; um sessions and welcome again to Code with Claude&nbsp; thanks for coming thanks for coming everyone [Music] [Music] [Music] oh yeah oh [Music] oh hey hey [Music] [Music]&nbsp;&nbsp; hey hey hey

---

